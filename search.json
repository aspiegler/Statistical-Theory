[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "",
    "text": "Preface\nStatistical Methods: Exploring the Uncertain is a set of interactive course materials designed to explore statistical methods. These materials are Open Education Resources (OER) designed to serve as both the textbook and in-class labs for MATH 3382: Statistical Theory at University of Colorado Denver. MATH 3382 is an undergraduate course in statistics at the 300 (junior) level that is required for all math majors at University of Colorado Denver. These materials are not intended for an introduction to statistics course for a more general audience of students from various backgrounds that are typically at the 100 or 200 levels. Students of Mathematics, Statistics, Data Science, Economics, Biostatistics, Computer Science and other STEM fields that want to advance beyond a typical introduction to statistics course are the intended audience.\nThe materials are designed for full semester (3 credit) course. Topics covered include exploratory data analysis, statistical inference, probability, sampling distributions, maximum likelihood estimators, method of moments estimators, properties of estimators, confidence intervals (both bootstrap and parametric methods), and hypothesis tests (both permutation tests and parametric methods)\nThe prerequisite for the course at CU Denver is multivariable calculus; however, students with a background in single variable calculus are able to work through the materials as well. The only instance where multivariable calculus might arise is with joint probability distributions and maximum likelihood estimators for distributions with two or more parameters, both of these applications can be skipped.\nStudents are not required to have any previous course work in statistics, probability, or coding in R (or any other language). The “essentials” of probability are covered in Chapter 2 with a focus on how we apply theory from probability to do statistics. These materials do not include a comprehensive treatment of probability."
  },
  {
    "objectID": "index.html#a-virtual-lab-for-exploring-statistics",
    "href": "index.html#a-virtual-lab-for-exploring-statistics",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "A Virtual Lab for Exploring Statistics",
    "text": "A Virtual Lab for Exploring Statistics\n\nThese materials are intended as set of activities to experiment and explore statistical theory and methods. Each interactive Jupyter notebooks is a “virtual laboratory” where we perform our experiments and summarize the results. The objectives of experimental mathematics/statistics are generally to make the subject more tangible, lively and fun.\nThe intent of introducing R is not to avoid a deep and rigorous understanding of statistics or to use R simply as a calculator. There are other sources that skim the surface of statistical concepts and instead focus on the coding side of things. These materials use R as an additional tool for further exploring statistics to gain a deeper insight into statistical models. Some of the objectives of implementing R code cells into the materials are to:\n\nEasily import and analyze real data from interesting studies and experiments.\nCreate insightful data visualizations.\nExplore features of the data to generate statistical questions worth investigating.\nDiscover patterns and relationships between variables.\nIntroduce and implement resampling methods.\nDevelop and test conjectures.\nConfirm analytically derived results.\nGain further insight and intuition.\nBridge the divide between theory and practice."
  },
  {
    "objectID": "index.html#how-to-access-edit-and-save-notebooks",
    "href": "index.html#how-to-access-edit-and-save-notebooks",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "How to Access, Edit and Save Notebooks",
    "text": "How to Access, Edit and Save Notebooks\n\nThis html version of the materials is not dynamic. You cannot edit the text or run code with the html version.\n \n\nAt the top of each notebook is a “button” such as the one above.\nClick the button to open an interactive Jupyter notebook version initialized to run R code in Google Colaboratory (or Colab).\nYou can begin working with the notebook right away in Colab! There is no software to install (or purchase!).\nYou can also access the materials directly on GitHub at https://github.com/CU-Denver-MathStats-OER/Statistical-Theory.git.\n\nEach Jupyter notebook contains both narrative text (in Markdown cells) and R code cells that you can create, modify, and run.\n\n\n\n\n\n\nCaution\n\n\n\nAlthough you do not need a Google account to interact with the notebooks, the Colab notebooks are “shared”, meaning you cannot save any changes to the initial shared document that opens. If you would like to save your changes, you first need to save a copy to your Google Drive. Then you can edit and save changes to your own version.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn addition to Jupyter notebooks versions that run in Colab, the original Quarto markdown documents (with file extension .qmd) are also included in the repo if you prefer to run and edit the labs using Posit, RStudio or other available applications. Both the Jupyter (.ipynb) and Quarto markdown (.qmd) versions are identical and both use R."
  },
  {
    "objectID": "index.html#what-programming-background-is-required",
    "href": "index.html#what-programming-background-is-required",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "What Programming Background is Required?",
    "text": "What Programming Background is Required?\n\nNo prior experience or knowledge of R, Markdown, LaTeX, or Colaboratory is assumed or required to begin working with these materials. After working with these materials, you will have some knowledge and experience with R, Markdown, LaTeX, and Colaboratory!\n\nWelcome to Colaboratory is a helpful notebook (with videos) to help introduce you to Colab.\nHere’s a helpful Markdown guide.\n\n\nWhat is R?\n\nR is a programming language used largely for statistical computing, data wrangling and visualization. We will be using R as a tool for exploring statistical theory. The first stable version of R was released in 2000, and after all of this time, there is a large community of R users that have already created tons of useful packages and shared interesting data sets that are frequently updated. We will create, modify, and run R code in Jupyter notebooks. No prior programming experience is required to begin working in R in these materials.\nThe goal of these materials are to investigate statistical theory, not learn how to be an expert R programmer. The hope is that we can use R as a tool for experimenting, gaining a deeper insight, and implementing resampling methods. In the process, hopefully you gain a familiarity with R and coding so it is no longer a barrier to future work! Have I mentioned no prior programming experience is required to begin working in R in these materials?\n\n\nWhat is LaTex?\n\nLaTeX is a system for rendering nice looking mathematical symbols, expressions, and equations. All of the mathematical notation in these materials are created using LaTeX. You can view and edit all of the LaTeX code in the Markdown cells. You do not need to become an expert in LaTeX, but having a familiarity with LaTeX is quite helpful and LaTeX can be used to typeset math in a number of different applications.\n\nHere is a useful dictionary of LaTeX math symbols to get a glimpse of LaTeX."
  },
  {
    "objectID": "index.html#how-to-contact-me",
    "href": "index.html#how-to-contact-me",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "How to Contact Me",
    "text": "How to Contact Me\n\nIf you have any questions, comments, or suggestions about these materials, please feel free to reach out to me (Adam) at adam.spiegler@ucdenver.edu.\n\nConsidering using these materials in your course? Please let me know if I can help.\nIf you do use some of these materials in your course, your feedback is welcome and appreciated.\nIf you materials that you would like to share or contribute to this project, great!"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nThis project was supported by the Colorado Department of Higher Education (CDHE) OER Grant Program. A big thank you to Megan Patnott for reviewing the materials and providing many great corrections and suggestions to improve the materials!"
  },
  {
    "objectID": "index.html#creative-commons-license-information",
    "href": "index.html#creative-commons-license-information",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler (University of Colorado Denver) is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This work is funded by an Institutional OER Grant from the Colorado Department of Higher Education (CDHE).\nFor similar interactive OER materials in other courses funded by this project in the Department of Mathematical and Statistical Sciences at the University of Colorado Denver, visit https://github.com/CU-Denver-MathStats-OER."
  },
  {
    "objectID": "index.html#quarto-books",
    "href": "index.html#quarto-books",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "Quarto Books",
    "text": "Quarto Books\n\nThese materials are creating using Quarto books. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01-Intro-to-Inference.html",
    "href": "01-Intro-to-Inference.html",
    "title": "1.1: Introduction to Statistics",
    "section": "",
    "text": "What is R?\nR is a programming language used largely for statistical computing, data wrangling and visualization. We will be using R as a tool for exploring statistical theory. The first stable version of R was released in 2000, and after all of this time, there is a large community of R users that have already created tons of useful packages and shared interesting data sets that are frequently updated.\nA fundamental application of statistics is to use data from a subset of a population to draw conclusions about the population.\nThis semester we will mainly focus on steps 3 and 4; however, the methods we learn are not as powerful without carefully considering steps 1 and 2!\nSince drawing a sample that resembles the population in every way (except smaller in number) is critical for drawing valid conclusions, how we pick samples is crucial. Sometimes we are limited by considerations such as how expensive or timely it is to collect data from a population. We always have an obligation to be ethical in how we collect data.\nSampling bias occurs when the method of selecting a sample causes the sample to differ from the population in some relevant way. The best way to avoid bias is to take the human element out of the process.\nOften in statistics we would like to investigate whether one variable is associated to another. Researchers carry out studies to understand the conditions and causes of certain outcomes.\nIf we are using one variable to help us understand or predict the values (or category) of another variable:"
  },
  {
    "objectID": "01-Intro-to-Inference.html#what-are-packages-in-r",
    "href": "01-Intro-to-Inference.html#what-are-packages-in-r",
    "title": "1.1: Introduction to Statistics",
    "section": "What Are Packages in R?",
    "text": "What Are Packages in R?\n\nR packages are a collection functions, sample data, and/or other code scripts. R installs a set of default packages during installation.\n\nThe files, code, and data associated to installed packages are saved in the cloud and not locally on your computer.\nMany R packages have already been installed in Google Colaboratory.\n\nRun the code cell below to get a list of all default R packages available in Google Colaboratory.\n\n# See a list of installed default packages\nallpack &lt;- installed.packages()\nrownames(allpack)"
  },
  {
    "objectID": "01-Intro-to-Inference.html#what-data-is-available-in-r",
    "href": "01-Intro-to-Inference.html#what-data-is-available-in-r",
    "title": "1.1: Introduction to Statistics",
    "section": "What Data is Available in R?",
    "text": "What Data is Available in R?\n\nR has many available data sets that we can easily import, investigate, and apply statistical methods and analysis that we will discover this semester.\n\nRun the code cell below to get a list of all available data sets in all available packages in R.\nA tab should open on the right displaying a long list of data sets.\nWe can close the tab in order to keep a larger working window.\n\n\ndata(package = .packages(all.available = TRUE))"
  },
  {
    "objectID": "01-Intro-to-Inference.html#loading-packages-with-the-library-command",
    "href": "01-Intro-to-Inference.html#loading-packages-with-the-library-command",
    "title": "1.1: Introduction to Statistics",
    "section": "Loading Packages with the library() Command",
    "text": "Loading Packages with the library() Command\n\nEach time we start or restart a new R session and want to access the library of functions and data in the package, we need to load the library of files in the package with the library() command.\n\nThe dplyr package is already installed in Google Colaboratory\nWe still need to use a library() command to load the package if we want to access data and functions in the package.\nIf we do not run the code cell below, we will not be able to run the rest of the code cells in this document without receiving error messages.\nRun the code cell below to load the dplyr package.\n\n\nlibrary(dplyr)\n\n\n\n\n\n\n\nCaution\n\n\n\nIf we take a break in our work, it is possible our R session will time out and close. Each time we restart an R session, we will need to rerun library() commands in order reload any packages we plan to use.\nThe same caution applies to any objects, vectors, or data frames we create or edit in an R session. If a session times out, and we want to use an object x that we previously created, we will need to run the code cell(s) where object x is created again before we can refer back to x in the current session."
  },
  {
    "objectID": "01-Intro-to-Inference.html#finding-help-documentation",
    "href": "01-Intro-to-Inference.html#finding-help-documentation",
    "title": "1.1: Introduction to Statistics",
    "section": "Finding Help Documentation",
    "text": "Finding Help Documentation\n\nAs with learning any new skill, it is always useful to know where to find help. R has been in use since 2000, and there is a large, active community of users that share lots of helpful advice online. Certainly Google or other search engines are a useful way to search and find help with R. Below are two additional websites useful for searching for help with R.\n\nThe developers of R have useful page where to find help.\nRseek is provided by Sasha Goodman at Stanford university. This engine lets you search several R related sites.\n\nWe can also find help without opening a separate browser window or tab. The ? help operator and help() function provide access to the help manuals for R functions, data sets, and other objects. Running a ? or help() command in a code cell opens a side bar with a tab displaying the help documentation.\n\nFor example, the package dplyr contains a data set called storms.\nWhere is the data from, and what variables are in the data set?\nRun the code cell below to access the help documentation for the storms data set.\n\nResizing the tab in the side bar may help the documentation be more readable.\nWe can close the tab if we want to increase the size of our working window.\n\n\n\n?storms"
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-1",
    "href": "01-Intro-to-Inference.html#question-1",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 1",
    "text": "Question 1\n\nAfter reading the storms help documentation, answer the following questions:\n\nWhat is the source of the data?\nWhat variables are include in the data?\nOver what period of time and how frequently are observations recorded?\n\n\nSolutions to Question 1"
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-2",
    "href": "01-Intro-to-Inference.html#question-2",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 2",
    "text": "Question 2\n\nInsert a code cell and run the command ?hist to see the help documentation for the histogram function.\n\nWhat option can we use to add a main title to the histogram?\nWhat option can we use to set the fill color for the bars of a histogram?\n\n\nSolution to Question 2"
  },
  {
    "objectID": "01-Intro-to-Inference.html#produce-data",
    "href": "01-Intro-to-Inference.html#produce-data",
    "title": "1.1: Introduction to Statistics",
    "section": "Produce Data",
    "text": "Produce Data\n\nWe need data to do statistics! How we design the collection of data is a crucial first step. If we collect the wrong data or biased data, we cannot gain reasonable insights to achieve our goal even if we do proper analysis. In the end, the conclusions we infer are only as strong as the data we collect.\n\nExperimental design is an entire field in itself and requires many considerations and techniques to ensure we can reach our ultimate goal.\nCollecting or finding the right data is a critical step that should be carefully planned.\nWe do not spend too much time on this aspect of the process this semester, but we should be very aware of how important and delicate this step can be.\nThere are many interesting data sets others have already collected that we can easily import into R for further analysis.\nThis semester we will mostly be working with data sets that others have already carefully collected rather than produce the data ourselves."
  },
  {
    "objectID": "01-Intro-to-Inference.html#explore-data",
    "href": "01-Intro-to-Inference.html#explore-data",
    "title": "1.1: Introduction to Statistics",
    "section": "Explore Data",
    "text": "Explore Data\n\nExploratory data analysis, or EDA for short, can be thought of as a cycle:\n\nGenerate questions about our data.\nSearch for answers by visualizing, transforming, and modeling our data.\nUse what we learn to refine your questions and/or generate new questions.\n\nThe main goal of EDA is to develop an understanding of your data. When we ask a question, the question focuses our attention on a specific part of the data set and helps us decide which graphs, models, or transformations to make.\n\nEDA can be an intricate process that requires developing some keen investigation skills.\nExploratory Data Analysis with R by Roger Peng is a free textbook to help you dig deeper into EDA if interested!\nOur focus this semester will not be on EDA, but at times we will be exploring data in order to uncover our own statistical questions."
  },
  {
    "objectID": "01-Intro-to-Inference.html#analyze-and-interpret",
    "href": "01-Intro-to-Inference.html#analyze-and-interpret",
    "title": "1.1: Introduction to Statistics",
    "section": "Analyze and Interpret",
    "text": "Analyze and Interpret\n\nAfter we explore data and generate statistical questions to investigate, we apply statistical methods to analyze the data and hopefully gain insight to draw conclusions. Sometimes, even though we carefully perform our analysis correctly, we are unable to reach a decisive conclusion. Sometimes instead of reaching an answer, the insight we gain informs us to refine our question. Other times, it simply leads to more questions! Probability is required to do statistical analysis, and we will explore some key results from probability that we will need to apply in this step."
  },
  {
    "objectID": "01-Intro-to-Inference.html#present-findings",
    "href": "01-Intro-to-Inference.html#present-findings",
    "title": "1.1: Introduction to Statistics",
    "section": "Present Findings",
    "text": "Present Findings\n\nFinally, after we have interpreted our results, we present our findings to others. Sometimes our colleagues specialize in fields outside of mathematics, statistics, or data science. They rely on us to breakdown our findings in more practical terms accessible to a more general audience."
  },
  {
    "objectID": "01-Intro-to-Inference.html#professional-ethics",
    "href": "01-Intro-to-Inference.html#professional-ethics",
    "title": "1.1: Introduction to Statistics",
    "section": "Professional Ethics",
    "text": "Professional Ethics\n\nAs we learn and refine our statistical tools, it is our duty to be mindful of the moral and ethical considerations at play in deciding what statistical questions we want to explore, how we produce data, what is the source of our data, what is the benefit to society of our work, and who is impacted by our analysis.\nBelow is a excerpt from An Introduction to the Science of Statistics: From Theory to Implementation by Joseph Watkins that we should always be very mindful of whether we are doing statistical analysis or other endeavors:\n\n“Those with particular training have a special obligation to bring to the public their special knowledge. Such public statements can take several forms. We can speak out as a member of society with no particular basis in our area of expertise. We can speak out based on the wisdom that comes with this specialized knowledge. Finally, we can speak out based on a formal procedure of gathering information and reporting carefully the results of our analysis. In each case, it is our obligation to be clear about the nature of that communication and that the our statements follow the highest ethical standards. In the same vein, as consumers of information, we should have a clear understanding of the perspective in any document that presents statistical information.”\n\nBelow are two sources with ethical guidance to help steer our practice of statistics.\n\nEthical Guidelines for Statistical Practice by the American Statistical Association (ASA).\nDeclaration on Professional Ethics by the International Statistical Institute."
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-3",
    "href": "01-Intro-to-Inference.html#question-3",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 3",
    "text": "Question 3\n\nIn the storms data set, is the data from a sample or a population? What information in the help documentation supports your answer? Recall you can run the command ?storms to open the help documentation.\n\nSolution to Question 3"
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-4",
    "href": "01-Intro-to-Inference.html#question-4",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 4",
    "text": "Question 4\n\nWhat statistical questions might be worth investigating among the variables in the storms data set? What data visualizations could be useful to uncover interesting questions?\nRun the summary(storms) command in the code cell below to view a numerical summary for each variable in the data set to help formulate your question.\n\nsummary(storms)\n\n\nSolution to Question 4"
  },
  {
    "objectID": "01-Intro-to-Inference.html#random-sampling-methods",
    "href": "01-Intro-to-Inference.html#random-sampling-methods",
    "title": "1.1: Introduction to Statistics",
    "section": "Random Sampling Methods",
    "text": "Random Sampling Methods\n\nWhen possible, collecting samples randomly without human interference is the best and therefore preferred method. Randomly selecting samples is the best way to avoid bias. Below are some common methods for selecting random samples and avoiding bias.\n\nSimple Random Sample\n\nWhen selecting a simple random sample, all individuals are equally likely to be selected.\n\nAssign 12 participants in an experiment a number 1 to 12. Randomly select 4 numbers and assign those individuals to treatment group A.\n\n\n\n\nImage Credit: Dan Kernler, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\n\nStratified Sample\n\nWhen selecting a stratified sample, the population is subdivided into groups based on some meaningful characteristic.\n\nDivide all voters into categories based on political party. Then randomly sample individuals from each political party so the overall sample has the same percentage of voters in each party as the population of all voters.\n\n\n\n\nImage Credit: Dan Kernler, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\n\nSystematic Sample\n\nWhen selecting a systematic sample, the first individual is chosen at random. Then a rule is used so that every \\(\\mbox{n}^{\\mbox{th}}\\) individual is selected after that.\n\nRandomly select a house on a street to survey. Then pick every third house after that to include in the survey.\n\n\n\n\nImage Credit: Dan Kernler, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\n\nCluster Sample\n\nWhen selecting a cluster sample groups rather than individual units of the target population are selected at random for the test.\n\nDivide all voters into 10 groups based on the last digit of their social security number. Randomly pick two numbers between 0 and 9. Select all voters whose social security number ends with one of those two numbers.\n\n\n\n\nImage Credit: Dan Kernler, CC BY-SA 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "01-Intro-to-Inference.html#biased-sampling-methods",
    "href": "01-Intro-to-Inference.html#biased-sampling-methods",
    "title": "1.1: Introduction to Statistics",
    "section": "Biased Sampling Methods",
    "text": "Biased Sampling Methods\n\n\nA convenience sample is when people or elements in a sample are selected on the basis of their accessibility and availability. For example, email a survey to your friends to gauge public support for a newly proposed law.\nVoluntary sampling is a type of a convenience sample. For example, robocall all registered voters and gather responses from those that answer the phone and are willing to participate."
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-5",
    "href": "01-Intro-to-Inference.html#question-5",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 5",
    "text": "Question 5\n\nFor each question below, which variable is the predictor variable and which is the response variable? How would your organize the data you collect in each case?\n\nDoes daily exercise reduce the risk of early onset dementia?\nIs rewarding people or punishing people a more effective incentive to help them quit smoking?\nIs a new vaccine effective at preventing disease?\n\n\nSolution to Question 5"
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-6",
    "href": "01-Intro-to-Inference.html#question-6",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 6",
    "text": "Question 6\n\nBoth studies below are designed to examine whether rewarding good behavior or punishing bad behavior is a more effective method to help people quit smoking. Which study do you believe is better designed? Why?\n\nStudy A\n\nEmployees at a large company voluntarily enroll in a quit smoking study. When they join, they are provided two options to select from:\n\nOption 1 (Reward-based group): If after six months the participant has quit smoking, they get an $800 reward.\nOption 2: (Deposit-based group): Pay an initial $150 refundable deposit. If after six months the participant:\n\nHas quit smoking, they receive their $150 deposit back plus an additional $800 reward.\nHas not quit smoking, then they do not receive their $150 deposit back.\n\n\nAfter six months, we compare the success rate between the two groups to determine which method is more effective.\n\n\nStudy B\n\nEmployees at a large company voluntarily enroll in a quit smoking study.\n\nWhen they join, they are randomly assigned to either be in the Reward-based group or Deposit-based group with the same exact reward and penalty system for each option as in Study A.\n\nAfter six months, we compare the success rate between the two groups to determine which method is more effective.\n\n\nSolution to Question 6"
  },
  {
    "objectID": "01-Intro-to-Inference.html#sec-confounding",
    "href": "01-Intro-to-Inference.html#sec-confounding",
    "title": "1.1: Introduction to Statistics",
    "section": "Confounding Variables",
    "text": "Confounding Variables\n\nA variable that is associated with both the predictor variable and the response variable is called a confounding or lurking variable.\n\n\n\nImage Credit: Adam Spiegler, CC BY-SA 4.0."
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-7",
    "href": "01-Intro-to-Inference.html#question-7",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 7",
    "text": "Question 7\n\nIn Question 6, identify a possible confounding variable in Study A or explain why there are no confounding variables. Identify a possible confounding variable in Study B or explain why there are no confounding variables.\n\nSolution to Question 7"
  },
  {
    "objectID": "01-Intro-to-Inference.html#experiments-and-observational-studies",
    "href": "01-Intro-to-Inference.html#experiments-and-observational-studies",
    "title": "1.1: Introduction to Statistics",
    "section": "Experiments and Observational Studies",
    "text": "Experiments and Observational Studies\n\n\nAn observational study is a study in which the researcher does not actively control the assignment of individuals to different treatments or levels of a predictor variable.\n\nIf the treatment groups are chosen by the individuals in the study, the samples in each treatment group are likely to differ in some meaningful way other than just the treatment.\n\nAn experiment is a study in which the researcher controls the assignment of individuals to different treatments or levels of a predictor variable.\nIn a randomized experiment the predictor variable for each individual is determined randomly, before the response variable is measured.\n\nIf treatment groups are randomly determined, they should be similar in every way except for the treatment itself.\nWhen properly designed, randomized experiments can show a predictor variable causes a change in the response variable.\n\n\nThere are almost always confounding variables in observational studies. Thus observational studies can almost never be used to establish causation. Sometimes it is not possible to design an experiment for ethical or practical reasons. We can still investigate whether two variables are associated with each other in observational studies."
  },
  {
    "objectID": "01-Intro-to-Inference.html#question-8",
    "href": "01-Intro-to-Inference.html#question-8",
    "title": "1.1: Introduction to Statistics",
    "section": "Question 8",
    "text": "Question 8\n\nIn Question 6, determine whether Study A is an observational study or an experiment. Determine whether Study B is an observational study or an experiment. Explain how you determined your answers.\n\nSolution to Question 8\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "02-EDA-Categorical.html",
    "href": "02-EDA-Categorical.html",
    "title": "1.2: Exploring Categorical Data",
    "section": "",
    "text": "An Overview of Exploratory Data Analysis\nExploratory data analysis, or EDA for short, can be thought of as a cycle:\nThe main goal of EDA is to develop an understanding of your data. When we ask a question, the question focuses our attention on a specific part of the data set and helps us decide which graphs, models, or transformations to make.\nThe dplyr package is perhaps one of the most useful R packages for data wrangling and EDA. Data wrangling is generally the cleaning, reorganizing, and transforming data so it can be more easily analyzed. dplyr also contains data sets that we can use to practice our wrangling and visualization skills. In this lab, we will work with the data set called storms.\n# loads dplyr package\nlibrary(dplyr)\nData frames are two-dimensional data objects and are the fundamental data structure used by most of R’s libraries of functions and data sets.\nEach column of a data frame is a variable (stored as a vector) of possibly different data types.\nBelow are some common functions used to get a first introduction to our data:\nR has 6 basic data types:\nSee the Appendix for an Fundamentals of Working with Data for more information.\nThere are many different ways we can great visualizations to gain insight into our data and search for possible patterns and relations between different variables.\nImagine we would like to compare the number of different category hurricanes that occurred in each month. In this case, we would like to compare two qualitative variables, namely category and month.\nThe data set gss_cat can be accessed from the forcats package. Below is a quote taken from the website of the GSS Data Explorer website maintained by NORC at the University of Chicago\n# load the forcats package\nlibrary(forcats)\n# open help documentation for gss_cat\n?gss_cat\n# get numerical summary of variables\nsummary(gss_cat)\n\n      year               marital           age                    race      \n Min.   :2000   No answer    :   17   Min.   :18.00   Other         : 1959  \n 1st Qu.:2002   Never married: 5416   1st Qu.:33.00   Black         : 3129  \n Median :2006   Separated    :  743   Median :46.00   White         :16395  \n Mean   :2007   Divorced     : 3383   Mean   :47.18   Not applicable:    0  \n 3rd Qu.:2010   Widowed      : 1807   3rd Qu.:59.00                         \n Max.   :2014   Married      :10117   Max.   :89.00                         \n                                      NA's   :76                            \n           rincome                   partyid            relig      \n $25000 or more:7363   Independent       :4119   Protestant:10846  \n Not applicable:7043   Not str democrat  :3690   Catholic  : 5124  \n $20000 - 24999:1283   Strong democrat   :3490   None      : 3523  \n $10000 - 14999:1168   Not str republican:3032   Christian :  689  \n $15000 - 19999:1048   Ind,near dem      :2499   Jewish    :  388  \n Refused       : 975   Strong republican :2314   Other     :  224  \n (Other)       :2603   (Other)           :2339   (Other)   :  689  \n              denom          tvhours      \n Not applicable  :10072   Min.   : 0.000  \n Other           : 2534   1st Qu.: 1.000  \n No denomination : 1683   Median : 2.000  \n Southern baptist: 1536   Mean   : 2.981  \n Baptist-dk which: 1457   3rd Qu.: 4.000  \n United methodist: 1067   Max.   :24.000  \n (Other)         : 3134   NA's   :10146"
  },
  {
    "objectID": "02-EDA-Categorical.html#finding-help-documentation",
    "href": "02-EDA-Categorical.html#finding-help-documentation",
    "title": "1.2: Exploring Categorical Data",
    "section": "Finding Help Documentation",
    "text": "Finding Help Documentation\n\n\nThe code cell below opens a glossary tab of all (most?) functions and data in the package dplyr.\n\n\n# open glossary of dplyr functions\nhelp(package = \"dplyr\")\n\n\nThe code cell below opens a help tab with information about the storms data set.\n\n\n# opens help tab with info about storms data set\n?storms"
  },
  {
    "objectID": "02-EDA-Categorical.html#question-1",
    "href": "02-EDA-Categorical.html#question-1",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 1",
    "text": "Question 1\n\nLet’s get to know the storms data set. Using some (or all) of the commands above, answer the following questions:\n\nHow many observations are in data set storms?\nHow many variables are storms?\n\nWhich variables are quantitative and which are categorical?\nWhich categorical variables have a ranking? Which do not?\n\nHint: Avoid using the View() function. Instead, use head() or tail() if you want to get a sense of what the raw data looks like.\n\nExperiment with some of the functions in the code cell below to answer the questions. Then type your answer in the space below.\n\nsummary(storms)  # summary of each variable in storms\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512            \n\n#head(storms)  # prints first 6 rows to screen\n#tail(storms)  # prints last 6 rows to screen\n#glimpse(storms)  # gives a glimpse of the data set\n#str(storms)   # summary of data structure\n\n\nSolution to Question 1"
  },
  {
    "objectID": "02-EDA-Categorical.html#question-2",
    "href": "02-EDA-Categorical.html#question-2",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 2:",
    "text": "Question 2:\n\nWhat additional information would you like to know about the storms data set that you were unable to find? What questions do you have about the data? In particular, what data is missing and why?\n\nSolution to Question 2:"
  },
  {
    "objectID": "02-EDA-Categorical.html#checking-data-types-using-typeof",
    "href": "02-EDA-Categorical.html#checking-data-types-using-typeof",
    "title": "1.2: Exploring Categorical Data",
    "section": "Checking Data Types Using typeof()",
    "text": "Checking Data Types Using typeof()\n\n\nThe typeof() function returns the R internal type or storage mode of any object.\n\n\ntypeof(1.0)\n\n[1] \"double\"\n\ntypeof(2)\n\n[1] \"double\"\n\ntypeof(3L)\n\n[1] \"integer\"\n\ntypeof(\"hello\")\n\n[1] \"character\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\ntypeof(storms$status)\n\n[1] \"integer\"\n\ntypeof(storms$year)\n\n[1] \"double\"\n\ntypeof(storms$name)\n\n[1] \"character\""
  },
  {
    "objectID": "02-EDA-Categorical.html#extracting-a-variable-by-name-with",
    "href": "02-EDA-Categorical.html#extracting-a-variable-by-name-with",
    "title": "1.2: Exploring Categorical Data",
    "section": "Extracting a Variable By Name with $",
    "text": "Extracting a Variable By Name with $\n\nIn the command typeof(storms$status), notice we refer to just the status variable of the storms data frame. A variable from a data frame may be extracted using $ and then specifying the name of the desired variable.\n\nFirst indicate the name of the data frame, storms.\nFollowed by a dollar sign $.\nThen indicate the name of the variable, for example wind.\nThe storms$wind is a vector that contains the wind speed of each storm.\n\n\nis.vector(storms)  # storms is not a vector\n\n[1] FALSE\n\nis.data.frame(storms)  # storms is a data frame\n\n[1] TRUE\n\nis.data.frame(storms$wind)  # status is not a data frame\n\n[1] FALSE\n\nis.vector(storms$wind)  # status is a vector\n\n[1] TRUE"
  },
  {
    "objectID": "02-EDA-Categorical.html#investigating-data-types-with-is.numeric",
    "href": "02-EDA-Categorical.html#investigating-data-types-with-is.numeric",
    "title": "1.2: Exploring Categorical Data",
    "section": "Investigating Data Types with is.numeric()",
    "text": "Investigating Data Types with is.numeric()\n\n\nThe is.numeric(x) function tests whether or not an object x is numeric.\nThe is.character(x) function tests whether x is a character or not.\nThe is.factor(x) function tests whether x is a factor or not.\nNote: Categorical data is typically stored as a factor in R.\n\n\nis.numeric(storms$year)  # year is numeric\n\n[1] TRUE\n\nis.numeric(storms$category)  # category is also numeric\n\n[1] TRUE\n\nis.numeric(storms$name)  # name is not numeric\n\n[1] FALSE\n\nis.character(storms$name)  # name is character string\n\n[1] TRUE\n\n\n\nis.numeric(storms$status)  # status is not numeric\n\n[1] FALSE\n\nis.character(storms$status)  # status is not a character\n\n[1] FALSE\n\nis.factor(storms$status)  # status is a factor which is categorical\n\n[1] TRUE"
  },
  {
    "objectID": "02-EDA-Categorical.html#converting-decimals-to-integers",
    "href": "02-EDA-Categorical.html#converting-decimals-to-integers",
    "title": "1.2: Exploring Categorical Data",
    "section": "Converting Decimals to Integers",
    "text": "Converting Decimals to Integers\n\nFrom the summary of the storms data set we first found above, we see that the variables year and month are being stored as double. These variables actually are integer values.\nWe can convert another variable of one format into another format using as.[new_datatype]()\n\nFor example, to convert to year to integer, we use as.integer(storms$year).\nTo convert a data type to character, we can use as.character(x).\nTo convert to a decimal (double), we can use as.numeric(x)\n\n\ntypeof(storms$year)\n\n[1] \"double\"\n\ntypeof(storms$month)\n\n[1] \"double\"\n\nstorms$year &lt;- as.integer(storms$year)\nstorms$month &lt;- as.integer(storms$month)\ntypeof(storms$year)\n\n[1] \"integer\"\n\ntypeof(storms$month)\n\n[1] \"integer\""
  },
  {
    "objectID": "02-EDA-Categorical.html#converting-to-categorical-data-with-factor",
    "href": "02-EDA-Categorical.html#converting-to-categorical-data-with-factor",
    "title": "1.2: Exploring Categorical Data",
    "section": "Converting to Categorical Data with factor()",
    "text": "Converting to Categorical Data with factor()\n\nSometimes we think a variable is one data type, but it is actually being stored (and thus interpreted by R) as a different data type. One common issue is categorical data is stored as characters or integers. We would like observations with the same values to be group together.\n\nThe status variable in storms is being properly stored as a factor.\nThe category variable in storms is being stored as a numeric since the classes are integers.\n\n\nsummary(storms$category)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   1.000   1.898   3.000   5.000   14382 \n\n\nThe summary of category computes statistics such as mean and median. Typically with categorical data, we prefer to count how many observations are in each class of the variable.\n\nIn the code cell below, we convert category to a factor, and then observe the resulting summary.\n\n\nstorms$category &lt;- factor(storms$category)\nsummary(storms$category)\n\n    1     2     3     4     5  NA's \n 2478   973   579   539   115 14382"
  },
  {
    "objectID": "02-EDA-Categorical.html#assignment-of-new-objects",
    "href": "02-EDA-Categorical.html#assignment-of-new-objects",
    "title": "1.2: Exploring Categorical Data",
    "section": "Assignment of New Objects",
    "text": "Assignment of New Objects\n\nNotice in the code cell above, we replace the original integers in the category column with a integers that are now stored as different levels of a categorical variable.\n\nTo store a data structure in the computer’s memory we must assign it a name.\n\nIn this case, we choose to give the variable the same name, an therefore overwrite the original category column.\n\nData structures can be stored using the assignment operator &lt;-."
  },
  {
    "objectID": "02-EDA-Categorical.html#question-3",
    "href": "02-EDA-Categorical.html#question-3",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 3",
    "text": "Question 3\n\n\nWhich month over the period from 1975-2021 had the greatest number of subtropical storms? Which table did you use to help answer your question?\nWhich month over the period from 1975-2021 had the greatest proportion of subtropical storms? Which table did you use to help answer your question?\n\nRun each of the four code cells below, and after interpreting the output of each, answer the questions in the space below.\n\nmy.table &lt;- table(storms$month, storms$status)  # gives counts\nmy.table\n\n    \n     disturbance extratropical hurricane other low subtropical depression\n  1            0            29         5         5                      0\n  4            0            40         0         0                      4\n  5            0            18         0        49                      5\n  6           13           130        18        82                     35\n  7           45           135       202       175                     11\n  8           25           275      1038       317                     36\n  9           41           732      2380       446                     34\n  10          14           520       799       219                     22\n  11           8           175       209        81                      4\n  12           0            14        33        31                      0\n    \n     subtropical storm tropical depression tropical storm tropical wave\n  1                  6                   2             23             0\n  4                  3                   1             18             0\n  5                 20                  49             60             0\n  6                 12                 213            276             0\n  7                  6                 397            625             7\n  8                 23                 975           1696            55\n  9                 72                1315           2448            41\n  10                66                 413           1024             0\n  11                42                 139            443             8\n  12                42                  21             71             0\n\n\n\nprop.grand &lt;- prop.table(my.table)  # gives proportions relative to grand total\nround(prop.grand, 4)\n\n    \n     disturbance extratropical hurricane other low subtropical depression\n  1       0.0000        0.0015    0.0003    0.0003                 0.0000\n  4       0.0000        0.0021    0.0000    0.0000                 0.0002\n  5       0.0000        0.0009    0.0000    0.0026                 0.0003\n  6       0.0007        0.0068    0.0009    0.0043                 0.0018\n  7       0.0024        0.0071    0.0106    0.0092                 0.0006\n  8       0.0013        0.0144    0.0544    0.0166                 0.0019\n  9       0.0022        0.0384    0.1248    0.0234                 0.0018\n  10      0.0007        0.0273    0.0419    0.0115                 0.0012\n  11      0.0004        0.0092    0.0110    0.0042                 0.0002\n  12      0.0000        0.0007    0.0017    0.0016                 0.0000\n    \n     subtropical storm tropical depression tropical storm tropical wave\n  1             0.0003              0.0001         0.0012        0.0000\n  4             0.0002              0.0001         0.0009        0.0000\n  5             0.0010              0.0026         0.0031        0.0000\n  6             0.0006              0.0112         0.0145        0.0000\n  7             0.0003              0.0208         0.0328        0.0004\n  8             0.0012              0.0511         0.0890        0.0029\n  9             0.0038              0.0690         0.1284        0.0022\n  10            0.0035              0.0217         0.0537        0.0000\n  11            0.0022              0.0073         0.0232        0.0004\n  12            0.0022              0.0011         0.0037        0.0000\n\n\n\nprop.row &lt;- prop.table(my.table, 1)  # gives proportions relative to row totals\nround(prop.row, 4)\n\n    \n     disturbance extratropical hurricane other low subtropical depression\n  1       0.0000        0.4143    0.0714    0.0714                 0.0000\n  4       0.0000        0.6061    0.0000    0.0000                 0.0606\n  5       0.0000        0.0896    0.0000    0.2438                 0.0249\n  6       0.0167        0.1669    0.0231    0.1053                 0.0449\n  7       0.0281        0.0842    0.1260    0.1092                 0.0069\n  8       0.0056        0.0619    0.2338    0.0714                 0.0081\n  9       0.0055        0.0975    0.3170    0.0594                 0.0045\n  10      0.0045        0.1690    0.2597    0.0712                 0.0071\n  11      0.0072        0.1578    0.1885    0.0730                 0.0036\n  12      0.0000        0.0660    0.1557    0.1462                 0.0000\n    \n     subtropical storm tropical depression tropical storm tropical wave\n  1             0.0857              0.0286         0.3286        0.0000\n  4             0.0455              0.0152         0.2727        0.0000\n  5             0.0995              0.2438         0.2985        0.0000\n  6             0.0154              0.2734         0.3543        0.0000\n  7             0.0037              0.2477         0.3899        0.0044\n  8             0.0052              0.2196         0.3820        0.0124\n  9             0.0096              0.1751         0.3260        0.0055\n  10            0.0214              0.1342         0.3328        0.0000\n  11            0.0379              0.1253         0.3995        0.0072\n  12            0.1981              0.0991         0.3349        0.0000\n\n\n\nprop.col &lt;- prop.table(my.table, 2)  # gives proportions relative to column totals\nround(prop.col, 4)\n\n    \n     disturbance extratropical hurricane other low subtropical depression\n  1       0.0000        0.0140    0.0011    0.0036                 0.0000\n  4       0.0000        0.0193    0.0000    0.0000                 0.0265\n  5       0.0000        0.0087    0.0000    0.0349                 0.0331\n  6       0.0890        0.0629    0.0038    0.0584                 0.2318\n  7       0.3082        0.0653    0.0431    0.1246                 0.0728\n  8       0.1712        0.1330    0.2216    0.2256                 0.2384\n  9       0.2808        0.3540    0.5081    0.3174                 0.2252\n  10      0.0959        0.2515    0.1706    0.1559                 0.1457\n  11      0.0548        0.0846    0.0446    0.0577                 0.0265\n  12      0.0000        0.0068    0.0070    0.0221                 0.0000\n    \n     subtropical storm tropical depression tropical storm tropical wave\n  1             0.0205              0.0006         0.0034        0.0000\n  4             0.0103              0.0003         0.0027        0.0000\n  5             0.0685              0.0139         0.0090        0.0000\n  6             0.0411              0.0604         0.0413        0.0000\n  7             0.0205              0.1126         0.0935        0.0631\n  8             0.0788              0.2766         0.2537        0.4955\n  9             0.2466              0.3730         0.3662        0.3694\n  10            0.2260              0.1172         0.1532        0.0000\n  11            0.1438              0.0394         0.0663        0.0721\n  12            0.1438              0.0060         0.0106        0.0000\n\n\n\nSolution to Question 3"
  },
  {
    "objectID": "02-EDA-Categorical.html#creating-bar-charts-with-plot",
    "href": "02-EDA-Categorical.html#creating-bar-charts-with-plot",
    "title": "1.2: Exploring Categorical Data",
    "section": "Creating Bar Charts with plot()",
    "text": "Creating Bar Charts with plot()\n\n\nThe plot() function is the most broadly used function for plotting different data types.\nThe type of plot generated by plot() depends on the data type(s) and the number of variable(s) we input.\n\nIf we input one categorical variable that is stored as factor, we get a bar chart.\nIf we input one quantitative variable, the plot is typically not very useful.\nIf a categorical variable is stored incorrectly as quantitative, we do not get an appropriate plot.\n\nIf we use plot() to display a categorical variable, the variable must be stored as a factor!\n\nRecall we converted category to a factor in a previous code cell.\nThe variable month is a quantitative variable.\n\n\n\n# plots appear in an array with 1 row and 2 columns\npar(mfrow = c(1, 2))  # create an array of plots\n\nplot(storms$category,  # categorical data\n     main = \"Hurricanes by Category\",  # main title\n     xlab = \"Hurricane Category\",  # horizontal axis label\n     ylab = \"Frequency\",  # vertical axis label\n     col = \"steelblue\")  # fill color of bars)\n\nplot(storms$month,  # quantitative data\n     main = \"Not Number of Storms in Month\",  # main title\n     xlab = \"Index (Row of Observation)\",  # horizontal axis label\n     ylab = \"Month\")  # vertical axis label"
  },
  {
    "objectID": "02-EDA-Categorical.html#creating-bar-charts-and-pie-charts-from-a-table",
    "href": "02-EDA-Categorical.html#creating-bar-charts-and-pie-charts-from-a-table",
    "title": "1.2: Exploring Categorical Data",
    "section": "Creating Bar Charts and Pie Charts from a Table",
    "text": "Creating Bar Charts and Pie Charts from a Table\n\nIf we want to keep month stored as an integer, but would like to create a visualization to display the number of storms that occurred in each month, we can:\n\nFirst use the table() function to count how many storms occurred in each month.\nThen create a bar chart using the barplot() function or pie chart using pie().\n\n\n# plots appear in an array with 1 row and 2 columns\npar(mfrow = c(1, 2))  # create an array of plots\n\nmonth.table &lt;- table(storms$month)  # create table of month counts\ncategory.table &lt;- table(storms$category)  # create table of category counts\n\nbarplot(month.table,  # input table of month counts\n        main = \"Storms in Each Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        ylab = \"Frequency\",  # vertical axis label\n        col = \"seagreen\")  # fill color of bars\n\npie(category.table,  # input table of category counts\n    main = \"Hurricanes by Category\")  # main title\n\n\n\n\nNote, pie() and barplot() both take tables as inputs. Even if a variable is stored as a factor, we need to store the counts in a table first.\n\nThe category variable in storms is stored as a factor, but the code below still crashes.\n\n\n# see what happens if input is not a table\npie(storms$category)"
  },
  {
    "objectID": "02-EDA-Categorical.html#grouped-frequency-bar-charts",
    "href": "02-EDA-Categorical.html#grouped-frequency-bar-charts",
    "title": "1.2: Exploring Categorical Data",
    "section": "Grouped Frequency Bar Charts",
    "text": "Grouped Frequency Bar Charts\n\nTo create a bar chart displaying the number of category hurricanes that occurred in each month:\n\nFirst create a two-way table of counts.\n\nThe second variable (month) is displayed on horizontal axis.\nWe get a separate bar for each level of the first variable (category).\n\nInput the table into the barplot() function.\n\nNote the option beside = TRUE groups the bars for each month.\nThe default option beside = FALSE stacks the bars.\n\n\n\n# two-way table of counts of category in each month\ncat.table &lt;- table(storms$category, storms$month)  # gives counts\n\n\n# create a vector of colors\nmy.colors &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create side by side bar chart\nbarplot(cat.table,  # use counts from contingency table\n        beside = TRUE,  # groups side-by-side\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors,  # fill color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add a legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(cat.table),  # get labels from row name in contingency table\n       fill = my.colors)  # use same fill colors"
  },
  {
    "objectID": "02-EDA-Categorical.html#stacked-relative-frequency-bar-charts",
    "href": "02-EDA-Categorical.html#stacked-relative-frequency-bar-charts",
    "title": "1.2: Exploring Categorical Data",
    "section": "Stacked Relative Frequency Bar Charts",
    "text": "Stacked Relative Frequency Bar Charts\n\nTo create a bar chart displaying the relative frequency (or proportion) of category hurricanes that occurred in each month:\n\nFirst create a two-way table of relative frequencies.\n\nPay attention to whether you want the proportions relative to grand, row, or column totals.\n\nInput the table into the barplot() function.\n\nThe default option beside = FALSE stacks the bars.\n\n\n\ncat.grand &lt;- prop.table(cat.table)  # gives proportions relative to grand total\ncat.row &lt;- prop.table(cat.table, 1)  # gives proportions relative to row totals\ncat.col &lt;- prop.table(cat.table, 2)  # gives proportions relative to column totals\n\n\npar(mfrow = c(1, 3))  # create an array of plots\n\n# create stacked bar chart 1\nbarplot(cat.grand,  # use proportions from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors,  # color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(cat.grand),  # get labels\n       fill = my.colors)  # use same colors\n\n##########\n\n# create stacked bar chart 2\nbarplot(cat.row,  # use proportions from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors,  # color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(cat.row),  # get labels\n       fill = my.colors)  # use same colors\n\n###########\n\n# create stacked bar chart 3\nbarplot(cat.col,  # use proportions from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors,  # color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(cat.col),  # get labels\n       fill = my.colors)  # use same colors"
  },
  {
    "objectID": "02-EDA-Categorical.html#question-4",
    "href": "02-EDA-Categorical.html#question-4",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 4",
    "text": "Question 4\n\nBased on the three plots generated in the previous code cell, answer the questions below.\n\nWhich month has the most hurricanes?\nIn which month is the proportion of category 1 hurricanes greatest?\n\n\nSolution to Question 4"
  },
  {
    "objectID": "02-EDA-Categorical.html#question-5",
    "href": "02-EDA-Categorical.html#question-5",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 5",
    "text": "Question 5\n\nWhat are the the differences in the three plots in the output of the previous code cell? Which of the three bar plots above do you believe best visualizes the occurrence of different category hurricanes by month? Which plot do you think is the least useful overall? Explain why.\n\nSolution to Question 5"
  },
  {
    "objectID": "02-EDA-Categorical.html#question-6",
    "href": "02-EDA-Categorical.html#question-6",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 6",
    "text": "Question 6\n\nCreate a plot to visualize one categorical variable in the gss_cat data set. Based on your plot, comment on any interesting features of the variable you plotted.\n\n# create a visualization for one categorical variable\n\n\nSolution to Question 6"
  },
  {
    "objectID": "02-EDA-Categorical.html#question-7",
    "href": "02-EDA-Categorical.html#question-7",
    "title": "1.2: Exploring Categorical Data",
    "section": "Question 7",
    "text": "Question 7\n\nCreate a plot to visualize the relationship between two categorical variables in the gss_cat data set. Based on your plot, comment on any interesting features or relations between two the variables you plotted.\n\n# create a visualization to illustrate the relation\n# between two categorical variables\n\n\nSolution to Question 7\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "03-EDA-Quantitative.html",
    "href": "03-EDA-Quantitative.html",
    "title": "1.3: Exploring Quantitative Data",
    "section": "",
    "text": "Types of Variables\nIn statistics, variables are the attributes measured or collected in data. We refer to them as variables since the values or classes of attributes typically vary from observation to observation. The term variable is used differently in statistics from the notion of a variable in algebra. There are two types of variables in statistics:\nThe type of statistical analysis we can do depends on whether:\nIn our work with Exploring Categorical Data, we performed an initial summary of the categorical variables in the storms data set. Today, we will investigate how to numerically and visually summarize quantitative variables.\nThe dplyr package contains a data set from the NOAA Hurricane Best Track Data that contains data on the following attributes of tracked North Atlantic storms since 1975:\nWhen we analyze the categorical variables in storms, we use counts and proportions. In the table created by the code cell below, we see how many observations there are in each storm classification.\ntbl.status &lt;- table(storms$status) # store counts for each storm classification\ntbl.status  # print table to screen\n\n\n           disturbance          extratropical              hurricane \n                   146                   2068                   4684 \n             other low subtropical depression      subtropical storm \n                  1405                    151                    292 \n   tropical depression         tropical storm          tropical wave \n                  3525                   6684                    111\nThe code cell below gives the proportion of storms in the data are in each storm classification.\n# table of counts for each storm classification\nprop.table(tbl.status)\n\n\n           disturbance          extratropical              hurricane \n           0.007657610            0.108465331            0.245672926 \n             other low subtropical depression      subtropical storm \n           0.073691388            0.007919857            0.015315221 \n   tropical depression         tropical storm          tropical wave \n           0.184884087            0.350571698            0.005821882\nWe used bar charts and pie charts to visualize the distribution and relations between categorical variables.\nplot(storms$category,  # categorical data\n     main = \"Hurricanes by Category\",  # main title\n     xlab = \"Hurricane Category\",  # horizontal axis label\n     ylab = \"Frequency\",  # vertical axis label\n     col = \"steelblue\")  # fill color of bars)\nAdditional resources for help with plotting data:\nTypical measurements of center are:\n\\[{\\large \\bar{x} = \\frac{\\mbox{sum of all values}}{\\mbox{total number of values}} =  \\sum_{i=1}^{n} \\frac{x_n}{n}}. \\]\nTypical measurements of spread are:\nA question we often wish to explore is what proportion of values in our data are less or equal to a specified value \\(x\\)? To answer this question, we count the total number of observations in our data that are less than or equal to \\(x\\), and then divide by the total number of observations in our data.\nWe have explored some of the categorical variables in the storms data set in our work with Exploring Categorical Data. We have discussed how we can summarize and plot a quantitative variable. Often in statistics we would like to compare the distribution of a quantitative variable for different classes of a categorical variable. For example, we may be interested in investigating the following:\nWe first check the data type of the month variable in storms using the typeof() function.\ntypeof(storms$month)  # check how months is stored\n\n[1] \"integer\"\nTo store a data structure in the computer’s memory we must assign it a name.\nData structures can be stored using the assignment operator &lt;- or =.\nSome comments:"
  },
  {
    "objectID": "03-EDA-Quantitative.html#loading-required-package",
    "href": "03-EDA-Quantitative.html#loading-required-package",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Loading Required Package",
    "text": "Loading Required Package\n\nIn order to access the storms data frame in the dplyr package, we first load the package with the library() function.\n\nlibrary(dplyr)  # load dplyr package"
  },
  {
    "objectID": "03-EDA-Quantitative.html#sec-03help",
    "href": "03-EDA-Quantitative.html#sec-03help",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Help Documentation for storms",
    "text": "Help Documentation for storms\n\nThe ? help operator and help() function provide access to the help manuals for R functions, data sets, and other objects. If at any point we want to learn more about data or a function used in this notebook, we can use the help operator. For example, ?typeof, ?str, ?hist, and ?boxplot will open a help tab with further details about each of function.\n\nRun the code cell below to access the help documentation for the storms data set.\n\n\n?storms  # open help tab"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-1",
    "href": "03-EDA-Quantitative.html#question-1",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 1",
    "text": "Question 1\n\nList all the quantitative variables in storms. Which are being stored as integer, and which are stored as double (decimals)?\n\nYou can edit, run and rerun the typeof() function in the first code cell below to help identify the data types of individual variables in storms.\nYou can use the str() function in the second code cell to identify the data types of all variables at once.\n\n\ntypeof(storms$year)\n\n[1] \"double\"\n\n\n\nSolution to Question 1"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-2",
    "href": "03-EDA-Quantitative.html#question-2",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 2",
    "text": "Question 2\n\nWhat wind speeds are classified as a Category 2 hurricane?\n\nSolution to Question 2"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-3",
    "href": "03-EDA-Quantitative.html#question-3",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 3",
    "text": "Question 3\n\nWhat does the variable tropicalstorm_force_diameter measure? What does it mean if a storm observation has a 0 for tropicalstorm_force_diameter?\n\nSolution to Question 3"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-4",
    "href": "03-EDA-Quantitative.html#question-4",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 4",
    "text": "Question 4\n\nEnter comments in the code cell below to help describe what each command performs. Then run the str() function after running the commands to see the updated data structure of storms.\n\nSolution to Question 4\n\n\n# enter your comments after each # \nstorms$year &lt;- as.integer(storms$year)  #\nstorms$month &lt;- as.integer(storms$month)  #\nstorms$hour &lt;- as.integer(storms$hour)  #\nstorms$category &lt;- factor(storms$category)  #\n\n\n# view the resulting data structure\nstr(storms)\n\ntibble [19,066 × 13] (S3: tbl_df/tbl/data.frame)\n $ name                        : chr [1:19066] \"Amy\" \"Amy\" \"Amy\" \"Amy\" ...\n $ year                        : int [1:19066] 1975 1975 1975 1975 1975 1975 1975 1975 1975 1975 ...\n $ month                       : int [1:19066] 6 6 6 6 6 6 6 6 6 6 ...\n $ day                         : int [1:19066] 27 27 27 27 28 28 28 28 29 29 ...\n $ hour                        : int [1:19066] 0 6 12 18 0 6 12 18 0 6 ...\n $ lat                         : num [1:19066] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ...\n $ long                        : num [1:19066] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ...\n $ status                      : Factor w/ 9 levels \"disturbance\",..: 7 7 7 7 7 7 7 7 8 8 ...\n $ category                    : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ wind                        : int [1:19066] 25 25 25 25 25 25 25 30 35 40 ...\n $ pressure                    : int [1:19066] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ...\n $ tropicalstorm_force_diameter: int [1:19066] NA NA NA NA NA NA NA NA NA NA ...\n $ hurricane_force_diameter    : int [1:19066] NA NA NA NA NA NA NA NA NA NA ..."
  },
  {
    "objectID": "03-EDA-Quantitative.html#histograms",
    "href": "03-EDA-Quantitative.html#histograms",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is special bar chart we use to display the distribution of values for a quantitative variable.\n\nWe first group the values into different ranges of values called bins of equal width.\n\nThis essentially converts the quantitative variable to an ordinal categorical variable with with each bin representing a different level.\nConsider the quantitative variable wind. We can use bin ranges such as 0-10 knots, 10-20 knots, … , 160-170 knots.\n\nEach bin range should have the same width.\nThe bins do not overlap.\nThe ordering of the bins is very important.\n\n\nThen we count how many values in the data are in each bin.\nA histogram is a bar chart that represents the number of values that are in each bin range.\nValues of the quantitative variable are measured on the horizontal axis.\nThe height of the bars over each bin range is the number of values (or frequency) in each bin range.\nBy default, the counts are right closed. For example, a wind value of 20 knots would be counted in the bin range 10-20 knots and not counted in the bin range 20-30 knots.\nA histogram should not have an spaces between consecutive bars. Empty space means no values are in that bin range.\nThe R function hist(x, [options]) creates a histogram.\nRun ?hist for more information about the available options for customizing a histogram, some of which are illustrated in the code cell below.\n\n\n# create a histogram\nhist(storms$wind,  # vector of values to plot\n     breaks = 15,  # number of bin ranges to use\n     xlab = \"wind speed (in knots)\",   # x-axis label\n     xlim = c(0,200),  # sets window for x-axis\n     ylab = \"Frequency\",  # y-axis label\n     ylim = c(0,5000),  # sets window for y-axis\n     main = \"Distribution of Storm Wind Speed\",  # main label\n     col = \"steelblue\")  # fill color of bars"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-5",
    "href": "03-EDA-Quantitative.html#question-5",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 5",
    "text": "Question 5\n\nBased on the histogram above, approximately how many storms have a wind speed less than or equal to 40 knots?\n\nSolution to Question 5"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-6",
    "href": "03-EDA-Quantitative.html#question-6",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 6",
    "text": "Question 6\n\nThe code cell below can help us check our answer.\n\nExplain what operation(s) the command in the code cell below. Running the code cell and compare the last 10 entries in the vector le.40 and the vector storms$wind to help determine your answer.\nThen run and explain what the second code cell below does. Hint: R reads the logical TRUE as the number 1 and FALSE as the number 0.\nHow accurate was your previous answer in Question 5?\n\n\nSolution to Question 6\n\n\nEnter comment in first code cell.\nEnter comment in second code cell.\nHow accurate was your answer in Question 5?\n\n\nle.40 &lt;- storms$wind &lt;= 40  # ??\n\ntail(storms$wind, 10)  # prints last 10 rows of wind speed vector\n\n [1] 45 45 45 40 35 35 35 35 40 40\n\ntail(le.40, 10)  # prints last 10 rows of logical vector le.40\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n# enter comment to interpret this command\nsum(le.40)  # ??\n\n[1] 9288\n\n\n\n\nChanging the Number of Bins\n\nA histogram can illustrate the general shape of the distribution of quantitative variable; however, the number of breaks we use can have a substantial impact.\n\nIf we include too few bins, we do not get much detail, and we may even get a misleading picture.\nIf we include too many bins, the histogram may be difficult to read.\nThe fun of interacting with data in R is we can play around and adjust the number of breaks and other options until we are satisfied.\n\n\n# create a histogram\nhist(storms$wind,  # vector of values to plot\n     breaks = 5,  # number of bin ranges to use\n     xlab = \"wind speed (in knots)\",   # x-axis label\n     xlim = c(0,200),  # sets window for x-axis\n     ylab = \"Frequency\",  # y-axis label\n     ylim = c(0,15000),  # sets window for y-axis\n     main = \"Storm Wind Speed (breaks = 5)\",  # main label\n     cex.lab=1.5, cex.axis=1.5, cex.main=1.5,  # increase font size\n     col = \"steelblue\")  # fill color of bars\n\n# create a histogram\nhist(storms$wind,  # vector of values to plot\n     breaks = 50,  # number of bin ranges to use\n     xlab = \"wind speed (in knots)\",   # x-axis label\n     xlim = c(0,200),  # sets window for x-axis\n     ylab = \"Frequency\",  # y-axis label\n     ylim = c(0,3000),  # sets window for y-axis\n     main = \"Storm Wind Speed (breaks = 50)\",  # main label\n     cex.lab=1.5, cex.axis=1.5, cex.main=1.5,  # increase font size\n     col = \"seagreen\")  # fill color of bars\n\n\n\n\n\n\n\n(a) Histogram with 5 Breaks (4 bins)\n\n\n\n\n\n\n\n(b) Histogram with 50 Breaks (49 bins)\n\n\n\n\nFigure 7.1: Changing the Breaks: Histograms of Wind Speed"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-7",
    "href": "03-EDA-Quantitative.html#question-7",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 7",
    "text": "Question 7\n\nHow would you describe the shape of the distribution of wind speed in the histograms above?\n\nSolution to Question 7"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-8",
    "href": "03-EDA-Quantitative.html#question-8",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 8",
    "text": "Question 8\n\nCreate a histogram to display the quantitative variable month. What does the shape of that graph tell you about the data?\n\nSolution to Question 8"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-9",
    "href": "03-EDA-Quantitative.html#question-9",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 9",
    "text": "Question 9\n\nCreate a histogram to display the quantitative variable long. What does the shape of that graph tell you about the data?\n\nSolution to Question 9"
  },
  {
    "objectID": "03-EDA-Quantitative.html#the-skewness-of-data",
    "href": "03-EDA-Quantitative.html#the-skewness-of-data",
    "title": "1.3: Exploring Quantitative Data",
    "section": "The Skewness of Data",
    "text": "The Skewness of Data\n\nThe skewness of the data describes the direction of the tail of the data. The tail of the data indicates the direction of outliers (if any).\n\n#par(mfrow = c(1, 3))  # Create a 1 x 3 array of plots\n\nhist(storms$wind, \n     xlab = \"wind speed (in knots)\",   # x-axis label\n     ylab = \"Frequency\",  # y-axis label\n     main = \"Distribution of Wind Speeds\",  # main title\n     cex.lab=1.7, cex.axis=1.7, cex.main=1.7,  # increase font size\n     col = \"steelblue\")  # fill color of bars\n\nhist(storms$month, \n     breaks = 12,  # number of breaks\n     xlab=\"Month\",   # x-axis label\n     ylab = \"Frequency\",  # y-axis label\n     main = \"Distribution of Months\",  # main title\n     cex.lab=1.7, cex.axis=1.7, cex.main=1.7,  # increase font size\n     col = \"coral1\")  # fill color of bars\n\nhist(storms$long, \n     breaks = 15,  # number of breaks\n     xlab=\"Degrees of Longitude\",   # x-axis label\n     ylab = \"Frequency\",  # y-axis label\n     main = \"Distribution of Longitude\",  # main title\n     cex.lab=1.7, cex.axis=1.7, cex.main=1.7,  # increase font size\n     col = \"aquamarine4\")  # fill color of bars\n\n\n\n\n\n\n\n(a) Skewed Right\n\n\n\n\n\n\n\n(b) Skewed Left\n\n\n\n\n\n\n\n(c) Approximately Symmetric\n\n\n\n\nFigure 7.2: Comparing the Shapes of Distributions\n\n\n\n\nThe distribution of wind speeds is skewed right.\nThe distribution of months is skewed left.\nThe distribution of longitude is approximately symmetric."
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-10",
    "href": "03-EDA-Quantitative.html#question-10",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 10",
    "text": "Question 10\n\nCompute the mean and median wind speed of the storms data. Interpret each value in practical terms. Be sure to include the units in your interpretation.\n\n\n\n\n\n\nTip\n\n\n\nWe can input the vector of wind speeds with the code storms$wind.\n\n\n\nSolution to Question 10"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-11",
    "href": "03-EDA-Quantitative.html#question-11",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 11",
    "text": "Question 11\n\nWhy do you think the mean wind speed is greater than the median wind speed?\n\nSolution to Question 11"
  },
  {
    "objectID": "03-EDA-Quantitative.html#relation-of-shape-to-measurements-of-center",
    "href": "03-EDA-Quantitative.html#relation-of-shape-to-measurements-of-center",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Relation of Shape to Measurements of Center",
    "text": "Relation of Shape to Measurements of Center\n\n\n\n\nImage Credit: Adam Spiegler, CC BY-SA 4.0.\n\n\n\nThe mean is more sensitive to outliers than the median. The mean is pulled in the direction of the tail.\nIf the shape of the histogram is symmetric, then the mean is equal to the median.\nIf the shape of a histogram is skewed to the left, the mean is less than the median.\nIf the shape of a histogram is skewed to the right, the mean is greater than the median."
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-12",
    "href": "03-EDA-Quantitative.html#question-12",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 12",
    "text": "Question 12\n\nWhich of the histograms (i)-(vi) has the largest range? The smallest range?\n\nSolution to Question 12"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-13",
    "href": "03-EDA-Quantitative.html#question-13",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 13",
    "text": "Question 13\n\nWhich of the histograms (i)-(vi) has the largest standard deviation? The smallest standard deviation?\n\nSolution to Question 13"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-14",
    "href": "03-EDA-Quantitative.html#question-14",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 14",
    "text": "Question 14\n\nGive the five number summary for the wind speed of all observations in the storms data set.\n\nSolution to Question 14"
  },
  {
    "objectID": "03-EDA-Quantitative.html#five-number-summaries-and-boxplots",
    "href": "03-EDA-Quantitative.html#five-number-summaries-and-boxplots",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Five Number Summaries and Boxplots",
    "text": "Five Number Summaries and Boxplots\n\nThe five number summary for wind speeds is \\((10, 30, 45, 65, 165)\\). Below is a boxplot for this data.\n\n25% of the wind speeds are between 10 and 30 knots.\n25% of the wind speeds are between 30 and 45 knots.\n25% of the wind speeds are between 45 and 65 knots.\n25% of the wind speeds are between 65 and 165 knots.\n\n\nboxplot(storms$wind,  # data to plot\n        main = \"Wind Speeds of Storms\",  # main title \n        xlab = \"Wind Speed (in knots)\",  # x-axis label\n        xaxt='n',  # turn off default ticks on x-axis\n        horizontal = TRUE)  # align horizontally\naxis(1, at = fivenum(storms$wind))  # add tickmarks at five number summary"
  },
  {
    "objectID": "03-EDA-Quantitative.html#how-to-read-and-create-boxplots",
    "href": "03-EDA-Quantitative.html#how-to-read-and-create-boxplots",
    "title": "1.3: Exploring Quantitative Data",
    "section": "How to Read and Create Boxplots",
    "text": "How to Read and Create Boxplots\n\nTo create a boxplot:\n\nFind the values of \\(Q_1\\), median, and \\(Q_3\\).\nDraw a box with edges at \\(Q_1\\) and \\(Q_3\\) and line inside the box for the median.\nIdentify the upper and lower fence to classify outliers:\n\nUpper fence \\(=Q_3 + 1.5(\\mbox{IQR})\\).\nLower fence \\(=Q_1 - 1.5(\\mbox{IQR})\\).\n\nExtend a line (whisker) from the lower edge of box to the smallest observation greater than the lower fence.\nExtend a line (whisker) from the upper edge of the box to the largest value that is less than the upper fence.\nThe observations that are less than the lower fence or greater than the upper fence are considered outliers.\n\nOutlier values are marked with individual points."
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-15",
    "href": "03-EDA-Quantitative.html#question-15",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 15",
    "text": "Question 15\n\nCompute the upper and lower fences for the wind speed observations in storms.\n\nSolution to Question 15"
  },
  {
    "objectID": "03-EDA-Quantitative.html#counting-observations-with-logical-statements",
    "href": "03-EDA-Quantitative.html#counting-observations-with-logical-statements",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Counting Observations with Logical Statements",
    "text": "Counting Observations with Logical Statements\n\nTo illustrate how we can count observations that satisfy a given condition, consider the a vector of 5 values: \\(31\\), \\(33\\), \\(34\\), \\(36\\), and \\(38\\). We store these values in the vector named test.data below. The command test.data &lt;= 35 applies a logical test to each of the 5 values in the vector:\n\nIs the value less than or equal to 35?\n\nRun the code cell below and check the output to verify the test works as expected.\n\ntest.data &lt;- c(31, 33, 34, 36, 38)  # vector of test data\ntest.data &lt;= 35  # logical test\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\n\nThe result TRUE is counted as 1.\nThe result FALSE is counted as 0.\nWe can use the sum() function to count how many TRUE results we have.\nRunning the code cell below, we verify that 3 values in test.data are less than or equal to 35.\n\n\nsum(test.data &lt;= 35)  # sum the TRUE results\n\n[1] 3\n\n\nWe can convert the count to a proportion by dividing by the total number of values in our data. Our vector test.data has a total of 5 observations; therefore, the proportion of values that are less than or equal to 35 is 3 out of 5 or \\(0.6\\). We can use the mean() to count the number of TRUE results and divide by the total number of all observations in one command to simplify the code.\n\nmean(test.data &lt;= 35)  # total values &lt;= 35 divided by total number of values\n\n[1] 0.6"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-16",
    "href": "03-EDA-Quantitative.html#question-16",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 16",
    "text": "Question 16\n\nWhat proportion of observations in storms$wind have a wind speed less than or equal to 50 knots?\n\nSolution to Question 16\n\n\n# what proportion of observations have wind less than or equal to 50"
  },
  {
    "objectID": "03-EDA-Quantitative.html#what-is-the-empirical-cumulative-distribution-function",
    "href": "03-EDA-Quantitative.html#what-is-the-empirical-cumulative-distribution-function",
    "title": "1.3: Exploring Quantitative Data",
    "section": "What is the Empirical Cumulative Distribution Function?",
    "text": "What is the Empirical Cumulative Distribution Function?\n\nThe empirical cumulative distribution function (ecdf) is typically denoted by the notation \\(\\mathbf{\\color{dodgerblue}{\\widehat{F}(x)}}\\). We read the notation \\(\\hat{F}\\) as F hat, and we will make use of the hat notation throughout the semester.\n\nThe input \\(x\\) is a value.\nThe output \\(\\widehat{F}(x)\\) of the ecdf is the proportion of values in the sample that are less than or equal to \\(x\\).\n\nRecall the vector test.data contains the values \\(31\\), \\(33\\), \\(34\\), \\(36\\), and \\(38\\). We can express the ecdf as a piecewise function.\n\\[\n\\widehat{F}(x) = \\left\\{\n\\begin{array}{ll}\n0  & x &lt; 31 \\\\\n0.2 &  31 \\leq x &lt; 33 \\\\\n0.4 &  33 \\leq x &lt; 34 \\\\\n0.6 &  34 \\leq x &lt; 36 \\\\\n0.8 &  36 \\leq x &lt; 38 \\\\\n1 & x \\geq 38\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "03-EDA-Quantitative.html#graphing-the-empirical-cumulative-distribution-function",
    "href": "03-EDA-Quantitative.html#graphing-the-empirical-cumulative-distribution-function",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Graphing the Empirical Cumulative Distribution Function",
    "text": "Graphing the Empirical Cumulative Distribution Function\n\nWe can plot the ecdf using the plot.ecdf() function in R, and the resulting plot is a piecewise, step function.\n\nplot.ecdf(test.data, col=\"steelblue\")"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-17",
    "href": "03-EDA-Quantitative.html#question-17",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 17",
    "text": "Question 17\n\nComplete the statements below to identify some key properties of ecdf’s.\n\nSolution to Question 17\n\n\nThe minimum output value of an ecdf is ??.\nThe maximum value output value of an ecdf is ??.\nThe ecdf is a ?? function since as \\(x\\) increases, \\(\\widehat{F}(x)\\) cannot decrease."
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-18",
    "href": "03-EDA-Quantitative.html#question-18",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 18",
    "text": "Question 18\n\nPlot the empirical cumulative distribution function for the wind speeds in the storms data set and check your answer to Question 16.\n\nSolution to Question 18\n\n\n# plot the ecdf for wind speeds in storms"
  },
  {
    "objectID": "03-EDA-Quantitative.html#converting-a-quantitative-variable-to-a-categorical-variable-with-factor",
    "href": "03-EDA-Quantitative.html#converting-a-quantitative-variable-to-a-categorical-variable-with-factor",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Converting a Quantitative Variable to a Categorical Variable with factor()",
    "text": "Converting a Quantitative Variable to a Categorical Variable with factor()\n\nMonths were initially stored as decimals. We converted month to an integer earlier, and we can see month is still stored as an integer. Let’s convert month to a factor so R will treat each month as a separate class.\n\nstorms$month &lt;- factor(storms$month)  # convert month to a categorical variable\nsummary(storms$month)  # check summary output after converting to factor\n\n   1    4    5    6    7    8    9   10   11   12 \n  70   66  201  779 1603 4440 7509 3077 1109  212"
  },
  {
    "objectID": "03-EDA-Quantitative.html#side-by-side-boxplots-with-plot",
    "href": "03-EDA-Quantitative.html#side-by-side-boxplots-with-plot",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Side by Side Boxplots with plot()",
    "text": "Side by Side Boxplots with plot()\n\nThe plot() function creates different types of plots depending on the data type and number of variables we enter.\n\nIf x is quantitative, plot(x) creates an index plot which is generally not too useful.\nIf x is categorical, plot(x) creates a bar chart.\n\n\npar(mfrow = c(1,2))  # create a 1 by 2 array of plots\nplot(storms$month)  # bar chart is created for categorical data\nplot(storms$wind)  # index plot is created for quantitative data\n\n\n\n\n\nIf x is categorical and y is quantitative, plot(y ~ x, data = [name]) creates side by side boxplots, one for each class of x.\nIf both x and y are quantitative variables, plot(y ~ x, data = [name]) creates a scatterplot.\n\n\npar(mfrow = c(1,2))  # create a 1 by 2 array of plots\nplot(wind ~ month, data = storms)  # side by side boxplots\nplot(wind ~ pressure, data = storms)  # scatterplot\n\n\n\n\nThe side by side boxplots created above are hard to read since we have 12 boxplots in total. The two months with the most storms data are August and September.\n\nHow can we compare storms only in August and September?"
  },
  {
    "objectID": "03-EDA-Quantitative.html#subsetting-and-filtering-data",
    "href": "03-EDA-Quantitative.html#subsetting-and-filtering-data",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Subsetting and Filtering Data",
    "text": "Subsetting and Filtering Data\n\nWe can compare data for only August and September using various methods. One common method is to subset all of the data in storms into two separate data frames, one for each month. Below are three different ways we can subset data:\n\nUsing the subset() function in base R.\nUsing the filter() function in the dplyr package.\nUsing logical statements.\n\nOther methods exist as well.\n\nThe subset() Function in Base R\n\nAs the name implies, the subset() function in base R is a really useful function for subsetting! We can open the help documentation with ?subset to learn how to apply this function. Below are some examples of different ways we may want to subset the storms data to analyze for storms that occurred in August.\n\n# keeps all variables for storms in August\naug &lt;- subset(storms, month == \"8\")\n\n# keeps only the wind speed variable for August storms\naug.wind &lt;- subset(storms, select = wind, month == \"8\")\n\n# drop = T drops the column name and creates a vector instead of a data frame\naug.wind.vec &lt;- subset(storms, select = wind, month == \"8\", drop = T) \n\n\n# we can see all variables are selected\nhead(aug)\n\n# A tibble: 6 × 13\n  name   year month   day  hour   lat  long status categ…¹  wind press…² tropi…³\n  &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 Caro…  1975 8        24    12  22.4 -69.8 tropi… &lt;NA&gt;       25    1011      NA\n2 Caro…  1975 8        24    18  21.9 -71.1 tropi… &lt;NA&gt;       25    1011      NA\n3 Caro…  1975 8        25     0  21.6 -72.5 tropi… &lt;NA&gt;       25    1010      NA\n4 Caro…  1975 8        25     6  21.2 -73.8 tropi… &lt;NA&gt;       25    1010      NA\n5 Caro…  1975 8        25    12  20.9 -75.1 tropi… &lt;NA&gt;       25    1011      NA\n6 Caro…  1975 8        25    18  20.6 -76.4 tropi… &lt;NA&gt;       25    1011      NA\n# … with 1 more variable: hurricane_force_diameter &lt;int&gt;, and abbreviated\n#   variable names ¹​category, ²​pressure, ³​tropicalstorm_force_diameter\n\n\n\n# just the wind variable is selected\nhead(aug.wind)\n\n# A tibble: 6 × 1\n   wind\n  &lt;int&gt;\n1    25\n2    25\n3    25\n4    25\n5    25\n6    25\n\n\n\n# wind speeds in august stored in a vector\nhead(aug.wind.vec)\n\n[1] 25 25 25 25 25 25"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-19",
    "href": "03-EDA-Quantitative.html#question-19",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 19",
    "text": "Question 19\n\nCompute the mean and median wind speed of storms in August. Compare the values of the mean and median. What does this tell us about the shape of the data?\n\nSolution to Question 19\n\n\n\n\n\n\nThe filter() Function in dplyr\n\nUsing the filter function in dplyr package, we can filter out just the August observations.\n\nNote you need to load the dplyr package with a library() in order to use filter().\nWe have already loaded dplyr since that is where the storms data is found.\nThe command below gives the same result as subset(storms, month == \"8\").\n\n\naug2 &lt;- filter(storms, month == \"8\")  # filter requires dplyr package\nhead(aug2)  # selects all variables\n\n# A tibble: 6 × 13\n  name   year month   day  hour   lat  long status categ…¹  wind press…² tropi…³\n  &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 Caro…  1975 8        24    12  22.4 -69.8 tropi… &lt;NA&gt;       25    1011      NA\n2 Caro…  1975 8        24    18  21.9 -71.1 tropi… &lt;NA&gt;       25    1011      NA\n3 Caro…  1975 8        25     0  21.6 -72.5 tropi… &lt;NA&gt;       25    1010      NA\n4 Caro…  1975 8        25     6  21.2 -73.8 tropi… &lt;NA&gt;       25    1010      NA\n5 Caro…  1975 8        25    12  20.9 -75.1 tropi… &lt;NA&gt;       25    1011      NA\n6 Caro…  1975 8        25    18  20.6 -76.4 tropi… &lt;NA&gt;       25    1011      NA\n# … with 1 more variable: hurricane_force_diameter &lt;int&gt;, and abbreviated\n#   variable names ¹​category, ²​pressure, ³​tropicalstorm_force_diameter\n\n\n\n\nUsing Logical Statements\n\nWhen writing more complex code such as for loops, it is often useful to subset data using logical statements. For example, storms[storms$month == \"8\", ] extracts just the rows that have a month value equal to 8.\n\n# extract rows from storms with month equal to 8\naug.logic &lt;- storms[storms$month == \"8\", ]\nhead(aug.logic)\n\n# A tibble: 6 × 13\n  name   year month   day  hour   lat  long status categ…¹  wind press…² tropi…³\n  &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 Caro…  1975 8        24    12  22.4 -69.8 tropi… &lt;NA&gt;       25    1011      NA\n2 Caro…  1975 8        24    18  21.9 -71.1 tropi… &lt;NA&gt;       25    1011      NA\n3 Caro…  1975 8        25     0  21.6 -72.5 tropi… &lt;NA&gt;       25    1010      NA\n4 Caro…  1975 8        25     6  21.2 -73.8 tropi… &lt;NA&gt;       25    1010      NA\n5 Caro…  1975 8        25    12  20.9 -75.1 tropi… &lt;NA&gt;       25    1011      NA\n6 Caro…  1975 8        25    18  20.6 -76.4 tropi… &lt;NA&gt;       25    1011      NA\n# … with 1 more variable: hurricane_force_diameter &lt;int&gt;, and abbreviated\n#   variable names ¹​category, ²​pressure, ³​tropicalstorm_force_diameter"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-20",
    "href": "03-EDA-Quantitative.html#question-20",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 20",
    "text": "Question 20\n\nUsing one of the methods above, create a data frame name sept that contains all variables for only the observations that occurred in September.\n\nSolution to Question 20\n\n\n# keeps all variables for storms in September"
  },
  {
    "objectID": "03-EDA-Quantitative.html#creating-side-by-side-boxplots-with-boxplot",
    "href": "03-EDA-Quantitative.html#creating-side-by-side-boxplots-with-boxplot",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Creating Side by Side Boxplots with boxplot",
    "text": "Creating Side by Side Boxplots with boxplot\n\nOnce we have created the data frames aug and sept, we can create side by side boxplots to compare the wind speeds for storms in these two months.\n\n# need to answer previous question first\nboxplot(aug$wind, sept$wind,  # enter two vectors of data\n        main = \"Comparing Wind Speeds in Aug. and Sept.\",   # main title\n        xlab = \"Wind Speed (in knots)\",  # x-axis label\n        horizontal = TRUE,  # align boxplots horizontally\n        names = c(\"August\", \"September\"),  # label each boxplot\n        col = c(\"seagreen\", \"steelblue\"))  # fill color for box"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-21",
    "href": "03-EDA-Quantitative.html#question-21",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 21",
    "text": "Question 21\n\nIn which month (August or September) are the wind speeds of storms more severe? What statistics did you use to draw your conclusion?\n\nSolution to Question 21"
  },
  {
    "objectID": "03-EDA-Quantitative.html#question-22",
    "href": "03-EDA-Quantitative.html#question-22",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Question 22",
    "text": "Question 22\n\nCreate side by side boxplots to compare the distribution of wind speeds in July, August and September.\n\nSolution to Question 22"
  },
  {
    "objectID": "03-EDA-Quantitative.html#why-cant-i-see-the-output",
    "href": "03-EDA-Quantitative.html#why-cant-i-see-the-output",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Why Can’t I See the Output?",
    "text": "Why Can’t I See the Output?\n\nIn the following code, we compute the mean of a vector. Why can’t we see the result after running it?\n\nw &lt;- storms$wind  # wind is now stored in w\nxbar.w &lt;- mean(w)  # compute mean wind speed and assign to xbar.w\n\nIn the code cell above, the output has been stored in an object that we can refer to later."
  },
  {
    "objectID": "03-EDA-Quantitative.html#printing-output-to-screen",
    "href": "03-EDA-Quantitative.html#printing-output-to-screen",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Printing Output to Screen",
    "text": "Printing Output to Screen\n\nOnce an object has been assigned a name, it can be printed by executing the name of the object or using the print function or just entering the object name.\n\nxbar.w  # print the mean wind speed to screen\nprint(xbar.w)  # print a different way"
  },
  {
    "objectID": "03-EDA-Quantitative.html#assigning-and-printing-an-object-at-once",
    "href": "03-EDA-Quantitative.html#assigning-and-printing-an-object-at-once",
    "title": "1.3: Exploring Quantitative Data",
    "section": "Assigning and Printing An Object At Once",
    "text": "Assigning and Printing An Object At Once\n\nAnother nice way to both execute, store, and print the output of a command is the parentheses ( ) method.\n\n(sd.w &lt;- sd(w))  # using ( ) around a command will execute, store and print output\n\n\nSometimes you want to see the result of a code cell, and sometimes you will not.\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "04-Overview-of-Probability.html",
    "href": "04-Overview-of-Probability.html",
    "title": "2.1: An Overview of Probability",
    "section": "",
    "text": "Sample Space, Outcomes and Events\nA statistical experiment or observation is any random activity that results in a definite outcome.\nLet \\(A\\) and \\(B\\) denote two events in sample space \\(\\Omega\\), then\nWe can generalize the calculations from the previous study on people arrested for small quantities of marijuana to obtain the following results:\nLet \\(A\\) and \\(B\\) denote two events in sample space \\(\\Omega\\), then\nOften in statistics we want to investigate questions such as:\nTwo events \\(A\\) and \\(B\\) are mutually exclusive (or disjoint) if they cannot occur at the same time, and therefore \\(P(A \\cap B) = 0\\).\nSpecial case: If events \\(A\\) and \\(B\\) are disjoint then we have \\(P(A \\cup B) = P(A)+P(B)\\).\nA function \\(P\\) that assigns a real number \\(P(A)\\) to each event \\(A\\) is a probability distribution or a probability measure if it satisfies the following three axioms:\n\\[P \\left( A \\cup B \\right) = P(A) + P(B).\\]"
  },
  {
    "objectID": "04-Overview-of-Probability.html#example-rolling-a-fair-six-sided-die",
    "href": "04-Overview-of-Probability.html#example-rolling-a-fair-six-sided-die",
    "title": "2.1: An Overview of Probability",
    "section": "Example: Rolling a Fair Six-Sided Die",
    "text": "Example: Rolling a Fair Six-Sided Die\n\n\n\n\nImage Credit: Steaphan Greene, CC BY-SA 3.0, via Wikimedia Commons\n\n\nThe study of probability was initially inspired by calculating odds of outcomes from card and dice games. For example, consider a rolling a fair six-sided die.\n\nThe sample space is \\(\\Omega = \\left\\{ 1, 2, 3, 4, 5, 6 \\right\\}\\)\nFor a fair die, each of the six possible outcomes has an equally likely chance of occurring.\nOne possible outcome is rolling a 4, \\(\\omega = 4\\)\nLet \\(A\\) denote the event that the roll is a multiple of 3, \\(A = \\left\\{ 3, 6 \\right\\}\\)."
  },
  {
    "objectID": "04-Overview-of-Probability.html#probabilities-with-equally-likely-outcomes",
    "href": "04-Overview-of-Probability.html#probabilities-with-equally-likely-outcomes",
    "title": "2.1: An Overview of Probability",
    "section": "Probabilities with Equally Likely Outcomes",
    "text": "Probabilities with Equally Likely Outcomes\n\nFor finite sample spaces, we often use counting to determine probabilities. A special case which we will encounter often is when each outcome in the sample space \\(\\Omega\\) is equally likely to occur, and therefore\n\\[ P(A) = \\frac{\\mbox{Number of outcomes in event $A$}}{\\mbox{Total number of outcomes in $\\Omega$}} = \\frac{|A|}{|\\Omega|}.\\]\nWe use the notation \\(P(A)\\) to denote the probability that event \\(A\\) occurs.\n\nProbabilities are proportions between \\(0\\) (impossible to occur) and \\(1\\) (certain to occur) that we typically represent as decimals or fractions.\nThe probability \\(P: \\Omega \\rightarrow \\mathbb{R}\\) is a function that maps each event in \\(\\Omega\\) to a number in the interval \\(0 \\leq P(A) \\leq 1\\).\nSometimes we convert the proportion to a percentage when giving a probability."
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-1",
    "href": "04-Overview-of-Probability.html#question-1",
    "title": "2.1: An Overview of Probability",
    "section": "Question 1",
    "text": "Question 1\n\nIf a person rolls a fair, six-sided die, what is the probability the result of the roll is a number that is divisible by 3?\n\nSolution to Question 1\n\n\n\n\n\n\nInstalling and Loading carData Package\n\nThe data set Arrests is in the carData package in R which is not installed in Google Colaboratory.\n\nFirst run the code cell below to to install the carData package.\n\n\n#install.packages(\"carData\")\n\n\nNext load the package with the library command so we an access the Arrests data set.\n\n\nlibrary(carData)\n\nWe can access and explore the data set Arrests in carData."
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-2",
    "href": "04-Overview-of-Probability.html#question-2",
    "title": "2.1: An Overview of Probability",
    "section": "Question 2",
    "text": "Question 2\n\nUse R functions such as summary(), str(), and/or ?Arrests to answer some questions about the data:\n\nWhat data is included in the Arrests data set?\nHow many observations are in the data?\nWhat is the population of interest?\nWhat is the source of the data?\nWhat are the categorical variables in the data set?\nWhat are the quantitative variables in the data set?\nAre the variable data types accurate, or do some variables need to be converted to other data types?\n\n\nSolution to Question 2\n\n\n# Use code cell to summarize and/or get help with Arrests data"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-3",
    "href": "04-Overview-of-Probability.html#question-3",
    "title": "2.1: An Overview of Probability",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose you would like to analyze whether Black arrestees are more or less likely to be not released with a summons (and therefore detained in jail) compared to White arrestees. The two variables of interest are therefore colour and released.\n\nQuestion 3a\n\nUse the table() function, create a two-way table to summarize the relation between colour and released.\n\nSolution to Question 3a\n\n\ntable(??)\n\n\n\n\nQuestion 3b\n\nYou should see from your output in Question 3a that more White arrestees were released (559) compared the Black arrestees that were released (333). Why is it problematic to compare counts of White and Black arrestees and conclude White arrestees are more likely to be detained (not be released)?\n\nSolution to Question 3b\n\n\n\n\n\n\n\nQuestion 3c\n\nWhat is the probability that a randomly selected arrestee in the study:\n\nWas not released?\nWas Black?\nWas not released and was Black?\nWas not released or was Black?\nGiven that a person was Black, what is the probability they were not released?\n\n\nSolution to Question 3c\n\nUse the code cell below to compute each of the probabilities (i)-(v).\n\n# Enter code to compute each of the following\n# Be sure to print results to screen!\n# Find the probability that a randomly selected person in the study:\n\n# (i) was not released\n\n\n# (ii) was Black\n\n\n# (iii) was not released and was Black\n\n\n# (iv) was not released or was Black?\n\n\n# (v) was not released given they were Black\n\nSummarize results below\n\nThe probability that a randomly selected person in the study was not released, \\(P(N)=??\\)\nThe probability that a randomly selected person in the study was Black, \\(P(B)=??\\)\nThe probability that a randomly selected person in the study was not released and was Black, \\(P(R \\cap B)=??\\)\nThe probability that a randomly selected person in the study was not released or was Black, \\(P(A \\cup D)=??\\)\nGiven that a person was Black, what is the probability they were not released, \\(P(R \\ | \\ B)\\)?\n\n\n\n\nQuestion 3d\n\nBased on the data from this study, do you believe Black arrestees are more, less, or equally likely to be detained (not be released) than White arrestees? Support your answer using probabilities. You may want to compute additional probabilities that were not asked in Question 3c before reaching your conclusion.\n\nSolution to Question 3d"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-4",
    "href": "04-Overview-of-Probability.html#question-4",
    "title": "2.1: An Overview of Probability",
    "section": "Question 4",
    "text": "Question 4\n\nMatch one of the Venn diagrams labelled (i)-(vi) in the table below to one of the set operations below. Note that two of the Venn Diagrams do not match any of the set operations.\nMatch one of the Venn diagrams labelled (i)-(vi) in the table below to one of the set operations below. Note that two of the Venn Diagrams do not match any of the set operations.\n\n\n\n\n\n\n\n\ndiagram (i)\ndiagram (ii)\ndiagram (iii)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiagram (iv)\ndiagram (v)\ndiagram (vi)\n\n\n\n\n\n\n\n\n\n\nImage credit: Adam Spiegler, CC BY-SA 4.0, via Wikimedia Commons\n\nSolution to Question 4\n\n\n\n\n\\(A^C\\)\n\\(A \\cup B\\)\n\\(A \\cap B\\)\n\\(A-B\\)\n\n\n\n\nDiagram ??\nDiagram ??\nDiagram ??\nDiagram ??"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-5",
    "href": "04-Overview-of-Probability.html#question-5",
    "title": "2.1: An Overview of Probability",
    "section": "Question 5",
    "text": "Question 5\n\nWhen a customer purchases a new car, they are presented with a menu of options such as heated steering wheel, parking assistant, satellite radio, etc. The two most popular options on a certain type of new car are a sunroof (denoted \\(S\\)) and heated seats (denoted by \\(H\\)). Answer the following questions if we know that\n\\[ P(S) = 0.6, \\quad P(H) = 0.45,\\mbox{ and }  P(H | S ) = 0.65. \\]\n\nQuestion 5a\n\nInterpret the practical meaning of \\(P(H | S ) = 0.65\\).\n\nSolution to Question 5a\n\n\n\n\n\n\n\nQuestion 5b\n\nCompute \\(P(H^C)\\) and interpret the meaning.\n\nSolution to Question 5b\n\n\n\n\n\n\n\nQuestion 5c\n\nCompute \\(P(S \\cap H)\\) and interpret the meaning.\n\nSolution to Question 5c\n\n\n\n\n\n\n\nQuestion 5d\n\nCompute \\(P(S | H)\\) and interpret the meaning.\n\nSolution to Question 5d\n\n\n\n\n\n\n\nQuestion 5e\n\nGiven a customer has purchased the sunroof option, are they more, less, or equally likely to get the heated seats option? Which probabilities did you use to determine your answer? Does this make practical sense in this context?\n\nSolution to Question 5e"
  },
  {
    "objectID": "04-Overview-of-Probability.html#definition-of-independent-events",
    "href": "04-Overview-of-Probability.html#definition-of-independent-events",
    "title": "2.1: An Overview of Probability",
    "section": "Definition of Independent Events",
    "text": "Definition of Independent Events\n\nTwo events \\(A\\) and \\(B\\) are independent if the occurrence of one has no effect on the occurrence of the other:\n\\[ P(B) = P(B \\ | \\ A) \\quad \\mbox{or} \\quad P(A) = P(A \\ | \\ B).\\]\nSpecial case: If events \\(A\\) and \\(B\\) are independent events then we have \\(P(A \\cap B) = P(A)P(B)\\)."
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-6",
    "href": "04-Overview-of-Probability.html#question-6",
    "title": "2.1: An Overview of Probability",
    "section": "Question 6",
    "text": "Question 6\n\nConsider the board game picture below. You begin at the start position and flip a fair coin. If the result of the flip is tails, you move forward one space on the board. If the result is heads, you move forward two spaces on the board. Keep flipping a fair coin (with tails moving one space and heads two spaces) until you reach the winner position.\n\n\n\nImage Credit: Adam Spiegler, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\nWhat is the probability that you win in exactly two flips?\nWhat is the probability that you win in exactly three flips?\nWhat is the probability that you win in exactly four flips?\n\n\nSolution to Question 6"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-7",
    "href": "04-Overview-of-Probability.html#question-7",
    "title": "2.1: An Overview of Probability",
    "section": "Question 7",
    "text": "Question 7\n\nDoes having health insurance help avoid bankruptcies? Let \\(B\\) denote the event a person goes bankrupt. Let \\(H\\) denote the event a person has health insurance.\n\nQuestion 7a\n\nWhat is the difference in the practical meaning \\(P(H \\ | \\ B)\\) and \\(P(B \\ | \\  H )\\)? Explain in practical terms that a non-statistician could understand, and avoiding technical language.\n\nSolution to Question 7a\n\n\n\n\n\n\n\nQuestion 7b\n\nCan you determine whether having health insurance has any affect on the likelihood that a person goes bankrupt by comparing \\(P(H \\ | \\ B)\\) and \\(P(B \\ | \\ H )\\)?\n\nIf so, explain how you would compare those to probabilities to help answer the question.\nIf not, what additional probability (or probabilities) would be useful to know in order to answer this question.\n\n\nSolution to Question 7b"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-8",
    "href": "04-Overview-of-Probability.html#question-8",
    "title": "2.1: An Overview of Probability",
    "section": "Question 8",
    "text": "Question 8\n\nGive an example of two events that are mutually exclusive, and give an example of two events that are not mutually exclusive.\n\nSolution to Question 8"
  },
  {
    "objectID": "04-Overview-of-Probability.html#partitions-of-sample-spaces",
    "href": "04-Overview-of-Probability.html#partitions-of-sample-spaces",
    "title": "2.1: An Overview of Probability",
    "section": "Partitions of Sample Spaces",
    "text": "Partitions of Sample Spaces\n\nA partition of a space \\(\\Omega\\) is a collection of disjoint sets such that \\(\\displaystyle \\bigcup_{i=1}^{\\infty} A_i = \\Omega\\).\n\n\n\nImage Credit: Adam Spiegler, CC BY-SA 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-9",
    "href": "04-Overview-of-Probability.html#question-9",
    "title": "2.1: An Overview of Probability",
    "section": "Question 9",
    "text": "Question 9\n\nFill in the blank to complete theorem below and explain (in words, pictures, or equations) how you determined your answer.\nLet \\(A_1\\), \\(A_2\\), \\(\\ldots , A_k\\) be a partition of \\(\\Omega\\). Then for any event \\(B\\),\n\\[P(B) = \\sum_{i=1}^k \\_\\_\\_\\_ .\\]\n\nSolution to Question 9"
  },
  {
    "objectID": "04-Overview-of-Probability.html#statement-of-bayes-theorem",
    "href": "04-Overview-of-Probability.html#statement-of-bayes-theorem",
    "title": "2.1: An Overview of Probability",
    "section": "Statement of Bayes’ Theorem",
    "text": "Statement of Bayes’ Theorem\n\nLet \\(A_1\\), \\(A_2\\), \\(\\ldots , A_k\\) be a partition of \\(\\Omega\\) such that \\(P(A_i)&gt;0\\) for each \\(i\\). If \\(B\\) is any event with \\(P(B)&gt;0\\), then for each \\(i=1, \\ldots k\\), we have\n\\[P(A_i \\mid B) = \\frac{P(A_i \\cap B) }{P(B)} = \\frac{P(B \\mid A_i) P(A_i)}{\\sum_{j=1}^k P(B \\mid A_j)P(A_j)}.\\]"
  },
  {
    "objectID": "04-Overview-of-Probability.html#question-10",
    "href": "04-Overview-of-Probability.html#question-10",
    "title": "2.1: An Overview of Probability",
    "section": "Question 10",
    "text": "Question 10\n\nSuppose that 30% of computers run Mac, 50% use PC, and 20% use Linux. A computer virus is created by hackers, and suppose that 65% of Mac, 82% of PC, and 50% of Linux computers get the virus.\n\nQuestion 10a\n\nWhat is the probability that a randomly selected computer has the virus?\n\nSolution to Question 10a\n\n\n\n\n\n\n\nQuestion 10b\n\nWhat is the probability that a randomly selected computer is PC given that the computer is infected by the virus?\n\nSolution to Question 10b\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html",
    "href": "05-Intro-to-Random-Variables.html",
    "title": "2.2: Introduction to Random Variables",
    "section": "",
    "text": "Randomness in a Music Playlist\nHow can we connect these questions about our data to the concepts of sample spaces and events to answer these question? The link can be made using random variables!\nIf \\(X\\) is a discrete random variable, we define the probability function or probability distribution or probability mass function (pmf) for \\(X\\) by\n\\[\\color{dodgerblue}{p(x) = P(X=x)}.\\]\nIf \\(X\\) is a discrete random variable, we define the cumulative distribution function (cdf) as the function\n\\[\\color{dodgerblue}{F(x)=P(X \\leq x) = \\sum_{k=\\rm{min \\ value}}^x p(k) }.\\]\nDiscrete random variables map outcomes in the sample space to integers. In many situations we would like to consider mapping outcomes to a continuous interval of values, not just integers. For example, what is the probability that randomly selected song has a tempo less than 82 beats per minute (BPM)?\nUsing modern technology such as music sequencers tempo has become a very precise measurement. Tempo is an important characteristic in electronic dance music where accurate measurement of a tune’s BPM is important to DJ’s when mixing music.\nsummary(hits$tempo)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  60.02   98.99  120.02  120.12  134.27  210.85"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#importing-the-spotify-hits-data-set",
    "href": "05-Intro-to-Random-Variables.html#importing-the-spotify-hits-data-set",
    "title": "2.2: Introduction to Random Variables",
    "section": "Importing the Spotify Hits Data Set",
    "text": "Importing the Spotify Hits Data Set\n\nThe data set spotify-hits.csv1 is stored online and contains audio statistics of the top 2000 tracks on Spotify from 2000-2019. The data is stored in a comma separated file (csv). We can use the function read.csv() to import the csv file into an R data frame we call hits.\n\nhits &lt;- read.csv(\"https://raw.githubusercontent.com/CU-Denver-MathStats-OER/Statistical-Theory/main/Data/spotify-hits.csv\")\n\n\nCleaning the Music Data Set\n\nIn the code cell below:\n\nWe convert artist, song, and genre to categorical variables using the factor() function.\nExtract the variables artist, song, tempo and genre (ignoring the rest).\nPrint the first 6 rows to screen to get a glimpse of the resulting data frame.\n\n\nhits$artist &lt;- factor(hits$artist)  # artist is categorical\nhits$song &lt;- factor(hits$song)  # song is categorical\nhits$genre &lt;- factor(hits$genre)  # genre is categorical\nhits &lt;- hits[,c(\"artist\", \"song\", \"tempo\", \"genre\")] \nhead(hits)  # display first 6 rows of data frame\n\n          artist                   song   tempo             genre\n1 Britney Spears Oops!...I Did It Again  95.053               pop\n2      blink-182   All The Small Things 148.726         rock, pop\n3     Faith Hill                Breathe 136.859      pop, country\n4       Bon Jovi           It's My Life 119.992       rock, metal\n5         *NSYNC            Bye Bye Bye 172.656               pop\n6          Sisqo             Thong Song 121.549 hip hop, pop, R&B"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-1",
    "href": "05-Intro-to-Random-Variables.html#question-1",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 1",
    "text": "Question 1\n\nExplore the data set hits. For example:\n\nHow many observations are in the data set?\nWhich artist had the most hits?\nWhat is the mean tempo?\nCreate a plot to display the distribution of tempo scores.\nWhat genre occurs most frequently?\n\n\nSolution to Question 1"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#creating-a-random-playlist",
    "href": "05-Intro-to-Random-Variables.html#creating-a-random-playlist",
    "title": "2.2: Introduction to Random Variables",
    "section": "Creating a Random Playlist",
    "text": "Creating a Random Playlist\n\nTo create a random playlist of five songs from the hits library of songs, we can run the code cell below.\n\nThe sample() function below has three inputs:\n\nThe “population” we will be sampling from.\n\nnrow(hits) returns the value 2000, the total number of observations (rows) in the data frame.\n\nThe size is the number of observations we will select.\nThe replace =TRUE options means we sample with replacement. Each time we pick a song, we place it back into the population and can select it in our playlist again.\nRun ?sample for more information.\n\nWe save the selected songs to the object called playlist.\nWe print the list of songs in playlist to the screen\n\n\n# index contains the 5 randomly selected songs\nindex &lt;- sample(nrow(hits), size=5, replace = TRUE)\nplaylist &lt;- hits[index,]  # save each song from index to a playlist\nhead(playlist)  # print the playlist\n\n                 artist            song   tempo        genre\n1563      Ariana Grande  Love Me Harder  98.992          pop\n34            Ruff Endz         No More  97.004          R&B\n508  The Pussycat Dolls       Don't Cha 120.003     pop, R&B\n250           Sean Paul Gimme the Light 107.288 hip hop, pop\n823      Britney Spears       Womanizer 139.000          pop"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#definition-of-a-random-variable",
    "href": "05-Intro-to-Random-Variables.html#definition-of-a-random-variable",
    "title": "2.2: Introduction to Random Variables",
    "section": "Definition of a Random Variable",
    "text": "Definition of a Random Variable\n\nA random variable is a function that maps each outcome \\(\\omega\\) in the sample space \\(\\Omega\\) to a real number \\(X(\\omega)\\).\n\\[ X: \\Omega \\to \\mathbb{R} \\]\nIn the music playlist example, each playlist is a different outcome in the sample space. There are a total of \\(2000^5 = 3.2 \\times 10^{16}\\) possible playlists in the sample space.\n\nWe can define random variable \\(X\\) to be the number of songs in the playlist from the pop genre.\n\nFor example, we could randomly select 5 songs with genres \\(\\omega =\\) (hip hop, country, pop, rock, pop)\nRandom variable \\(X\\) is \\(X(\\omega) = 2\\) since there are 2 pop songs in the playlist.\n\nThe set of possible values for \\(X\\) is the discrete set \\(\\left\\{ 0, 1, 2, 3, 4, 5 \\right\\}\\).\n\n\nCounting Pop Songs\n\nOne potential tricky issue with song genre is many songs are classified in multiple genres. The blink-182 song All The Small Things is in both the pop and rock genres. To simplify the analysis here, we will count songs that are only classified in pop genre (and no other genres). We will not count All The Small Things as a pop song since it is also classified as rock.\n\nsum(hits$genre == \"pop\")  # counts number of songs classified as only pop\n\n[1] 428\n\n\n\nThe hits data frame contains 428 pop songs.\nThe proportion of all songs that are pop songs is \\(\\frac{428}{2000} = 0.214\\) or \\(21.4\\%\\)."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-2",
    "href": "05-Intro-to-Random-Variables.html#question-2",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 2",
    "text": "Question 2\n\nIf \\(21.4\\%\\) of all the songs are classified solely in the genre pop:\n\nCompute \\(P(X=0)\\), the probability of picking (with replacement) a random playlist of 5 songs that contains no pop songs.\nCompute \\(P(X=5)\\), the probability of picking (with replacement) a random playlist of 5 songs all of which are pop songs.\n\nYou can use an R code cell to help with the calculation.\n\n# P(X = 0)\n\n\n# P(X=5)\n\n\nSolution to Question 2"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#the-pmf-and-cdf-for-the-number-of-pop-songs",
    "href": "05-Intro-to-Random-Variables.html#the-pmf-and-cdf-for-the-number-of-pop-songs",
    "title": "2.2: Introduction to Random Variables",
    "section": "The PMF and CDF for the Number of Pop Songs",
    "text": "The PMF and CDF for the Number of Pop Songs\n\nIn the case of counting the number of pop songs in a 5 song playlist, we can organize the values of the pmf \\(p(x)\\) in a table.\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(p(x)\\)\n0.3000\n0.4084\n0.2224\n0.0606\n0.0082\n0.0004\n\n\n\nFrom the pmf table above, we have the corresponding cdf below.\n\n\n\n\\(x\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(F(x)\\)\n0.3000\n0.7084\n0.9308\n0.9914\n0.9996\n1"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-3",
    "href": "05-Intro-to-Random-Variables.html#question-3",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 3",
    "text": "Question 3\n\nInterpret the practical meaning of \\(p(3) = 0.0606\\) and \\(F(3) = 0.9914\\).\n\nSolution to Question 3"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-4",
    "href": "05-Intro-to-Random-Variables.html#question-4",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 4",
    "text": "Question 4\n\nWhat is the probability of picking a playlist with at least 3 pop songs?\n\nSolution to Question 4\n\n\n\n\n\n\nPlotting Distribution Functions\n\n\nn &lt;- 5  # set number of trials n\np &lt;- 0.241  # prob of success in a trial\nx &lt;- 0:n  # vector from 0 to n=3\n\npar(mfrow=c(1, 2))\nplot(x, dbinom(x, size = n, prob = p), \n     main = \"Graph of pmf\", \n     xlab = \"Number of Pop Songs\", \n     ylab = \"p(x)\")\n\nplot(x, pbinom(x, size = n, prob = p), \n     type=\"s\", \n     main = \"Graph of cdf\", \n     xlab = \"Number of Pop Songs\", \n     ylab = \"F(x)\")\n\n\n\npar(mfrow=c(1, 1))"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-5",
    "href": "05-Intro-to-Random-Variables.html#question-5",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(X\\) a discrete random variable with pmf and cdf denoted \\(p(x)\\) and \\(F(x)\\), respectively. Determine if each statement is True or False.\n\n\\(0 \\leq p(x) \\leq 1\\) for all \\(x\\).\n\\(0 \\leq F(x) \\leq 1\\) for all \\(x\\).\n\\(\\displaystyle \\sum_{\\rm{all}\\  x} p(x) = 1\\).\n\\(\\displaystyle \\sum_{\\rm{all}\\  x} F(x) = 1\\).\n\\(\\displaystyle \\lim_{x \\to \\infty} p(x) = 1\\).\n\\(\\displaystyle \\lim_{x \\to \\infty} F(x) = 1\\).\nThe pmf must be a nondecreasing function.\nThe cdf must be a nondecreasing function.\n\n\nSolution to Question 5\n\nList the properties (a)-(h) that are indeed TRUE."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-6",
    "href": "05-Intro-to-Random-Variables.html#question-6",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 6",
    "text": "Question 6\n\nBelow is a histogram representing the distribution of tempos for the 2000 songs in hits. Approximate the value of \\(P(X &lt; 85)\\), the probability that a randomly selected songs has a tempo less than 85 BPM.\n\nExperiment with the number of breaks to improve the accuracy of your approximation!\n\n\n# create a histogram\nhist(hits$tempo,  # vector of tempo measurements\n########################################################\n# Student To-Do: Adjust breaks\n########################################################\n     breaks = 10,  # number of bin ranges to use\n########################################################     \n     labels = TRUE,  # add count labels above each bar\n     xlab = \"Tempo (BPM)\",   # x-axis label\n     xaxt='n',  # turn off default x-axis ticks\n     yaxt='n',  # turn off default y-axis ticks\n     ylab = \"Frequency\",  # y-axis label\n     ylim = c(0,650),  # sets window for y-axis\n     main = \"Distribution of Tempos\",  # main label\n     col = \"steelblue\")  # fill color of bars\naxis(1, at = seq(60, 220, 10))  # set custom ticks on x-axis\naxis(2, at = seq(0, 650, 50))  # set custom ticks on y-axis\n\n\n\n\n\nSolution to Question 6"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#relative-frequency-histograms",
    "href": "05-Intro-to-Random-Variables.html#relative-frequency-histograms",
    "title": "2.2: Introduction to Random Variables",
    "section": "Relative Frequency Histograms",
    "text": "Relative Frequency Histograms\n\nOur initial frequency histogram of tempos in Question 6 measured the count or frequency of songs that fall in each bin of the histogram. A relative frequency histogram rescales the vertical axis to units proportion per unit of \\(X\\).\n\nWe can create a relative frequency histogram by adding the option freq = FALSE in the hist() function."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-7",
    "href": "05-Intro-to-Random-Variables.html#question-7",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 7",
    "text": "Question 7\n\nBased on the relative frequency histogram below, approximate the value of \\(P(X &lt; 85)\\), the probability that a randomly selected songs has a tempo less than 85 BPM.\n\nThe option freq = FALSE is added to the hist command below. Run the code cell below and compare the result with the histogram above. What is different about the two histograms? What is similar? What are the units of the horizontal axis?\nUsing the output from the code cell below, approximate \\(P(X &lt; 85)\\). Hint: The area of each bar corresponds to the proportion of songs in hits that are in the corresponding bin range of tempos.\n\n\n# create a histogram\nhist(hits$tempo,  # vector of tempo measurements\n     freq = FALSE,\n     breaks = 20,  # number of bin ranges to use\n     labels = TRUE,  # add count labels above each bar\n     xlab = \"Tempo (BPM)\",   # x-axis label\n     xaxt='n',  # turn off default x-axis ticks\n     yaxt='n',  # turn off default y-axis ticks\n     ylab = \"Relative Frequency\",  # y-axis label\n     ylim = c(0,0.025),  # sets window for y-axis\n     main = \"Distribution of Tempos\",  # main label\n     col = \"steelblue\")  # fill color of bars\naxis(1, at = seq(60, 220, 10))  # set custom ticks on x-axis\naxis(2, at = seq(0, 0.025, 0.005))  # set custom ticks on y-axis\n\n\n\n\n\nSolution to Question 7"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#density-plots",
    "href": "05-Intro-to-Random-Variables.html#density-plots",
    "title": "2.2: Introduction to Random Variables",
    "section": "Density Plots",
    "text": "Density Plots\n\nA density plot can informally be considered as a relative frequency histogram where we choose incredibly small widths for each bin range. One way to create a density plot in R is to:\n\nFirst convert the values of a quantitative variable to densities with the density() function.\n\n\nFor more information, see density help documentation.\n\n\nUse the plot() function to plot the densities.\n\n\nFor more advanced density plots see https://r-graph-gallery.com/density-plot.html.\n\n\n# approximate densities and then plot\nplot(density(hits$tempo),  # convert to density and then plot\n     ylab = \"density (proportion per BPM)\",  # vertical axis label\n     xlab = \"Tempo (in BPM)\",  # horizontal axis label\n     main = \"Distribution of Song Tempos\")  # main title\n\n\n\n\n\nReading a Density Plot\n\nIf we want to use a density plot compute \\(P(X &lt; 85)\\), the probability that a random selected song in hits has a tempo less than 85 BPM:\n\nWe approximate the AREA below the density curve over the interval from 0 to 85 BPM.\nIf we have a theoretical model \\(f(x)\\) for the density, we can use definite integrals to compute these proportions!\nRun the code cell below to sketch an area corresponding to \\(P(X&lt;85)\\). There is nothing to edit in the code cell.\n\n\n###############################\n# Area representing P(X &lt; 85)\n# Run code without editting\n##############################\nden &lt;- density(hits$tempo)\n\nplot(den,  # plot density of tempo\n     ylab = \"density (proportion per BPM)\",  # vertical axis label\n     xlab = \"Tempo (in BPM)\",  # horizontal axis label\n     xaxt = 'n',  # turn off default x-axis ticks\n     main = \"Distribution of Song Tempos\")  # main title\n\n\n# Fill area from 0 to 85 BPM\nvalue &lt;- 85\n\npolygon(c(den$x[den$x &lt;= value ], value),\n        c(den$y[den$x &lt;= value ], 0),\n        col = \"slateblue1\",\n        border = 1)\naxis(1, at = seq(0, value, value))  # set custom ticks on x-axis"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#probability-density-function-pdf",
    "href": "05-Intro-to-Random-Variables.html#probability-density-function-pdf",
    "title": "2.2: Introduction to Random Variables",
    "section": "Probability Density Function (pdf)",
    "text": "Probability Density Function (pdf)\n\nIf \\(X\\) is a continuous random variable, the probability density function (pdf), denoted \\(f(x)\\), satisfies the following properties:\n\n\\(f(x) \\geq 0\\) for all \\(x\\),\n\\(\\displaystyle \\int_{-\\infty}^{\\infty} f(x) = 1\\), and\n\\(\\displaystyle P(a &lt; x &lt; b) = \\int_a^b f(x) \\, dx\\)"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#cumulative-distribution-function-cdf",
    "href": "05-Intro-to-Random-Variables.html#cumulative-distribution-function-cdf",
    "title": "2.2: Introduction to Random Variables",
    "section": "Cumulative Distribution Function (cdf)",
    "text": "Cumulative Distribution Function (cdf)\n\nIf \\(X\\) is a continuous random variable, the cumulative distribution function (cdf), denoted \\(F(x)\\), is\n\\[\\color{dodgerblue}{F(x) = P(X &lt; x) = \\int_{-\\infty}^x f(t) \\, dt}.\\]\nThus we have the important relations between a pdf and cdf of a continuous random variable X:\n\nThe cdf \\(F(x)\\) is an antiderivative of the pdf \\(f\\).\nThe pdf \\(f(x)\\) is the derivative of \\(F(x)\\)."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-8",
    "href": "05-Intro-to-Random-Variables.html#question-8",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 8",
    "text": "Question 8\n\nThe figure below contains 8 plots of either a probability density function or a cumulative distribution function of a continuous random variable. Match each probability density function shown in (a)-(d) with a corresponding graph of a cumulative distribution function (I)-(IV). Explain how you determined your answers.\n\n\n\nImage Credit: Adam Spiegler, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\nSolution to Question 8\n\n\n\n\nGraph (a)\nGraph (b)\nGraph (c)\nGraph (d)\n\n\n\n\n??\n??\n??\n??"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#properties-of-continuous-random-variables",
    "href": "05-Intro-to-Random-Variables.html#properties-of-continuous-random-variables",
    "title": "2.2: Introduction to Random Variables",
    "section": "Properties of Continuous Random Variables",
    "text": "Properties of Continuous Random Variables"
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-9",
    "href": "05-Intro-to-Random-Variables.html#question-9",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 9",
    "text": "Question 9\n\nLet \\(X\\) is a continuous random variable with pdf and cdf denoted \\(f(x)\\) and \\(F(x)\\), respectively. Determine if each statement is True or False.\n\n\\(0 \\leq f(x) \\leq 1\\) for all \\(x\\).\n\\(0 \\leq F(x) \\leq 1\\) for all \\(x\\).\n\\(\\displaystyle \\int_{\\infty}^{-\\infty} f(x) \\, dx= 1\\).\n\\(\\displaystyle \\int_{\\infty}^{-\\infty} F(x) \\, dx = 1\\).\n\\(\\displaystyle \\lim_{x \\to \\infty} f(x) = 1\\).\n\\(\\displaystyle \\lim_{x \\to \\infty} F(x) = 1\\).\nThe pdf must be a nondecreasing function.\nThe cdf must be a nondecreasing function.\n\n\nSolution to Question 9\n\nList the properties (a)-(h) that are indeed TRUE."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#question-10",
    "href": "05-Intro-to-Random-Variables.html#question-10",
    "title": "2.2: Introduction to Random Variables",
    "section": "Question 10",
    "text": "Question 10\n\nThe probability of a Lithium-ion battery failing at time \\(x\\) (in years) is given by the probability density function below.\n\\[f(x) = \\left\\{ \\begin{array}{ll}\n\\frac{1}{3}e^{-\\frac{x}{3}} & x \\geq 0 \\\\\n0 & \\mbox{otherwise}\n\\end{array} \\right.\\]\n\nQuestion 10a\n\nCompute the probability that a Lithium-ion battery lasts more than 2 years.\n\nSolution to Question 10a\n\n\n\n\n\n\n\nQuestion 10b\n\nInterpret the meaning of \\(P(X \\leq 3)\\) and \\(P(X &lt; 3)\\).\n\nSolution to Question 10b\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "05-Intro-to-Random-Variables.html#footnotes",
    "href": "05-Intro-to-Random-Variables.html#footnotes",
    "title": "2.2: Introduction to Random Variables",
    "section": "",
    "text": "Downloaded from Kaggle May 4, 2023↩︎"
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html",
    "href": "06-Expected-Value-and-Variance.html",
    "title": "2.3: Expected Value and Variance",
    "section": "",
    "text": "How Much is the Raffle Ticket Worth?\nThe expected value for a discrete random variable \\(X\\) is denoted \\(\\color{dodgerblue}{E(X)}\\). We compute\n\\[\\color{dodgerblue}{E(X) = \\mu_{X} =  \\sum_x \\left( x \\cdot p(x) \\right)}. \\]\nThe variance for a discrete random variable \\(X\\) is one common way to measure how spread out (in relation to the expected value) are the values of \\(X\\). We compute\n\\[\\color{dodgerblue}{\\mbox{Var}(X) = \\sigma_X^2 =  E\\big( (x-\\mu_X)^2 \\big) = \\sum_x \\left( (x-\\mu_X)^2 \\cdot p(x) \\right)}. \\]\nThe standard deviation for a discrete random variable \\(X\\) is the square root of the variance,\n\\[ \\mbox{SD}(X) = \\sigma_X  =  \\sqrt{\\mbox{Var}(X)}. \\]\nLet \\(f(x)\\) and \\(F(x)\\) denote the probability density function and cumulative distribution function for a continuous random variable \\(X\\).\n\\[E(X) = \\mu_X = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx.\\]\n\\[\\mbox{Var}(X) = E(X-\\mu_X)^2 = E(X^2) - \\mu_X^2.\\]\n\\[\\int_{-\\infty}^x f(t) \\, dt = 0.5 \\qquad \\mbox{or equivalently} \\qquad F(x) =0.5.\\]\nLet \\(X\\) and \\(Y\\) denote a random variables, and let \\(a\\) and \\(b\\) denote constants. Then we have the following properties.\nIn Introduction to Random Variables we discovered the following properties for a random variable \\(X\\)."
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#question-1",
    "href": "06-Expected-Value-and-Variance.html#question-1",
    "title": "2.3: Expected Value and Variance",
    "section": "Question 1",
    "text": "Question 1\n\nLet random variable \\(X\\) be the amount of money won by a randomly selected raffle ticket. We let \\(p(x)\\) denote the corresponding probability mass function of \\(X\\).\n\nQuestion 1a\n\nFill in the values of \\(x\\) and \\(p(x)\\) in the table below.\n\nSolution to Question 1a\n\nFill in the blanks to complete the table.\n\n\n\n\\(x\\)\n??\n??\n??\n??\n\n\n\n\n\\(p(x)\\)\n??\n??\n??\n??\n\n\n\n\n\n\n\n\n\nQuestion 1b\n\nIf somebody offered to buy a raffle ticket from you, what do you think fair value is for the ticket?\n\nSolution to Question 1b\n\n\n\n\n\n\n\n\nQuestion 1c\n\nConsider another ticket raffle. There 1,000 tickets with the following payouts:\n\n500 randomly selected tickets will win a grand prize of $15.\nThe other 500 tickets do not win a prize.\n\nLet random variable \\(Y\\) be the amount of money won by a randomly selected raffle ticket.\nSomebody offers one free ticket to either raffle \\(X\\) or raffle \\(Y\\) (but not both!), which would you prefer and why?\n\nSolution to Question 1c"
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#question-2",
    "href": "06-Expected-Value-and-Variance.html#question-2",
    "title": "2.3: Expected Value and Variance",
    "section": "Question 2",
    "text": "Question 2\n\nUsing properties of discrete random variables, show that for any discrete random variable \\(X\\) with pmf \\(p(X)\\) and expected value \\(E(X) = \\mu\\), we have\n\\[\\mbox{Var}(X) = E(X^2)  - \\mu^2.\\]\n\nSolution to Question 2\n\nFinish the proof below!\nLet \\(X\\) be a discrete random variable with pmf \\(p(x)\\) and expected value \\(E(X)=\\mu\\). Then we have\n\\[\\begin{array}{rcll}\n\\mbox{Var}(X) &=&  \\sum_x \\left( (x-\\mu)^2 \\cdot p(x) \\right) & \\mbox{(by definition)}\\\\\n&=& \\sum_x \\left( (x^2 -2 \\mu x + \\mu^2) \\cdot p(x) \\right) & \\mbox{(distributing squared term)}\\\\\n&=& \\sum_x \\left( x^2 \\cdot p(x) -2 \\mu x  \\cdot p(x)  + \\mu^2  \\cdot p(x)  \\right) & \\mbox{(distributing p(x))}\\\\\n&=& \\sum_x \\left( x^2 \\cdot p(x) \\right) - \\sum_x \\left(2 \\mu x  \\cdot p(x) \\right)  + \\sum_x \\left( \\mu^2  \\cdot p(x)  \\right) & \\mbox{(properties of summation)}\\\\\n&=& ?? & \\mbox{(??)} \\\\\n&=& ?? & \\mbox{(??)} \\\\\n&=& ?? & \\mbox{(??)} \\\\\n&=& ?? & \\mbox{(??)} \\\\\n\\end{array}\\]\n\n\nTherefore, we see that \\(\\mbox{Var}(X) = E(X^2) - \\mu^2\\)."
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#question-3",
    "href": "06-Expected-Value-and-Variance.html#question-3",
    "title": "2.3: Expected Value and Variance",
    "section": "Question 3",
    "text": "Question 3\n\nLet \\(X\\) and \\(Y\\) denote the raffle ticket random variables from Question 1.\nCalculate \\(E(X)\\) and \\(\\mbox{Var}(X)\\).\n\nQuestion 3a\n\nCalculate \\(E(X)\\) and \\(\\mbox{Var}(X)\\).\n\nSolution to Question 3a\n\n\n\n\n\n\n\n\nQuestion 3b\n\nCalculate \\(E(Y)\\) and \\(\\mbox{Var}(Y)\\).\n\nSolution to Question 3b"
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#question-4",
    "href": "06-Expected-Value-and-Variance.html#question-4",
    "title": "2.3: Expected Value and Variance",
    "section": "Question 4",
    "text": "Question 4\n\nConsider the random variable \\(X\\) with pdf\n\\[ f_X(x) = \\left\\{ \\begin{array}{ll}\n\\frac{x}{8}, &  0 \\leq x \\leq 4 \\\\\n0, &  \\mbox{otherwise}\n\\end{array} \\right. .\\]\n\nQuestion 4a\n\nOn a separate piece of paper, sketch a graph of the pdf, \\(f_X\\).\n\nSolution to Question 4a\n\nSketch a graph on a separate piece of paper.\n\n\n\n\n\n\nQuestion 4b\n\nEnter the formula for \\(F_X\\) below. Then on a separate piece of paper sketch the graph \\(F_X\\).\n\nSolution to Question 4b\n\n\\[F_X(x) = \\left\\{ \\begin{array}{ll}\n0 & x &lt; 0\\\\\n?? & 0 \\leq x \\leq 4 \\\\\n1 & x &gt; 4\n\\end{array} \\right.\\]\n\n\nSketch a graph on a separate piece of paper.\n\n\n\n\n\n\nQuestion 4c\n\nCalculate \\(P(X &lt; 1)\\) and illustrate this value on each of your graphs in the solutions to Question 1a and Question 1b.\n\nSolution to Question 4c\n\n\n\n\n\n\n\n\nQuestion 4d\n\nCalculate \\(E(X)\\) and illustrate this on your graph in the solution to Question 1a.\n\nSolution to Question 4d\n\n\n\n\n\n\n\n\nQuestion 4e\n\nGive the median value and illustrate this value on both of your graphs in the solutions to Question 1a and Question 1b.\n\nSolution to Question 4e\n\n\n\n\n\n\n\n\nQuestion 4f\n\nCompute \\(\\mbox{Var}(X)\\).\n\nSolution to Question 4f"
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#question-5",
    "href": "06-Expected-Value-and-Variance.html#question-5",
    "title": "2.3: Expected Value and Variance",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(X\\) and \\(Y\\) denote the raffle ticket random variables from Question 1.\n\nQuestion 5a\n\nDo you believe random variables \\(X\\) and \\(Y\\) are independent random variables? Explain why or why not.\n\nSolution to Question 5a\n\n\n\n\n\n\n\n\nQuestion 5b\n\nIf you purchase 3 raffle tickets from raffle \\(X\\) and 2 raffle tickets from raffle \\(Y\\), what is the expected value of your winnings?\n\nSolution to Question 5b\n\n\n\n\n\n\n\n\nQuestion 5c\n\nIf you purchase 3 raffle tickets from raffle \\(X\\) and 2 raffle tickets from raffle \\(Y\\), what is the variance of your winnings?\n\nSolution to Question 5c"
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#question-6",
    "href": "06-Expected-Value-and-Variance.html#question-6",
    "title": "2.3: Expected Value and Variance",
    "section": "Question 6",
    "text": "Question 6\n\nThe data set spotify-hits.csv1 is stored online and contains audio statistics of the top 2000 tracks on Spotify from 2000-2019. The data is stored in a comma separated file (csv).\n\nWe can use the function read.csv() to import the csv file into an R data frame we call hits.\n\n\nhits &lt;- read.csv(\"https://raw.githubusercontent.com/CU-Denver-MathStats-OER/Statistical-Theory/main/Data/spotify-hits.csv\")\n\nIn the code cell below:\n\nWe convert artist, song, and genre to categorical variables using the factor() function.\nExtract the variables artist, song, energy, acousticness, and genre (ignoring the rest).\nPrint the first 6 rows to screen to get a glimpse of the resulting data frame.\n\n\nhits$artist &lt;- factor(hits$artist)  # artist is categorical\nhits$song &lt;- factor(hits$song)  # song is categorical\nhits$genre &lt;- factor(hits$genre)  # genre is categorical\nhits &lt;- hits[,c(\"artist\", \"song\", \"energy\", \"acousticness\", \"genre\")] \nhead(hits)  # display first 6 rows of data frame\n\n          artist                   song energy acousticness             genre\n1 Britney Spears Oops!...I Did It Again  0.834       0.3000               pop\n2      blink-182   All The Small Things  0.897       0.0103         rock, pop\n3     Faith Hill                Breathe  0.496       0.1730      pop, country\n4       Bon Jovi           It's My Life  0.913       0.0263       rock, metal\n5         *NSYNC            Bye Bye Bye  0.928       0.0408               pop\n6          Sisqo             Thong Song  0.888       0.1190 hip hop, pop, R&B\n\n\n\nEnergy: A measure of how energetic a song is from \\(0.0\\) to \\(1.0\\) (least to most energy) of. Typically, energetic songs are fast, loud, and noisy.\nAcousticness: A measure from \\(0.0\\) to \\(1.0\\) (least to most acoustic) of depending on how significant the use of acoustic instruments are in the song.\n\nLet \\(X\\) denote the energy of a randomly selected song, and let \\(Y\\) denote the acousticness of a randomly selected song. We define a new song metric, \\(Z\\), that is a weighted mean of score \\(X\\) and \\(Y\\):\n\\[Z = \\frac{3X + 2Y}{5}\\]\n\nQuestion 6a\n\nDo you believe \\(X\\) and \\(Y\\) are independent random variables? Explain why or why not.\n\nSolution to Question 6a\n\n\n\n\n\n\n\n\nQuestion 6b\n\nUse R to compute \\(E(X)\\), \\(E(Y)\\), and \\(E(Z)\\). Check whether or not the property \\(E(aX + bY) = aE(X) + bE(Y)\\) holds in this context.\n\nHint: Recall R, the function mean(x) calculates the mean (expected value) of x.\n\n\nSolution to Question 6b\n\n\n# use code cell to compare variances\nx &lt;- hits$energy  # random variable x\ny &lt;- hits$acousticness  # random variable y\nz &lt;- (3*x + 2*y) / 5  # random variable z\n\n\n\n\n\n\n\n\nQuestion 6c\n\nUse R to compute \\(\\mbox{Var}(X)\\), \\(\\mbox{Var}(Y)\\), and \\(\\mbox{Var}(Z)\\). Check whether or not the property \\(\\mbox{Var}(aX + bY) = a^2\\mbox{Var}(X) + b^2\\mbox{Var}(Y)\\) holds in this context.\n\nHint: The function var(x) calculates the variance of x.\n\n\nSolution to Question 6c\n\n\n# use code cell to compare variances\n\n\n\n\n\n\n\n\nQuestion 6d\n\nDetermine whether each of the statements below are true or false. If false, explain why.\nFor any two random variables \\(X\\) and \\(Y\\) and constants \\(a\\) and \\(b\\):\n\nIt always follows that \\(E(aX + bY) = aE(X) + bE(Y)\\).\nIt always follows that \\(\\mbox{Var}(aX + bY) = a^2\\mbox{Var}(X) + b^2\\mbox{Var}(Y)\\).\n\n\nQuestion 6d"
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#sec-propdis",
    "href": "06-Expected-Value-and-Variance.html#sec-propdis",
    "title": "2.3: Expected Value and Variance",
    "section": "Properties of Discrete Random Variables",
    "text": "Properties of Discrete Random Variables\n\nFor a discrete random variable \\(X\\), let \\(p(x)\\) and \\(F(x)\\) denote the pmf and cdf, respectively, we have:\n\n\\(0 \\leq p(x) \\leq 1\\) for all \\(x\\)\n\\(\\displaystyle \\sum_{\\rm{all}\\  x} p(x) = 1\\)\n\\(F(x) = \\displaystyle P(X \\leq x) = \\sum_{k= x_{\\rm min}}^x p(k)\\)\n\\(0 \\leq F(x) \\leq 1\\) for all \\(x\\)\n\\(\\displaystyle \\lim_{x \\to \\infty} F(x) = 1\\)\n\\(F(x)\\) is nondecreasing."
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#sec-propcont",
    "href": "06-Expected-Value-and-Variance.html#sec-propcont",
    "title": "2.3: Expected Value and Variance",
    "section": "Properties of Continuous Random Variables",
    "text": "Properties of Continuous Random Variables\n\nFor a continuous random variable \\(X\\), let \\(f(x)\\) and \\(F(x)\\) denote the pdf and cdf, respectively, we have:\n\n\\(f(x) \\geq 0\\) for all \\(x\\)\n\\(\\displaystyle \\int_{-\\infty}^{\\infty} f(x) = 1\\)\n\\(\\displaystyle P(a &lt; x &lt; b) = \\int_a^b f(x) \\, dx\\)\n\\(\\displaystyle F(x) = \\int_{-\\infty}^x f(t) \\, dt\\)\nThe \\(F(x)\\) is an antiderivative of \\(f\\).\nThe \\(f(x)\\) is the derivative of \\(F(x)\\).\n\\(0 \\leq F(x) \\leq 1\\) for all \\(x\\).\n\\(\\displaystyle \\lim_{x \\to \\infty} F(x) = 1\\).\n\\(F(x)\\) is nondecreasing.\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "06-Expected-Value-and-Variance.html#footnotes",
    "href": "06-Expected-Value-and-Variance.html#footnotes",
    "title": "2.3: Expected Value and Variance",
    "section": "",
    "text": "Downloaded from Kaggle May 4, 2023↩︎"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html",
    "href": "07-Common-Discrete-RandVar.html",
    "title": "2.4: Common Discrete Distributions",
    "section": "",
    "text": "A Case Study in Randomness: Jury Duty\nThe jury plays the most crucial role in a trial. They determine how cases are ultimately resolved! The jury serves as an impartial reviewer of the facts presented in criminal and civil cases. The goal of randomly selecting the jury is to remove the bias in the jury selection process.\nA random sample of 12 adults cannot be a perfect representative of the population in all ways. No two distinct random samples of 12 adults is going to be same, yet we hope different juries would rule similarly based on the same set of facts if they are truly impartial. The jury system plays a vital role in the system of justice in the United States, and randomness plays a central role in helping reduce bias in court rulings.\nConsider a district with \\(n\\) people eligible for jury duty. On the first of each month the court chooses a new pool of \\(k\\) people from which juries will be selected. How many different jury pools of size \\(k\\) can be selected from a population with size \\(n\\)?\n\\[\\begin{array}{lclclcccl}\n\\mbox{choice 1} & & \\mbox{choice 2} &  & \\mbox{choice 3}  & & & & \\mbox{choice k} \\\\\nn & \\times & (n-1) & \\times  & (n-2) & \\times & \\ldots & \\times & (n-(k-1))\n\\end{array}\n= \\frac{n!}{(n-k)!}.\\]\nNote the factorial of a non-negative integer \\(n\\) is denoted \\(\\color{dodgerblue}{n!}\\) and is the product of all positive integers less than or equal to \\(n\\):\n\\[\\color{dodgerblue}{n! = 1 \\times 2 \\times 3 \\times \\ldots \\times (n-1) \\times n}\\]\nRecall random variables map outcomes in a sample space to a subset of the real numbers. In this example, an outcome is 5 randomly selected people for the jury. We define random variable \\(X\\) to be the number of independent voters on a randomly selected jury of 5 people. For example, we have:\nWe map each outcome to an integer \\(X\\), and then we can compute values of the corresponding probability mass function (pmf) \\(p(x) = P(X=x)\\).\nIf we repeat a Bernoulli trial that has probability of success \\(p\\) for each trial, we can count the number of failures, \\(X\\), that occur before the first success. In such cases, \\(X\\) follows a geometric distribution, and we write \\(\\color{dodgerblue}{X \\sim \\mbox{Geom}(p)}\\).\n\\[f(x) = q^xp \\quad \\mbox{for } x = 0, 1, 2, \\ldots .\\]\nThe Poisson distribution applies when the average frequency of occurrences in a given time period is known to be \\(\\lambda\\), and each occurrence is independent of the others. Let \\(X\\) denote the number of times an event occurs in the given time period, then we have \\(\\color{dodgerblue}{X \\sim \\mbox{Pois}(\\lambda)}\\).\nFor each situation, identify which distribution best describes the distribution of the random variable. Then calculate the probability."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#how-jurors-are-selected",
    "href": "07-Common-Discrete-RandVar.html#how-jurors-are-selected",
    "title": "2.4: Common Discrete Distributions",
    "section": "How Jurors Are Selected",
    "text": "How Jurors Are Selected\n\nThe federal judicial branch in the United States decides the constitutionality of federal laws and resolves disputes about federal laws. The federal court system is divided into 94 district courts. The US District Court in the District of Colorado randomly selects jurors from voter registration lists, driver license records, and state-issued adult identification records, by a computerized method. Below is an explanation of how juries are chosen in the District of Colorado. See The District of Colorado Juror Information for full details.\n\n“This selection process creates the court’s ‘Master Jury Wheel’. (This term originated in the days when names were placed into a large barrel-type wheel and turned around to mix them up. Today, computers are used to select names randomly.) From the ‘Master Jury Wheel’, jurors are randomly selected for a one month term of service or occasionally longer depending on the court’s jury needs.”"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-1",
    "href": "07-Common-Discrete-RandVar.html#question-1",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 1",
    "text": "Question 1\n\nConsider the selection of a jury of 5 people. We want to see whether the jury is representative in terms of political party. Note we initially choose a jury of 5 people to more quickly recognize a pattern. After identifying a pattern, we can extend our results to juries of 12 people. Let \\(Y\\) denote a jury member who identifies as an independent voter. Let \\(N\\) denote a jury member who is not independent (identify as Democrat, Republican, or with another political party).\n\nThere is one possible outcome in the event: 0 people on the jury identify an Independent.\nWe can represent that outcome as NNNNN.\n\n\nQuestion 1a\n\nList all possible outcomes in the event: Exactly 1 out of 5 jurors is independent.\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nList all possible outcomes in the event: Exactly 2 out of 5 jurors are independent.\n\nSolution to Question 1b"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#ignoring-the-order-of-selection-n-choose-k",
    "href": "07-Common-Discrete-RandVar.html#ignoring-the-order-of-selection-n-choose-k",
    "title": "2.4: Common Discrete Distributions",
    "section": "Ignoring the Order of Selection: N Choose K",
    "text": "Ignoring the Order of Selection: N Choose K\n\nIf we want to count the total number of possible jury pools, and we do not care about the order in which the people are selected, then we want to count the number of combinations.\n\nFirst count the possible outcomes as we illustrated above, taking into account the order in which the jury is selected.\nThen we ignore the order that people are selected by considering the same set of people (chosen in any order) as the same. Using the result that there are \\(k!\\) ways of permuting the order of the \\(k\\) people selected.\n\nThus, the number of combinations of \\(k\\) items out of a set of \\(n\\), denoted \\(_nC_k\\) also referred to as \\(n\\) choose \\(k\\), is\n\\[\\color{dodgerblue}{\\begin{pmatrix} n \\\\ k \\end{pmatrix} = \\frac{n!}{k! (n-k)!}.}\\]\n\n\n\n\n\n\nNote\n\n\n\nSee the Appendix: Why Divide By \\(k!\\) if you are curious why we divide by \\(k!\\) when we do not care about the order that the items are selected."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-2",
    "href": "07-Common-Discrete-RandVar.html#question-2",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 2",
    "text": "Question 2\n\nUse R to calculate the total number of outcomes in the event: Exactly 2 out of 5 jurors are independent voters.\n\n\n\n\n\n\nTip\n\n\n\nThe function factorial(x) calculates \\(x!\\) for a non-negative integer \\(x\\).\n\n\n\nSolution to Question 2\n\n\n# solution using factorial(x)\n\n\n\n\n\n\n\n\nTip\n\n\n\nAn Even Better Tip: The function choose(n, k) calculates \\(\\begin{pmatrix} n \\\\ k \\end{pmatrix}\\) for non-negative integers \\(n\\) and \\(k\\).\n\n\n\n# solution using choose(n, k)"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-3",
    "href": "07-Common-Discrete-RandVar.html#question-3",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 3",
    "text": "Question 3\n\nAccording to a recent article “Why Independent Voters Are Key To Winning Colorado”, PBS News. Oct 3, 2020, approximately 42% of Colorado’s voters identify as independent.\nIn solving Question 2, there is a general pattern we can model to compute the probability of choose exactly \\(x\\) jurors that identify as an independent voters in a jury of 5 people. In this question, we generalize the pattern to answer questions about a jury of 12 people.\n\nQuestion 3a\n\nWhat is the probability of randomly selecting a jury of 12 people with exactly 0 jurors that identify as an independent voter?\n\nSolution to Question 3a\n\n\n# Use R to help compute P(X=0)\n\n\n\n\n\n\nQuestion 3b\n\nWhat is the probability of randomly selecting a jury of 12 people with exactly 1 juror that identify as an independent voter?\n\nSolution to Question 3b\n\n\n# Use R to help compute P(X=1)\n\n\n\n\n\n\nQuestion 3c\n\nWhat is the probability of randomly selecting a jury of 12 people with exactly 2 jurors that identify as independent voters?\n\nSolution to Question 3c\n\n\n# Use R to help compute P(X=2)\n\n\n\n\n\n\nQuestion 3d\n\nWhat is the probability of randomly selecting a jury of 12 people with at most 2 jurors that identify as independent voters?\n\nSolution to Question 3d\n\n\n# Use R to help compute probability X is at most 2\n\n\n\n\n\n\nQuestion 3e\n\nWhat is the probability of randomly selecting a jury of 12 people with at least 2 jurors that identify as independent voters?\n\nSolution to Question 3e\n\n\n# Use R to help compute probability X is at least 2"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-4",
    "href": "07-Common-Discrete-RandVar.html#question-4",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 4",
    "text": "Question 4\n\nA jury of 12 people is randomly selected from a population that is 42% independent voters. Let random variable \\(X\\) denote the number of jurors that identify as an independent voter.\n\nQuestion 4a\n\nUsing your answers to Question 3, give the values for \\(p(0)=P(X=0)\\), \\(p(1)=P(X=1)\\), \\(p(2)=P(X=2)\\), and \\(F(2) = P(X \\leq 2)\\).\n\nSolution to Question 4a\n\n\n\\(p(0)=\\) ??\n\\(p(1)=\\) ??\n\\(p(2)=\\) ??\n\\(F(2)=\\) ??\n\n\n\n\n\n\nQuestion 4b\n\nGive a possible formula for the pmf of \\(X\\), \\(p(x)=P(X=x)\\).\n\nSolution to Question 4b\n\nA formula has been partially completed. Fill in the missing parts.\n\\[P(X=x) = \\begin{pmatrix} ?? \\\\ ?? \\end{pmatrix} (0.42)^{??} (1-0.42)^{??}\\]"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#a-trial-with-two-possible-outcomes",
    "href": "07-Common-Discrete-RandVar.html#a-trial-with-two-possible-outcomes",
    "title": "2.4: Common Discrete Distributions",
    "section": "A Trial with Two Possible Outcomes",
    "text": "A Trial with Two Possible Outcomes\n\nA Bernoulli trial is an experiment that has exactly two possible outcomes:\n\nThe probability that the outcome of a trial is a success (\\(\\color{dodgerblue}{X=1}\\)) is denoted \\(\\color{dodgerblue}{p}\\).\nOtherwise, the probability of a failure (\\(\\color{tomato}{X=0}\\)) is \\(\\color{tomato}{q=1-p}\\).\n\\(X\\) has a Bernoulli Distribution with probability mass function\n\n\\[f(x) = \\left\\{ \\begin{array}{ll}\np^x(1-p)^{1-x} & \\mbox{for } x \\in \\left\\{ 0, 1 \\right \\} \\\\\n0 , & \\mbox{otherwise}\n\\end{array} \\right.\\]"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#a-formula-for-the-binomial-distribution",
    "href": "07-Common-Discrete-RandVar.html#a-formula-for-the-binomial-distribution",
    "title": "2.4: Common Discrete Distributions",
    "section": "A Formula for the Binomial Distribution",
    "text": "A Formula for the Binomial Distribution\n\nLet random variable \\(X\\) be the number of successes out of \\(n\\) trials, where each trial is identical and independent.\n\n\\(X\\) has a Binomial Distribution, written \\(\\color{dodgerblue}{X \\sim \\mbox{Binom}(n,p)}\\).\nThe probability mass function is\n\n\\[f(x) = \\left\\{ \\begin{array}{ll}\n\\left( \\begin{array}{c} n\\\\ x \\end{array} \\right) p^x(1-p)^{n-x} & \\mbox{for } x =0,1,2, \\ldots , n \\\\\n0 & \\mbox{otherwise}\n\\end{array} \\right. \\]\n\n\n\n\n\n\nNote\n\n\n\nSee Appendix: Typsetting Arrays in LaTeX for more information on how to typeset formulas such as the piecewise function above using LaTeX."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#expected-value-and-variance-of-binomial-distributions",
    "href": "07-Common-Discrete-RandVar.html#expected-value-and-variance-of-binomial-distributions",
    "title": "2.4: Common Discrete Distributions",
    "section": "Expected Value and Variance of Binomial Distributions",
    "text": "Expected Value and Variance of Binomial Distributions\n\n\nThe expected value can be calculated with the shortcut \\(\\color{dodgerblue}{E(X) = np}\\).\nThe variance can be calculated with the shortcut \\(\\color{dodgerblue}{\\mbox{Var}(X) = np(1-p) = npq}\\)."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#binomial-distribution-functions-in-r",
    "href": "07-Common-Discrete-RandVar.html#binomial-distribution-functions-in-r",
    "title": "2.4: Common Discrete Distributions",
    "section": "Binomial Distribution Functions in R",
    "text": "Binomial Distribution Functions in R\n\nIn R, the we can use the functions:\n\ndbinom(x, n, p) calculates the probability of exactly \\(x\\) successes out \\(n\\) trials, \\(\\color{dodgerblue}{p(x)=P(X=x)}\\).\npbinom(x, n, p) calculates the probability of at most \\(x\\) successes out \\(n\\) trials, \\(\\color{tomato}{F(x)=P(X \\leq x)}\\).\nrbinom(m, n, p) randomly sample \\(m\\) values from \\(X \\sim \\mbox{Binom}(n, p)\\) (with replacement).\nqbinom(q, n, p) compute the qth quantile."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#plotting-a-binomial-distribution",
    "href": "07-Common-Discrete-RandVar.html#plotting-a-binomial-distribution",
    "title": "2.4: Common Discrete Distributions",
    "section": "Plotting a Binomial Distribution",
    "text": "Plotting a Binomial Distribution\n\nThe figure below gives the graphs of the pmf and cdf for a binomial distribution with \\(n=40\\) and \\(p = 0.25\\)."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-5",
    "href": "07-Common-Discrete-RandVar.html#question-5",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 5",
    "text": "Question 5\n\nSuppose a manufacturer of electronic chargers knows that 5% of the chargers it produces are defective. The manufacturer sends a shipment of 20 randomly selected chargers to a customer. Write an R command to compute the probability of each event.\n\nQuestion 5a\n\nAll 20 chargers sent are good (not defective).\n\nSolution to Question 5a\n\n\n# solution to 5a\n\n\n\n\n\n\nQuestion 5b\n\nAt most 18 batteries are good.\n\nSolution to Question 5b\n\n\n# solution to 5b\n\n\n\n\n\n\nQuestion 5c\n\nAt most 2 batteries are defective.\n\nSolution to Question 5c\n\n\n# solution to 5c"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-6",
    "href": "07-Common-Discrete-RandVar.html#question-6",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 6",
    "text": "Question 6\n\nLet \\(X\\) be the value of the result of rolling a fair six-sided die.\n\nQuestion 6a\n\nWrite out the probability mass function \\(f(x)\\).\n\nSolution to Question 6a\n\n\n\\[f(x) = \\left\\{ \\begin{array}{ll}\n?? & x = 1, 2, 3, 4, 5, 6 \\\\\n0 & \\mbox{otherwise}\n\\end{array} \\right.\\]\n\n\n\n\n\nQuestion 6b\n\nWhat is the expected value of rolling a fair-six sided die?\n\nSolution to Question 6b\n\n\n\n\n\n\n\nQuestion 6c\n\nWhat is the expected value of rolling a fair die with sides \\(1, 2, 3, \\ldots , k\\)?\n\n\n\n\n\n\nTip\n\n\n\nRecall the sum of the first \\(k\\) integers is \\(\\displaystyle \\sum_{n=1}^k n = \\frac{k(k+1)}{2}\\).\n\n\n\nSolution to Question 6c\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee Appendix: The Discrete Uniform Distribution for a summary of key formulas, graphs, shortcuts, and R functions for working with a Discrete Uniform Distribution."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-7",
    "href": "07-Common-Discrete-RandVar.html#question-7",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 7",
    "text": "Question 7\n\nA sports marketer for the Denver Nuggets randomly calls people in the Denver area until she encounters someone who attended a Nuggets game last season. Suppose we know that 10% of the population attended a Nuggets game last season, and we consider a call to a person who did attend a game last season as a success. What is the probability that the marketer has their first success on their eighth call?\n\nSolution to Question 7\n\n\n# solution to question 7"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-8",
    "href": "07-Common-Discrete-RandVar.html#question-8",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 8",
    "text": "Question 8\n\nSuppose we know that on average twelve cars cross a certain bridge each minute during rush hour. Find the probability that seventeen or more cars cross the bridge during a one minute span of time in rush hour.\n\nSolution to Question 8\n\n\n# solution to question 8"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-9",
    "href": "07-Common-Discrete-RandVar.html#question-9",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 9",
    "text": "Question 9\n\nRecently, a nurse commented that when a patient calls the medical advice line claiming to have the flu, the chance that he or she truly has the flu (and not just a nasty cold) is only about 4%. Of the next 25 patients calling in claiming to have the flu, what is the probability that exactly 4 patients will have the flu?\n\nSolution to Question 9\n\n\n# solution to question 9"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-10",
    "href": "07-Common-Discrete-RandVar.html#question-10",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 10",
    "text": "Question 10\n\nA online retailer sells an average of 5 big screen TV’s on a given day. What is the probability they sell 9 TV’s in a day?\n\nSolution to Question 10\n\n\n# solution to question 10"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#question-11",
    "href": "07-Common-Discrete-RandVar.html#question-11",
    "title": "2.4: Common Discrete Distributions",
    "section": "Question 11",
    "text": "Question 11\n\nIt is known that 3% of airbags manufactured by a certain car company are defective. What is the probability that the first defective air bag occurs when the fifth item is inspected?\n\nSolution to Question 11\n\n\n# solution to question 11"
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#sec-append-binom",
    "href": "07-Common-Discrete-RandVar.html#sec-append-binom",
    "title": "2.4: Common Discrete Distributions",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\n\\(X \\sim \\mbox{Binom}(n,p)\\) is number of successes from \\(n\\) identical and independent trials each with probability of success \\(p\\).\n\\[f(x; n, p) = \\left\\{ \\begin{array}{ll} \\left( \\begin{array}{c} n\\\\ x \\end{array} \\right) p^x(1-p)^{n-x} \\ \\ & \\mbox{for } x =0,1,2, \\ldots , n\\\\    & \\\\    0 \\ \\ & \\mbox{otherwise} \\end{array} \\right.\\]\n\nThe expected value is \\(E(X) = \\mu_X = np\\).\nThe variance is \\(\\mbox{Var} (X) = \\sigma_X^2 = np(1-p)\\).\nUse dbinom(x, n, p) to calculate \\(p(x) = P(X = x)\\).\nUse pbinom(x, n, p) to calculate \\(F(x) = P(X \\leq x)\\).\nUse rbinom(m, n, p) to generate a random sample of size \\(m\\) from population \\(X \\sim \\mbox{Binom}(n,p)\\).\nUse qbinom(q, n, p) to find the qth percentile of \\(X \\sim \\mbox{Binom}(n,p)\\)."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#sec-append-unif",
    "href": "07-Common-Discrete-RandVar.html#sec-append-unif",
    "title": "2.4: Common Discrete Distributions",
    "section": "The Discrete Uniform Distribution",
    "text": "The Discrete Uniform Distribution\n\nThere are \\(k\\) distinct outcomes each with an equal likelihood of occurring.\n\\[f(x; k) = \\left\\{ \\begin{array}{ll} \\dfrac{1}{k} \\ \\ & \\mbox{for } x = 1, 2, \\ldots,  k\\\\ 0 \\ \\ & \\mbox{otherwise} \\end{array} \\right.\\]\n\nThe expected value is \\(\\displaystyle E(X) = \\mu_X = \\frac{k+1}{2}\\).\nThe variance is \\(\\displaystyle \\mbox{Var} (X) = \\sigma_X^2 = \\frac{k^2-1}{12}\\).\nNote functions such as dunif(), punif(), runif(), and qunif() are for the continuous (not discrete) uniform distribution.\nIt is typically easier to calculate probabilities directly rather than use technology."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#sec-append-geom",
    "href": "07-Common-Discrete-RandVar.html#sec-append-geom",
    "title": "2.4: Common Discrete Distributions",
    "section": "The Geometric Distribution",
    "text": "The Geometric Distribution\n\n\\(X \\sim \\mbox{Geom}(p)\\) is the number of failures that occur before the first success, where \\(p\\) denotes the probability of a success and \\(q=1-p\\) is the probability of a failure.\n\\[f(x; p) = \\left\\{ \\begin{array}{ll} q^xp  \\ \\ & \\mbox{for }  x = 0, 1, 2, \\ldots  \\\\\n0 & \\mbox{otherwise} \\end{array} \\right.  .\\]\n\nThe expected value is \\(E(X) = \\mu_X = \\dfrac{q}{p}\\).\nThe variance is \\(\\mbox{Var} (X) = \\sigma_X^2 = \\dfrac{q}{p^2}\\).\nUse dgeom(x, p) to calculate \\(p(x) = P(X = x)\\).\nUse pgeom(x, p) to calculate \\(F(x) = P(X \\leq x)\\).\nUse rgeom(m, p) to generate a random sample of size \\(m\\) from population \\(X \\sim \\mbox{Geom}(p)\\).\nUse qgeom(q, p) to find the qth percentile of \\(X \\sim \\mbox{Geom}(p)\\)."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#sec-append-pois",
    "href": "07-Common-Discrete-RandVar.html#sec-append-pois",
    "title": "2.4: Common Discrete Distributions",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\n\n\\(X \\sim \\mbox{Pois}(\\lambda)\\) is the number of times an event occurs in a fixed time period, where \\(\\lambda\\) denotes the mean number of occurrences in the given time period.\n\\[f(x; \\lambda) = \\left\\{ \\begin{array}{ll}  e^{-\\lambda} \\dfrac{\\lambda^x}{x!} \\ \\ & \\mbox{for } x = 0, 1, 2, \\ldots  \\\\\n0 & \\mbox{otherwise} \\end{array} \\right. .\\]\n\nThe expected value is \\(E(X) = \\mu_X = \\lambda\\).\nThe variance is \\(\\mbox{Var} (X) = \\sigma_X^2 = \\lambda\\).\nUse dpois(x, lambda) to calculate \\(p(x) = P(X = x)\\).\nUse ppois(x, lambda) to calculate \\(F(x) = P(X \\leq x)\\).\nUse rpois(m, lambda) to generate a random sample of size \\(m\\) from population \\(X \\sim \\mbox{Pois}(\\lambda)\\).\nUse qpois(q, lambda) to find the qth percentile of \\(X \\sim \\mbox{Pois}(\\lambda)\\)."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#sec-append-why",
    "href": "07-Common-Discrete-RandVar.html#sec-append-why",
    "title": "2.4: Common Discrete Distributions",
    "section": "Why Divide By \\(k!\\)?",
    "text": "Why Divide By \\(k!\\)?\n\nWhere does \\(k!\\) possible ways of permuting the order come from?\n\nThe symmetric group \\(\\color{dodgerblue}{S_k}\\) on \\(k\\) items consists of all the possible ways of permuting the order of the \\(k\\) items.\nEach element corresponds to a permutation applied to the group of \\(k\\) items.\nFor example \\(S_3\\) is the symmetric group on three items.\n\nThe identity element is \\((1,2,3)\\) which does not change the ordering of items \\(1, 2, 3\\).\nThe element \\((1, \\color{tomato}{3}, \\color{tomato}{2})\\) is the transpose of items 2 and 3.\nThe element \\((3, 1, 2)\\) is a composition of two transpositions: - First transpose items 1 and 2, that is element \\((\\color{tomato}{2}, \\color{tomato}{1}, 3)\\). - Then transpose items 1 and 3 to get \\((\\color{tomato}{3}, 1, \\color{tomato}{2})\\).\n\nThe number of elements in the group is called the order of the group and denoted \\(\\color{dodgerblue}{|S_k|}\\).\nFor example, \\(S_3\\) has \\(|S_3|=6=3!\\) elements that can be represented as\n\n\\[(1,2,3), (1,3,2), (2, 1, 3), (2, 3, 1), (3, 2, 1), \\mbox{ and } (3, 1, 2).\\]\n\nIn fact, for any symmetric group we have \\(\\color{dodgerblue}{|S_k|=k!}\\).\nThus, there are \\(k!\\) ways of permutating the order of \\(k\\) items."
  },
  {
    "objectID": "07-Common-Discrete-RandVar.html#sec-append-latex",
    "href": "07-Common-Discrete-RandVar.html#sec-append-latex",
    "title": "2.4: Common Discrete Distributions",
    "section": "Typsetting Arrays in LaTeX",
    "text": "Typsetting Arrays in LaTeX\n\n\nThe array environment is started with \\begin{array}.\nNext we indicate how many columns and how each column is aligned.\n\n{ll} means two columns aligned to the left.\n{lrc} would be three columns: the first aligned left, then right, then the last column is centered.\nWe use the & symbol to indicate a column break.\nWe use \\\\ to indicate a row break.\n\nIf we want a big curly brace on the left of the array, enter \\left\\{ before beginning the array.\nWe do not want a brace on the other side of the array, so we use \\right. to close off the left brace without using any matching symbol on the right.\nWe use \\mbox{otherwise} to type the text otherwise inside the equation.\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html",
    "href": "08-Common-Continuous-Distributions.html",
    "title": "2.5: Common Continuous Distributions",
    "section": "",
    "text": "Normal Distributions\nThere are many situations where data tends to be distributed symmetrically around a central value with no bias to the left or right such as the location of beads on a Galton board or heights of people. For example, if we randomly select an adult from the population:\nNormal distributions arise in many settings: heights of people, size of items produced by machines, and most importantly in statistics, data sets resulting from many independent random events. The shape of a normal distribution is determined by two parameters:\nThe empirical rule for normal distributions states approximately:\nThe number of standard deviations a data value \\(x\\) is from the mean \\(\\mu\\) is called the z-score for the value \\(x\\).\n\\[\\color{dodgerblue}{\\large z = \\frac{x-\\mu}{\\sigma}}.\\]\nFor example, the \\(z\\)-score of an adult with a total cholesterol level of 140 mg/dL is \\(z=\\frac{140-200}{40} = -1.5\\). This means a total cholesterol level of 140 mg/dL is 1.5 standard deviations below the mean level.\nWhen we compute the \\(z\\)-score, we are “standardizing” the normally distributed data. We describe values of random variable \\(X\\) in terms of how many standard deviations each value \\(x\\) is from the mean \\(\\mu\\). The resulting distribution of \\(z\\)-scores is called the standard normal distribution that has mean \\(0\\) and standard deviation \\(1\\).\nThe probability density function for a normal distribution \\(X \\sim N(\\mu,\\sigma)\\) is called a Gaussian function whose formula is\n\\[\\color{dodgerblue}{\\large f(x; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2}}.\\]\nTo find the probability that an adult woman in Israel has a BMI less than \\(18.5\\), we need to calculate the area to the left of \\(x=18.5\\) under the normal curve with formula \\(f(x; 26.5, 4.5)\\). We can try to evaluate\n\\[P(X &lt; 18.5) = \\int_{0}^{18.5} \\frac{1}{4.5\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x-26.5}{4.5} \\right)^2} \\, dx\\]\nIf a continuous random variable is uniformly distributed on the interval \\(\\lbrack a , b \\rbrack\\), then the pdf is\n\\[f(x; a, b) = \\left\\{ \\begin{array}{ll} \\frac{1}{b-a}, & a \\leq x \\leq b\\\\ 0, & \\mbox{otherwise} \\end{array} \\right. .\\]\nDistributions come in all shapes in and sizes. For example:\nExponential distributions are often useful when considering the length of time between successive occurrences of an event."
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-1",
    "href": "08-Common-Continuous-Distributions.html#question-1",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 1",
    "text": "Question 1\n\nWhat effect does increasing the standard deviation have on the shape of a normal distribution? What effect does increasing the mean have on the shape of a normal distribution?\n\nSolution to Question 1"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-2",
    "href": "08-Common-Continuous-Distributions.html#question-2",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 2",
    "text": "Question 2\n\nCholesterol is a fat-like substance present in all cells in your body. Humans require some cholesterol to properly function and remain healthy. If a person’s blood has too much cholesterol, then they are at a higher risk of developing heart disease. A blood test is used to measure cholesterol levels. Below is an excerpt from a study1 on the distribution of cholesterol levels.\n\nCholesterol is a growing issue because of its impact on human health. Cigarette smoking, high blood pressure, and high blood cholesterol are the most clearly established risk factors that have been identified as being strongly associated with coronary heart disease (CHD). Total serum cholesterol level (SCL) is a major risk factor for CHD which is the leading cause of death in the United States. CHD is responsible for more deaths than all forms of cancer combined.\n\nThe distribution of cholesterol levels for adults age 20 or older is approximately normally distributed with a mean of 200 mg/dL and standard deviation 40 mg/dL. Let \\(X\\) be the cholesterol level of a randomly selected adult age 20 or above. Thus, we have \\(X \\sim N(200, 40)\\).\n\nQuestion 2a\n\nWhat proportion of adults age 20 or above have total cholesterol levels between 200 mg/dL and 240 mg/dL?\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nWhat proportion of adults age 20 or above have total cholesterol levels between 160 mg/dL and 280 mg/dL?\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nIndividuals that have a total cholesterol level more than 240 mg/dL should be regarded as high risk for heart disease. What proportion of the adult population age 20 or above is at high risk for heart disease?\n\nSolution to Question 2c\n\n\n\n\n\n\n\nQuestion 2d\n\nHow many standard deviations from the mean is a total cholesterol level of 140 mg/dL?\n\nSolution to Question 2d"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-3",
    "href": "08-Common-Continuous-Distributions.html#question-3",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 3",
    "text": "Question 3\n\nBody Mass Index (BMI) is often used as a measurement to determine whether a person’s weight is considered healthy. BMI is computed by dividing a person’s weight (in kilograms) by their height (in meters) squared . BMI is easy and inexpensive to measure, thus it is frequently used to screen for whether a person is malnourished, healthy, or overweight. For example, some countries have passed laws preventing companies from hiring models that are considered underweight based on BMI.\n\nIsrael became the first country ever to pass legislation banning the use of “underweight” models in local ads and publications. The new law employs an interesting tactic: Models must prove that their Body Mass Index (BMI) is higher than the World Health Organization’s indication of malnourishment (a BMI of 18.5) by producing an up-to-date medical report — no older than three months — at all shoots to be used in the Israeli market2.\n\nLike heights and cholesterol levels, BMI distribution is approximately normal. In Israel, adult women have a mean BMI of \\(26.5\\) with a standard deviation \\(4.5\\).\n\nQuestion 3a\n\nCalculate the \\(z\\)-score of an adult Israeli woman with a BMI of \\(18.5\\).\n\nSolution to Question 3a\n\n\n\n\n\n\n\nQuestion 3b\n\nInterpret the meaning of the value in Question 3a, and give an estimate for the proportion of adult women in Israel who are legally underweight.\n\nSolution to Question 3b"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#using-a-standard-normal-distribution-table",
    "href": "08-Common-Continuous-Distributions.html#using-a-standard-normal-distribution-table",
    "title": "2.5: Common Continuous Distributions",
    "section": "Using a Standard Normal Distribution Table",
    "text": "Using a Standard Normal Distribution Table\n\nA standard normal distribution table can be used to determine the areas under the standard normal distribution, \\(N(0,1)\\). There are different variations of standard normal distributions. The table in the figure below gives the area under the standard normal distribution to the left of a given \\(z\\)-score. For example, if we want to determine what proportion of adult women in Israel have a BMI less than \\(18.5\\):\n\nFind the corresponding \\(z\\)-score of the BMI measurement \\(x=18.5\\). In Question 3a, we determined \\(z \\approx -1.78\\).\nLook down the first column of the table and identify the row corresponding to the \\(z\\)-score to one decimal place of accuracy, \\(z=-1.7\\)\nIdentify the column corresponding to the second decimal place of the \\(z\\)-score, \\(0.08\\).\nThe area to the left of \\(z=-1.78\\) is the value located at the intersection of the row in step 2 and column in step 3.\n\nFrom the figure below, we have \\(P(X &lt; 18.5) = P(Z &lt; -1.78) \\approx 0.0375\\).\n\n\n\nUsing a Standard Normal Distribution Table"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#using-r-to-compute-areas",
    "href": "08-Common-Continuous-Distributions.html#using-r-to-compute-areas",
    "title": "2.5: Common Continuous Distributions",
    "section": "Using R to Compute Areas",
    "text": "Using R to Compute Areas\n\nStandard normal distribution tables were very useful prior to spread statistical software and technology. If we are working in R, we do not need to use tables to get approximations. Instead, we can use R functions to compute these areas more accurately.\nSimilar to binomial, geometric, and Poisson distributions, R has built in functions to perform calculations with normal distributions. In particular, the function pnorm() calculates values of the cumulative distribution function for a normal distribution:\n\npnorm(x, mean, sd)\\(=P(X&lt;x)\\) gives the area to the left of \\(x\\) under \\(N(\\mu,\\sigma)\\).\npnorm(x, mean,  sd, lower.tail=FALSE)\\(=P(X&gt;x)\\) gives the area to the right of \\(x\\) under \\(N(\\mu,\\sigma)\\).\n\nOr you can find the \\(z\\)-score and use the standard normal distribution in R as well:\n\npnorm(z, 0, 1)\\(=P(Z&lt;z)\\) gives the area to the left of \\(x\\) under \\(N(0,1)\\).\npnorm(z, 0,  1, lower.tail=FALSE)\\(=P(Z&gt;z)\\) gives the area to the right of \\(x\\) under \\(N(0,1)\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not use the function dnorm(x, mean, sd) to compute probabilities related to normal distribution since dnorm() returns the height of the probability density function. For continuous random variables, \\(P(X &lt; x)\\) is the area to the left of \\(x\\), not the height of the function. Be sure to use pnorm(x, mean, sd) to compute \\(P(X &lt; x)\\)."
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-4",
    "href": "08-Common-Continuous-Distributions.html#question-4",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 4",
    "text": "Question 4\n\nUse R to compute the proportion of adult women in Israel that have a BMI below \\(18.5\\).\n\nSolution to Question 4\n\n\n# use code cell to help with calculation in question 4"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-5",
    "href": "08-Common-Continuous-Distributions.html#question-5",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 5",
    "text": "Question 5\n\nIntelligence quotient (IQ) scores are normally distributed. The mean IQ score is 100 points and the standard deviation is 16 points.\n\nQuestion 5a\n\nWhat proportion of people have an IQ score above 116?\n\nSolution to Question 5a\n\n\n\n\n\n\n\nQuestion 5b\n\nMarilyn vos Savant has been known to have the highest recorded IQ in the world. The \\(z\\)-score of her test result is \\(z=5.4\\). What is her IQ score?\n\nSolution to Question 5b\n\n\n\n\n\n\n\nQuestion 5c\n\nWhat proportion of people have an IQ score between 75 and 90?\n\nSolution to Question 5c\n\n\n\n\n\n\n\nQuestion 5d\n\nWhat is the 90th percentile for IQ? In other words, find the IQ score such that 90% of the people score less than that score?\n\n\n\n\n\n\nTip\n\n\n\nSee Appendix: Normal Distributions for a summary of key formulas, graphs, shortcuts, and R functions for normal distributions.\n\n\n\nSolution to Question 5d"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-6",
    "href": "08-Common-Continuous-Distributions.html#question-6",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 6",
    "text": "Question 6\n\nLet continuous random variable \\(X\\) be uniformly distributed over the closed interval \\(\\lbrack a , b \\rbrack\\). Derive a formula for \\(\\mbox{Var}(X)\\). Your answer will depend on parameters \\(a\\) and \\(b\\).\n\nSolution to Question 6"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-7",
    "href": "08-Common-Continuous-Distributions.html#question-7",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 7",
    "text": "Question 7\n\nLet \\(X\\) denote the time (in minutes) spent waiting for the next light-rail to arrive at Union Station. On a separate piece of paper, sketch a possible graph for the probability distribution function of \\(X\\). Explain how you determined the shape of your graph.\n\nSolution to Question 7"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#question-8",
    "href": "08-Common-Continuous-Distributions.html#question-8",
    "title": "2.5: Common Continuous Distributions",
    "section": "Question 8",
    "text": "Question 8\n\nAt a 911 call center, calls come in at an average rate of one call every two minutes. Let \\(X\\) denote the time that elapses from one call to the next, and assume \\(X\\) has an exponential distribution.\n\n\n\n\n\n\nTip\n\n\n\nSee Appendix: Exponential Distributions for useful properties of exponential distributions.\n\n\n\nQuestion 8a\n\nGive a formula and sketch the graph of the pdf \\(f\\) (on a separate piece of paper).\n\nSolution to 8a\n\n\n\n\n\n\n\nQuestion 8b\n\nFind the probability that after a call is received, it takes more than three minutes for the next call to occur. Illustrate this value on your graph in Question 8a.\n\nSolution to 8b\n\n\n\n\n\n\n\nQuestion 8c\n\nFind a formula for the cdf, \\(F\\).\n\nSolution to 8c\n\n\n\n\n\n\n\nQuestion 8d\n\nFind a formula for the inverse of the cdf, \\(F^{-1}\\).\n\nSolution to 8d\n\n\n\n\n\n\n\nQuestion 8e\n\nNinety-percent of all calls occur within how many minutes of the previous call?\n\n\n\n\n\n\nTip\n\n\n\nUse your previous answer in Question 8d.\n\n\n\nSolution to 8e\n\n\n\n\n\n\n\nQuestion 8f\n\nSuppose that two minutes have elapsed since the last call. Find the probability that the next call will occur within the next minute.\n\n\n\n\n\n\nTip\n\n\n\nThis is a conditional probability, so consider using Bayes’ Theorem.\n\n\n\nSolution to 8f"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#sec-append-normal",
    "href": "08-Common-Continuous-Distributions.html#sec-append-normal",
    "title": "2.5: Common Continuous Distributions",
    "section": "Normal Distributions",
    "text": "Normal Distributions\n\nIf \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we write \\(\\color{dodgerblue}{X \\sim N( \\mu ,\\sigma)}\\). The formula for the probability density is called the Gaussian and is given below.\n\\[f(x; \\mu, \\sigma) = \\dfrac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2}\\]\n\nThe expected value is \\(E(X) = \\mu\\).\nThe variance is \\(\\mbox{Var} (X) = \\sigma^2\\).\nUse dnorm(x, mu, sigma) to calculate \\(f(x)\\).\n\nRecall for continuous distributions, \\(f(x)\\) gives the height of the pdf.\nProbabilities are areas under \\(f(x)\\) (not the heights).\nDO NOT use dnorm() to compute probabilities.\n\nUse pnorm(x, mu, sigma) to calculate \\(P(X \\leq x)\\), the area to the left of \\(X=x\\).\n\nOne last warning, be sure to use pnorm() to compute probabilities not dnorm().\n\nUse rnorm(n, mu, sigma) to generate a random sample of size \\(n\\) from population \\(X \\sim N(mu, sigma)\\).\nUse qnorm(q, mu, sigma) to find the qth percentile.\nWe have the 68%-95%-99.5% Empirical Rule.\nWe can standardize the normal distribution using z-scores: \\[z = \\frac{x - \\mu}{\\sigma}\\]"
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#sec-append-unif",
    "href": "08-Common-Continuous-Distributions.html#sec-append-unif",
    "title": "2.5: Common Continuous Distributions",
    "section": "Continuous Uniform Distributions",
    "text": "Continuous Uniform Distributions\n\n\\(X\\) is a continuous uniform distribution when \\(X\\) is equally likely to equal to any value on the closed, continuous interval \\(\\lbrack a , b \\rbrack\\). If a continuous random variable is uniformly distributed on the interval \\(\\lbrack a , b \\rbrack\\), the pdf is\n\\[f(x; a,b) = \\left\\{ \\begin{array}{ll} \\dfrac{1}{b-a} & a \\leq x \\leq b\\\\ 0 & \\mbox{otherwise} \\end{array} \\right. .\\]\nThe corresponding cdf is\n\\[F(x) = \\left\\{ \\begin{array}{ll}\n0, &  x&lt;a \\\\\n\\frac{x-a}{b-a}, &  a \\leq x \\leq b \\\\\n1,  &  x&gt;b\n\\end{array} \\right. .\\]\n\nThe expected value is \\(E(X) = \\dfrac{a+b}{2}\\)\nThe variance is \\(\\mbox{Var}(X) = \\dfrac{(b-a)^2}{12}\\)\n\nWhen working with uniform distributions, it is typically easier to calculate probabilities “by hand” without the need for technology. However, R does have functions to help!\n\nUse dunif(x, a, b) to find the height of the pdf function, which is \\(\\frac{1}{b-a}\\).\n\nThere is really no need to use the function since the pdf has a constant height.\nDO NOT use dunif() to compute probabilities.\n\nUse punif(x, a, b) to calculate \\(P(X \\leq x)=\\int_a^x \\frac{1}{b-a} \\, dt = \\frac{x-a}{b-a}\\).\n\nProbabilities can be found by computing areas of rectangles which may be easier!\n\nUse runif(n, a, b) to generate a random sample of size \\(n\\) from population \\(X \\sim \\mbox{Unif}(a, b)\\).\nUse qunif(q, a, b) to find the qth percentile."
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#sec-append-exp",
    "href": "08-Common-Continuous-Distributions.html#sec-append-exp",
    "title": "2.5: Common Continuous Distributions",
    "section": "Exponential Distributions",
    "text": "Exponential Distributions\n\nLet \\(X\\) be the amount of time between the successive events if we know the average time between occurrences is \\(\\mu\\). The rate parameter \\(\\lambda = \\frac{1}{\\mu}\\) is the average number of times the event occurs per unit of time. Then \\(X\\) is exponentially distributed with rate parameter \\(\\lambda\\), and we write \\(\\color{dodgerblue}{X \\sim \\mbox{Exp} (\\lambda)}\\).\nThe pdf for \\(X \\sim \\mbox{Exp} (\\lambda)\\) is the exponential function\n\\[f(x; \\lambda) = \\lambda e^{-\\lambda x} \\quad \\mbox{for } x &gt;0 .\\]\n\nThe expected value is \\(E(X) = \\dfrac{1}{\\lambda}=\\mu\\)\nThe variance is \\(\\mbox{Var}(X) = \\dfrac{1}{\\lambda^2} = \\mu^2\\).\nUse dexp(x, lambda) to find the height of the pdf function, which is \\(\\lambda e^{-\\lambda x}\\).\n\nDO NOT use dexp() to compute probabilities.\n\nUse pexp(x, lambda) to calculate \\(P(X \\leq x)=\\int_0^x \\lambda e^{-\\lambda t} \\, dt\\).\n\nIt never hurts to practice your integration and check with pexp()!\n\nUse rexp(n, lambda) to generate a random sample of size \\(n\\) from population \\(X \\sim \\mbox{Exp}(\\lambda)\\).\nUse qexp(q, lambda) to find the qth percentile."
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#sec-append-gamma",
    "href": "08-Common-Continuous-Distributions.html#sec-append-gamma",
    "title": "2.5: Common Continuous Distributions",
    "section": "Gamma Distributions",
    "text": "Gamma Distributions\n\nA gamma distribution is a distribution that arises naturally in processes for which the waiting times between events are relevant. Suppose we know that \\(\\mu\\) is the mean amount of time between successive occurrences of an event. Let \\(X\\) be the amount of time it takes for the event to occur \\(r\\) times.\n\nThe rate parameter \\(\\color{dodgerblue}{\\lambda = \\frac{1}{\\mu}}\\) tells us how many times an event occurs per unit of time.\nThe shape parameter \\(\\color{dodgerblue}{r}\\) denotes the total number of occurrences we will wait to occur.\n\nThen \\(X\\) will follow a gamma distribution that we denote \\(\\color{dodgerblue}{X \\sim \\mbox{Gamma} ( r, \\lambda)}\\). The pdf is given by\n\\[f(x;r, \\lambda) = \\frac{1}{(r-1)!} \\lambda^r x^{r-1} e^{-\\lambda x} \\quad \\mbox{for } x &gt;0 .\\]\n\nThe expected value is \\(E(X) = \\dfrac{r}{\\lambda}\\)\nThe variance is \\(\\mbox{Var}(X) = \\dfrac{r}{\\lambda^2}\\).\nUse dgamma(x, r, lambda) to find the height of the pdf function which is generally not too useful.\n\nDO NOT use dgamma() to compute probabilities.\n\nUse pgamma(x, r, lambda) to calculate \\(P(X \\leq x)\\).\nUse rgamma(n, r, lambda) to generate a random sample of size \\(n\\) from population \\(X \\sim \\mbox{Gamma}(r, \\lambda)\\).\nUse qgamma(q, r, lambda) to find the qth percentile.\n\n\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "08-Common-Continuous-Distributions.html#footnotes",
    "href": "08-Common-Continuous-Distributions.html#footnotes",
    "title": "2.5: Common Continuous Distributions",
    "section": "",
    "text": "Prasad Tharu B, Tsokos CP. A Statistical Study of Serum Cholesterol Level by Gender and Race. J Res Health Sci. 2017 Jul 25. ↩︎\n“Israel Passes Law Requiring Models to Show Health Records and Meet Weight Standards”, New York Magazine by Charlotte Cowles on April 20, 2012↩︎"
  },
  {
    "objectID": "09-Joint-Distributions.html",
    "href": "09-Joint-Distributions.html",
    "title": "2.6: Joint Distributions",
    "section": "",
    "text": "Situations with Multiple Random Variables\nThus far we have been studying probability distributions for a single random variable. In many situations, we would like to incorporate multiple random variables in our analysis.\n\\[p_X(x) = P(X=x) = \\sum_y p(x,y). \\]\n\\[p_Y(y) = P(Y=y) = \\sum_x p(x,y).\\]\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint probability density function \\(\\color{dodgerblue}{f(x,y)}\\).\n\\[\\color{dodgerblue}{f_X(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy}. \\]\n\\[\\color{dodgerblue}{f_Y(y) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dx}. \\]\nLet \\(X\\) and \\(Y\\) be continuous random variables with joint pdf \\(f(x,y)\\). Then for any two dimensional subset \\(A \\subseteq \\mathbb{R}^2\\),\n\\[ P \\big( (X,Y) \\in A \\big) = \\int \\int_A f(x,y) \\, dx \\, dy .\\]\nTwo random variables \\(X\\) and \\(Y\\) are said to be independent if for every pair of \\(x\\) and \\(y\\) values,\n\\[\\begin{aligned}\n\\color{dodgerblue}{f(x,y) = f_X(x) \\cdot f_Y(y)} & \\qquad \\mbox{when } X \\mbox{ and } Y \\mbox{ are continuous, or}\\\\\n\\color{dodgerblue}{p(x,y) = p_X(x) \\cdot p_Y(y)} & \\qquad \\mbox{when } X \\mbox{ and } Y \\mbox{ are discrete.}\n\\end{aligned}\\]\nNotice this definition implies when \\(A\\) and \\(B\\) are independent events, then \\(\\color{dodgerblue}{P(A \\cap B) = P(A)P(B)}\\).\nLet \\(X\\) and \\(Y\\) be two random variables with joint pdf \\(f(x,y)\\). If \\(\\color{dodgerblue}{Z=h(X,Y)}\\), then\n\\[E( {\\color{dodgerblue}{Z}} ) = E( {\\color{dodgerblue}{h(X,Y)}} ) = \\left\\{ \\begin{array}{ll}\n\\displaystyle \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} {\\color{dodgerblue}{h(x,y)}} \\cdot f(x,y) \\, dx \\, dy , & \\mbox{if X and Y are continuous} \\\\\n& \\\\\n\\displaystyle \\sum_y \\sum_x {\\color{dodgerblue}{h(x,y)}} \\cdot f(x,y) , &  \\mbox{if X and Y are discrete} \\end{array} \\right. .\\]\nThis is often referred to as the Law of the Unconscious Statistician since we do not need to know the distribution \\(f_Z(z)\\) in order to compute \\(E(Z)\\).\nLet \\(X\\) and \\(Y\\) be two random variables and consider a linear combination \\(aX+bY\\) for \\(a\\) and \\(b\\) two constants. Then\n\\[\\color{dodgerblue}{E(aX+bY)=aE(X)+bE(Y)}.\\]\nA special case for products: Let \\(X\\) and \\(Y\\) be two independent random variables. Then additionally we have the following properties."
  },
  {
    "objectID": "09-Joint-Distributions.html#question-1",
    "href": "09-Joint-Distributions.html#question-1",
    "title": "2.6: Joint Distributions",
    "section": "Question 1",
    "text": "Question 1\n\nA large insurance agency services a number of customers who have purchased both a homeowner’s policy and an automobile policy from the agency. For each type of policy, a deductible amount must be specified. For an automobile policy, the choices are \\(\\$100\\) and \\(\\$250\\), whereas for a homeowner’s policy, the choices are \\(\\$0\\), \\(\\$100\\), and \\(\\$200\\).\nSuppose an individual with both types of policy is selected at random from the agency’s files. Let \\(A\\) be the deductible amount on the auto policy and \\(H\\) the deductible amount on the homeowner’s policy. The joint probability mass function for \\(A\\) and \\(H\\) is denoted \\(\\color{dodgerblue}{p(a,h)=P(A=a \\mbox{ and } H=h)}\\) and can be summarized in two-way tables.\n\n\n\n\\(p(a,h)\\)\n\\(H=0\\)\n\\(H=100\\)\n\\(H=200\\)\nTotal\n\n\n\n\n\\(a=100\\)\n\\(0.20\\)\n\\(0.10\\)\n\\(0.20\\)\n??\n\n\n\\(a=250\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.30\\)\n??\n\n\nTotal\n??\n??\n??\n\\(1\\)\n\n\n\n\nQuestion 1a\n\nInterpret the meaning of the value \\(p(100,0)=0.2\\) in this context.\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nCompute \\(P(A=250)\\) and interpret the meaning in this context.\n\nSolution to Question 1b\n\n\n\n\n\n\n\nQuestion 1c\n\nCompute \\(P(H=100)\\) and interpret the meaning in this context.\n\nSolution to Question 1c"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-2",
    "href": "09-Joint-Distributions.html#question-2",
    "title": "2.6: Joint Distributions",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the pmf from the insurance example Question 1, write a piecewise formula for \\(p_A(a)\\) and \\(p_H(h)\\).\n\nSolution to Question 2\n\n\\[p_A(a) = \\left\\{ \\begin{array}{ll}\n?? & a=100 \\\\\n?? & a=250 \\\\\n0 & \\mbox{otherwise}\n\\end{array} \\right.\\]\n\\[p_H(h) = \\left\\{ \\begin{array}{ll}\n?? & h=0 \\\\\n?? & h=100 \\\\\n?? & h=200 \\\\\n0 & \\mbox{otherwise}\n\\end{array} \\right.\\]"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-3",
    "href": "09-Joint-Distributions.html#question-3",
    "title": "2.6: Joint Distributions",
    "section": "Question 3",
    "text": "Question 3\n\nA grocery store has two types of checkout lines:\n\nSelf-checkout registers where customers scan items, pay, and bag their groceries on their own.\nFull-service registers where a cashier scans and bags items for the customer.\n\nOn a randomly selected day, let \\(X\\) be the proportion of time that the self-checkout registers are in use, and let \\(Y\\) be the proportion of time that the full-service cashiers are in use. Random variables \\(X\\) and \\(Y\\) are continuous each with values between 0 and 1. Then the set of possible values for the pair \\((X, Y)\\) is therefore rectangle \\(A= \\left\\{ (x, y): 0 \\leq x \\leq 1, 0 \\leq y \\leq 1 \\right\\}\\) in \\(\\mathbb{R}^2\\). Suppose the joint pdf of \\((X,Y)\\) is given by\n\\[ f(x,y) = \\left\\{ \\begin{array}{ll}\n\\dfrac{3}{4} \\left( 2x+y^2 \\right), & 0 \\leq x \\leq 1, 0 \\leq y \\leq 1\\\\\n0 , & \\mbox{otherwise}\n\\end{array} \\right. .\\]\n\nQuestion 3a\n\nGive a formula for \\(f_X(x)\\) (using integrals).\n\nSolution to Question 3a\n\n\n\n\n\n\n\nQuestion 3b\n\nUse your answer from Question 3a to calculate and interpret the value of \\(P \\left( 0 \\leq X \\leq \\frac{1}{4} \\right)\\).\n\nSolution to Question 3b\n\n\n\n\n\n\n\nQuestion 3c\n\nGive a formula for \\(f_Y(y)\\).\n\nSolution to Question 3c\n\n\n\n\n\n\n\nQuestion 3d\n\nCalculate and interpret the value of \\(\\displaystyle P \\left( 0 \\leq X \\leq \\frac{1}{4} , \\ 0 \\leq Y \\leq \\frac{1}{2} \\right)\\).\n\nSolution to Question 3d\n\n\n\n\n\n\n\nQuestion 3e\n\nSet up and evaluate a double integral to compute the probability that the self-checkout registers are in use more than the full-service registers.\n\nSolution to Question 3e"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-4",
    "href": "09-Joint-Distributions.html#question-4",
    "title": "2.6: Joint Distributions",
    "section": "Question 4",
    "text": "Question 4\n\nIn the insurance example in Question 1, are random variables \\(A\\) and \\(H\\) independent? Explain how you determined your answer, and then interpret the practical significance of your answer.\n\nSolution to Question 4"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-5",
    "href": "09-Joint-Distributions.html#question-5",
    "title": "2.6: Joint Distributions",
    "section": "Question 5",
    "text": "Question 5\n\nIn the grocery store example in Question 3, are random variables \\(X\\) and \\(Y\\) independent? Explain how you determined your answer, and then interpret the practical significance of your answer.\n\nSolution to Question 5"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-6",
    "href": "09-Joint-Distributions.html#question-6",
    "title": "2.6: Joint Distributions",
    "section": "Question 6",
    "text": "Question 6\n\nLet \\(X\\) and \\(Y\\) be the values (\\(1, 2, \\ldots ,6\\)) rolled by each of two die. Assume that \\(X\\) and \\(Y\\) are independent, and define the random variable \\(Z=h(x,y)=xy\\) which is the product of the two rolls. Calculate \\(E(Z)\\), the expected value of \\(Z\\), the product of the two rolls.\n\nSolution to Question 6"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-7",
    "href": "09-Joint-Distributions.html#question-7",
    "title": "2.6: Joint Distributions",
    "section": "Question 7",
    "text": "Question 7\n\nProve that expected value and property above.\n\nSolution to Question 7\n\nLet \\(X\\) and \\(Y\\) be two continuous random variables and let \\(a\\) and \\(b\\) denote two constants. Then we have\n\\[\\begin{aligned}\nE(aX+bY) &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (ax+by)f(x,y) \\, dx \\, dy  & \\mbox{Law of the Unconscious Statistician} \\\\\n&= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} {\\color{dodgerblue}{axf(x,y)}} \\, dx \\, dy  + \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} {\\color{dodgerblue}{byf(x,y)}} \\, dx \\, dy & \\mbox{Explain step 1} \\\\\n&= {\\color{dodgerblue}{a}} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} xf(x,y) \\, dx \\, dy  + {\\color{dodgerblue}{b}} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} yf(x,y) \\, dx \\, dy & \\mbox{Explain step 2} \\\\\n&= a \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} xf(x,y) \\, {\\color{dodgerblue}{dy \\, dx}}  + b \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} yf(x,y) \\, dx \\, dy & \\mbox{Explain step 3} \\\\\n&= a \\int_{-\\infty}^{\\infty} {\\color{dodgerblue}{x}} \\left( \\int_{-\\infty}^{\\infty} f(x,y) \\, dy \\right) \\, dx  + b \\int_{-\\infty}^{\\infty} {\\color{dodgerblue}{y}} \\left( \\int_{-\\infty}^{\\infty} f(x,y) \\, dx \\right) \\, dy & \\mbox{Explain step 4} \\\\\n&= a \\int_{-\\infty}^{\\infty} x {\\color{dodgerblue}{f_X(x)}} \\, dx  + b \\int_{-\\infty}^{\\infty} y {\\color{dodgerblue}{f_Y(y)}} \\, dy & \\mbox{Explain step 5} \\\\\n&= a {\\color{dodgerblue}{E(X)}}  + b {\\color{dodgerblue}{E(Y)}} & \\mbox{Explain step 6} \\\\\n\\end{aligned}\\]\n\nExplanation of Steps of Proof:\nStep 1:\nStep 2:\nStep 3:\nStep 4:\nStep 5:\nStep 6:"
  },
  {
    "objectID": "09-Joint-Distributions.html#question-8",
    "href": "09-Joint-Distributions.html#question-8",
    "title": "2.6: Joint Distributions",
    "section": "Question 8",
    "text": "Question 8\n\nLet \\(X\\) and \\(Y\\) be two independent random variables. Prove \\(\\mbox{Var}(aX+bY)=a^2\\mbox{Var}(X)+b^2\\mbox{Var}(Y)\\).\nAn outline of the proof is provided below. Fill in the missing details.\n\nSolution to Question 8\n\nLet \\(X\\) and \\(Y\\) be two continuous, independent random variables. Then using the property \\(\\mbox{Var}(X) = E(X^2) - \\big( E(X) \\big)^2\\), we have\n\\[\\mbox{Var}(aX+bY) = E \\left( {\\color{tomato}{??}} \\right) - \\left( {\\color{tomato}{??}} \\right)^2.\\]\nUsing the previous result, we have\n\\[\\left( E(aX+bY) \\right)^2 = {\\color{tomato}{??}}\\]\nNext we simplify \\(E \\left( (aX+bY)^2 \\right)\\) as follows\n\\[E \\left( (aX+bY)^2 \\right) = {\\color{tomato}{??}}\\]\nSince \\(X\\) and \\(Y\\) are independent, we can apply the property that \\({\\color{mediumseagreen}{E(XY) = E(X) E(Y)}}\\). This gives\n\\[\\begin{aligned}\n\\mbox{Var}(aX+bY) &= E \\left( (aX+bY)^2 \\right) - \\left( E(aX+bY) \\right)^2\\\\\n&= \\left( {\\color{dodgerblue}{a^2 E(X^2)}} + {\\color{mediumseagreen}{ab E(X)E(Y)}} + {\\color{tomato}{b^2 E(Y^2)}} \\right) - \\left( {\\color{dodgerblue}{a^2 \\big( E(X) \\big)^2}} + {\\color{mediumseagreen}{abE(X)E(Y)}} + {\\color{tomato}{b^2 \\big( E(Y) \\big)^2}} \\right) \\\\\n&= {\\color{dodgerblue}{\\left( a^2 E(X^2) - a^2 \\big( E(X) \\big)^2 \\right)}} + {\\color{tomato}{\\left( b^2 E(Y^2) - b^2 \\big( E(Y) \\big)^2 \\right)}}\\\\\n&=  a^2 {\\color{dodgerblue}{\\left(E(X^2) - \\big( E(X) \\big)^2 \\right)}} + b^2 {\\color{tomato}{\\left(  E(Y^2) - \\big( E(Y) \\big)^2 \\right)}}\\\\\n&= a^2 {\\color{dodgerblue}{\\mbox{Var}(X)}} + b^2 {\\color{tomato}{\\mbox{Var}(Y)}}\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\n\nWe DID need to use the assumption that \\(X\\) and \\(Y\\) are independent! Without the assumption of independence, the proof above would not work.\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html",
    "href": "10-Sampling-Dist-Mean.html",
    "title": "3.1: Sampling Distribution of Means",
    "section": "",
    "text": "The Population Data for BMI\nLet \\(X\\) denote the distribution of BMI of all adult men. We can approximate this distribution by \\(X \\sim N(26, 4)\\). First, we generate a vector of 100 BMI values that we store in bmi. The 100 BMI values go from a minimum value that is 4 standard deviations below the mean, \\(26-4 \\cdot 4 = 10\\), up to a maximum value that is 4 standard deviations above the mean, \\(26+4 \\cdot 4 = 42\\). Run the code cell below to create the vector of BMI values.\nbmi &lt;- seq(26 - 4*4, 26 + 4*4, length=100)\nA sample of \\(n=4\\) adult men are randomly selected. The mean BMI of the sample is calculated using the formula for a sample mean:\n\\[\\bar{x} = \\frac{x_1 + x_2 + x_3+x_4}{4}.\\]\nThen we repeat this process many, many times:\nThe sampling distribution of the means is the distribution of all sample means obtained by repeatedly picking random samples each of size \\(n\\).\nA sampling distribution for the mean BMI for \\(n=4\\) can be constructed with the code below.\n# creates an empty vector to store results\nn4.bmi.bar &lt;- numeric(1000) \n\n# a for loop that generates 1000 random samples \n# each size n=4, and calculates the sample mean.\nfor (i in 1:1000)\n{\n  n4.bmi.sample &lt;- rnorm(4, 26, 4)  # randomly picks 4 values from N(26,4)\n  n4.bmi.bar[i] &lt;- mean(n4.bmi.sample)  # calculate sample mean\n}\n\n# plot the sampling distribution as a histogram\nhist(n4.bmi.bar, xlim = c(14, 38), \n     xlab = \"Mean BMI of Sample\",\n     main = \"Sampling Distribution of Mean BMI for n=4\",\n     xaxt='n')\naxis(1, at=seq(14, 38, 4), pos=0)  # sets ticks on x-axis\nabline(v = 26, col = \"red\", lwd = 2, lty = 2)  # draws vertical line at mu\nWe can use a quantile-quantile plot (also called a qq-plot) to compare the shape of our sampling distribution to the standard normal distribution \\(N(0,1)\\). A qq-plot is a great tool for determining how “normal” a distribution is, and we will shortly explore how to read qq-plots plots more carefully. For now we only consider the the following:\nRun the code cell below to generate a qq-plot for the sampling distribution for the mean.\nqqnorm(n4.bmi.bar)\nqqline(n4.bmi.bar)\nLet \\(Y\\) denote all times (in minutes) that people wait before their train arrives at a certain train station. If we know the average wait time between arriving trains is \\(4\\) minutes, then we can use an exponential distribution to model the distribution of wait times between arriving trains. The rate parameter is \\(\\color{dodgerblue}{\\lambda = \\frac{1}{4}}\\). The distribution of wait times between successive trains is therefore \\(\\color{dodgerblue}{Y \\sim \\mbox{Exp} \\left( \\frac{1}{4} \\right)}\\).\nRun the code below to plot the probability density function for the train wait times, \\(Y \\sim \\mbox{Exp} \\left( \\frac{1}{4} \\right)\\). There is nothing to edit in the code cell below, but be sure to compare the resulting plot with your answer in Question 1.\nwait &lt;- seq(0, 10, length=100)  # wait times from 0 to 10 min plotted\npdf.wait &lt;- dexp(wait, 1/4)  # height of pdf function\n\nplot(wait,  # wait times plotted on x-axis\n     pdf.wait,  # corresponding pdf heights plotted on y-axis\n     type=\"l\",  # plot type \"l\" will connect points with a line \n     lty=1,  # lty=1 plots solid line\n     xlab=\"Wait Time (in minutes)\",  # label on x-axis\n     ylab=\"Density\",  # label on y-axis\n     main=\"Distribution of Population\")  # main label\nThe help documentation for the data set quakes in the dplyr package provides the following summary:\nThe data set contains the five variables listed below.\nRun the code cell below to load the dplyr data set so we can access the quakes data set.\nlibrary(dplyr)\nBefore collecting our thoughts, it will be helpful to introduce new notation to help us communicate and share observations. For each of the three contexts (BMI, train wait times, earthquake depths), we have been considering three different distributions:\nWe can avoid some confusion by carefully using different notation when expressing the mean and standard deviation depending on whether we are summarizing a population, a sample, or a sampling distribution. When describing the mean of a distribution we use the notation:\nWhen describing the standard deviation of a distribution we use the notation:\nLet \\(X_1, X_2, \\ldots , X_n\\) be independent, identically distributed (iid) random variables from a population with mean \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\), then as long as \\(n\\) is large enough (informally \\(\\mathbf{n \\geq 30}\\)), the sampling distribution for the mean, \\(\\overline{X}\\) will:\nThe three statements above become more accurate as \\(n\\) gets larger. We summarize the results more concisely below.\n\\[{\\large \\color{dodgerblue}{\\overline{X} \\sim N \\left( \\mu_{\\overline{X}} , \\sigma_{\\overline{X}} \\right) = N \\left( \\mu_X  , \\frac{\\sigma_X}{\\sqrt{n}} \\right)}}\\]"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-1",
    "href": "10-Sampling-Dist-Mean.html#question-1",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 1",
    "text": "Question 1\n\nConsider three distributions:\n\nLet \\(X\\) denote the distribution of Body Mass Index (BMI) of all adult men.\nLet \\(Y\\) denote all times (in minutes) that people wait before their train arrives at a certain train station.\nLet \\(Z\\) denote the depth (in km) of all earthquakes that have occurred near Fiji since 1964.\n\nFor each population \\(X\\), \\(Y\\), and \\(Z\\), do you believe the distribution will be approximately symmetric, skewed left, skewed right, or whether you are not sure what to expect. For each population, explain how you determined your answer.\n\nSolution to Question 1\n\n\nShape of population \\(X\\):\n\n\n\n\nShape of population \\(Y\\):\n\n\n\n\nShape of population \\(Z\\):"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-2",
    "href": "10-Sampling-Dist-Mean.html#question-2",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 2",
    "text": "Question 2\n\nThe code cell below generates a plot to display the distribution of BMI values. \\(X\\). Before running the code cell:\n\nInterpret each line of the code.\nAdd comments to explain what each command will do.\n\nAfter adding your comments, run the code cell and check the output looks reasonable.\n\nSolution to Question 2\n\nReplace each ?? in the code cell below with an appropriate comment to explain what the code is doing.\n\n# add your comment to explain the command below\npdf.bmi &lt;- dnorm(bmi, 26, 4)  # ??\n\nplot(bmi,  # bmi values are plotted on x-axis\n     pdf.bmi,  # corresponding heights plotted on y-axis\n     type=\"l\",  # plot type \"l\" will connect points with a line \n     lty=1,  # lty=1 plots solid line\n#################################################    \n#  add comments to explain each of the remaining options\n#################################################\n     xlab=\"Body Mass Index (BMI)\",  # ??\n     ylab=\"Density\",  # ??\n     main=\"Distribution of Population\")  # ??\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAfter running the code, be sure the output looks correct."
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#picking-one-random-sample",
    "href": "10-Sampling-Dist-Mean.html#picking-one-random-sample",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Picking One Random Sample",
    "text": "Picking One Random Sample\n\nWe can use the command rnorm(n, mean, sd) to pick a random sample of size \\(n\\) from a normal distribution \\(N(\\mu, \\sigma)\\)."
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-3",
    "href": "10-Sampling-Dist-Mean.html#question-3",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 3",
    "text": "Question 3\n\nReplace the question mark in the code cell below to randomly select 4 individual BMI’s from the population \\(X \\sim N(26,4)\\).\n\nSolution to Question 3\n\nEdit and run the code cell below.\n\n# enter a command to randomly picks 4 values from N(26,4)\nmy.sample &lt;- ??\n  \nmy.sample  # print your sample to the screen"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-4",
    "href": "10-Sampling-Dist-Mean.html#question-4",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 4",
    "text": "Question 4\n\nCalculate the mean and standard deviation of your sample stored in my.sample using the code below. After computing the statistics to summarize your sample, consider the following questions:\n\nHow do your sample statistics compare to the population parameters \\(\\mu = 26\\) and \\(\\sigma =4\\)?\nHow do expect your sample statistics will compare to the statistics that others in class obtain from their own random samples?\n\n\n# enter a command to compute the mean of my.sample\n\n# enter a command to compute the st. dev. of my.sample\n\n\nSolution to Question 4\n\nAfter checking the output of the code cell above, be sure to address the two questions in the bullets above."
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-5",
    "href": "10-Sampling-Dist-Mean.html#question-5",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 5",
    "text": "Question 5\n\nIn the code cell below, enter commands to compute the center (as measured by the mean) and spread (as measured by the standard deviation) of the sampling distribution created in the previous code cell with samples each size \\(n=4\\). After computing the statistics to summarize your sampling distribution, comment on how these values compare to the population parameters \\(\\mu=26\\) and \\(\\sigma =4\\).\n\n\n\n\n\n\nNote\n\n\n\nSample means from the previous code cell are stored in the vector n4.bmi.bar.\n\n\n\n# enter a command to compute the mean of the sampling dist\n\n# enter a command to compute the st. dev. of the sampling dist\n\n\nSolution to Question 5\n\nAfter completing and running the code cell above, comment on how the output compares to the mean and standard deviation of the population \\(X\\)."
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-6",
    "href": "10-Sampling-Dist-Mean.html#question-6",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 6",
    "text": "Question 6\n\nOpen the app https://adamspiegler.shinyapps.io/clt_bmi/ to experiment with changing the sample size \\(n\\) used when constructing a sampling distribution for the mean BMI. In particular, explore the following properties of the sampling distribution for the mean BMI:\n\nWhat is the shape of the sampling distribution?\nWhat is the mean of the sampling distribution?\nWhat is the standard deviation of the sampling distribution?\nHow does the population change when generating different sampling distributions?\n\nFill in values that describe these properties of the sampling distribution for \\(n=4\\), \\(n=9\\), \\(n=16\\), and \\(n=81\\).\n\nSolution to Question 6\n\n\n\n\n\n\nProperty\nPopulation\n\\(n=4\\)\n\\(n=9\\)\n\\(n=16\\)\n\\(n=81\\)\n\n\n\n\nShape\nNormal\n\n\n\n\n\n\nMean\n26\n\n\n\n\n\n\nStandard Deviation\n4"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-7",
    "href": "10-Sampling-Dist-Mean.html#question-7",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 7",
    "text": "Question 7\n\nOpen the app https://adamspiegler.shinyapps.io/clt_wait/ to experiment with changing the sample size \\(n\\) used when constructing a sampling distribution of the sample mean wait time \\(\\overline{Y}\\) between successive trains at a train station if we assume the time between successive trains is \\(Y \\sim \\mbox{Exp} \\left( \\frac{1}{4} \\right)\\). After experimenting with the app, complete the table below to summarize your observations when sample sizes \\(n=4\\), \\(n=9\\), \\(n=16\\), and \\(n=81\\) are used to construct a sampling distribution.\n\nSolution to Question 7\n\n\n\n\n\n\nProperty\nPopulation\n\\(n=4\\)\n\\(n=9\\)\n\\(n=16\\)\n\\(n=81\\)\n\n\n\n\nShape\nSkewed Right\n\n\n\n\n\n\nMean\n4\n\n\n\n\n\n\nStandard Deviation\n2"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#numerical-summary-of-quakes-data",
    "href": "10-Sampling-Dist-Mean.html#numerical-summary-of-quakes-data",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Numerical Summary of Quakes Data",
    "text": "Numerical Summary of Quakes Data\n\nThe first command will run and print the output of the summary() function that gives numerical summaries for all variable in the data set quakes. In the second and third lines of code, we use mean() and sd() functions to compute the mean and standard deviation, respectively, of the all depth values stored in quakes$depth. Run the code cells below and note the population mean and standard deviation of the earthquake depths.\n\n# requires dplyr package loaded above\nsummary(quakes)\n\n      lat              long           depth            mag      \n Min.   :-38.59   Min.   :165.7   Min.   : 40.0   Min.   :4.00  \n 1st Qu.:-23.47   1st Qu.:179.6   1st Qu.: 99.0   1st Qu.:4.30  \n Median :-20.30   Median :181.4   Median :247.0   Median :4.60  \n Mean   :-20.64   Mean   :179.5   Mean   :311.4   Mean   :4.62  \n 3rd Qu.:-17.64   3rd Qu.:183.2   3rd Qu.:543.0   3rd Qu.:4.90  \n Max.   :-10.72   Max.   :188.1   Max.   :680.0   Max.   :6.40  \n    stations     \n Min.   : 10.00  \n 1st Qu.: 18.00  \n Median : 27.00  \n Mean   : 33.42  \n 3rd Qu.: 42.00  \n Max.   :132.00  \n\n\n\nmean(quakes$depth)\n\n[1] 311.371\n\nsd(quakes$depth)\n\n[1] 215.5355"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#graphical-summary-of-depth-of-quakes",
    "href": "10-Sampling-Dist-Mean.html#graphical-summary-of-depth-of-quakes",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Graphical Summary of Depth of Quakes",
    "text": "Graphical Summary of Depth of Quakes\n\nBefore we construct a sampling distribution for the mean depth of the earthquake data in quakes, run the plot() function in the code cell below to create a density plot of the depths and get a sense of the population we will be sampling from.\n\nplot(density(quakes$depth),  # plot density of quake depth\n         xlab = \"Depth (in km)\",  # x-axis label\n         main = \"Depths of All Earthquakes in Fiji Since 1964\",  # main label\n         xaxt='n')  # exclude default x-axis\n\n# set custom ticks on x-axis\naxis(1, at=seq(-100, 800, 100), pos=0)  \n# draw vertical line at population mean\nabline(v = mean(quakes$depth), col = \"red\", lwd = 2, lty = 2)"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-8",
    "href": "10-Sampling-Dist-Mean.html#question-8",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 8",
    "text": "Question 8\n\nOpen the app https://adamspiegler.shinyapps.io/clt_quake/ to experiment with changing the sample size \\(n\\) used when constructing a sampling distribution of the sample mean earthquake depth \\(\\overline{Z}\\) using the observed data in quakes$depth. After experimenting with the app, complete the table below to summarize your observations when sample sizes \\(n=4\\), \\(n=9\\), \\(n=16\\), and \\(n=81\\) are used to construct a sampling distribution.\n\nSolution to Question 8\n\n\n\n\n\n\nProperty\nPopulation\n\\(n=4\\)\n\\(n=9\\)\n\\(n=16\\)\n\\(n=81\\)\n\n\n\n\nShape\nBimodal and Symmetric\n\n\n\n\n\n\nMean\n311.37\n\n\n\n\n\n\nStandard Deviation\n215.54"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-9",
    "href": "10-Sampling-Dist-Mean.html#question-9",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 9",
    "text": "Question 9\n\nFor each of the three sampling distributions we examined in Question 6, Question 7, and Question 8, summarize how the shape of the sampling distribution of the sample means changed as the size of the samples, \\(n\\), increased.\n\nSolution to Question 9"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-10",
    "href": "10-Sampling-Dist-Mean.html#question-10",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 10",
    "text": "Question 10\n\nFor each of the three sampling distributions we examined in Question 6, Question 7, and Question 8, summarize how the mean (center) of the sampling distribution changed as the size of the samples, \\(n\\), increased.\n\nSolution to Question 10"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-11",
    "href": "10-Sampling-Dist-Mean.html#question-11",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 11",
    "text": "Question 11\n\nFor each of the three sampling distributions we examined in Question 6, Question 7, and Question 8, summarize how the standard error of the sampling distribution changed as the size of the samples, \\(n\\), increased.\n\nSolution to Question 11"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-12",
    "href": "10-Sampling-Dist-Mean.html#question-12",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 12",
    "text": "Question 12\n\nUsing properties of the expected value of linear combinations of random variables prove the following:\n\\[\\mu_{\\overline{X}} = E \\left( \\overline{X} \\right) = E \\left( \\frac{X_1 + X_2 + \\ldots + X_n}{n} \\right) = \\mu_X.\\]\n\nSolution to Question 12"
  },
  {
    "objectID": "10-Sampling-Dist-Mean.html#question-13",
    "href": "10-Sampling-Dist-Mean.html#question-13",
    "title": "3.1: Sampling Distribution of Means",
    "section": "Question 13",
    "text": "Question 13\n\nUsing properties of the variance of linear combinations of independent random variables prove the following:\n\\[ \\sigma^2_{\\overline{X}} = \\mbox{Var} \\left( \\overline{X} \\right) = \\mbox{Var} \\left( \\frac{X_1 + X_2 + \\ldots + X_n}{n} \\right) = \\frac{\\sigma^2_X}{n}.\\]\n\nSolution to Question 13\n\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html",
    "href": "11-Sampling-Dist-Prop.html",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "",
    "text": "Sampling from a Binomial Distribution\nFair representation on a jury is one example of a situation where analyzing a distribution of sample proportions may be appropriate. Very often we encounter statistical questions that ask us to approximate or compare proportions. For example\nWe can use a sampling distribution of proportions to help analyze statistical questions regarding proportions. In these situations:\n\\[\\color{dodgerblue}{\\boxed{\n\\hat{p} = \\frac{\\mbox{Number of successes}}{\\mbox{Size of sample}} = \\frac{X}{n}\n}}.\\]\nA sampling distribution of proportions is a distribution of the sample proportions, \\(\\hat{P}\\), computed from many (or all) random samples each size \\(n\\) picked independently from the same population (with probability \\(p\\) of picking a “success”).\nBefore investigating the theory, we will first walk through a statistical simulation for constructing a sampling distribution for proportions from many randomly selected samples. Hopefully we can see similarities in this process as with our initial construction of sampling distributions for means, and these connections will be useful in constructing sampling distributions with statistics. We will compare the results of the simulation with the theory we explore after.\nWhen constructing a sampling distribution for proportions, we noted that the probability of getting a certain number of “successes”, \\(X\\), in a sample size \\(n\\) can be modeled using a binomial distribution. We can make use of properties of binomial distributions to obtain a theoretical model for the sampling distribution of sample proportions. The theory gives another method for describing sampling distributions using formulas as opposed to statistical simulations that require technology.\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) be a binomial random variable, and let \\(\\widehat{P} = \\frac{X}{n}\\) denote the distribution of sample proportions. Then if the sample size \\(n\\) satisfies both \\(\\mathbf{np \\geq 10}\\) and \\(\\mathbf{n(1-p) \\geq 10}\\) , the sampling distribution for \\(\\widehat{P}\\) will:\nWe summarize the results of the Central Limit Theorem (CLT) for Proportions more concisely below:\n\\[\\color{dodgerblue}{\\boxed{ \\widehat{P} \\sim N \\left( \\mu_{\\widehat{P}} , \\sigma_{\\widehat{P}} \\right) = N \\left( p  , \\sqrt{\\frac{p(1-p)}{n}} \\right) \\qquad \\mbox{ as long as both } np \\geq 10 \\mbox{ and } n(1-p) \\geq 10.}}\\]\nIn the jury pool example, we have \\(n=20\\), \\(p=0.42\\), and thus \\(np = 8.4 &lt; 10\\). Our sample is not seemingly large enough for the CLT according to the conditions stated for proportions above. However, the \\(np \\geq 10\\) and \\(n(1-p) \\leq 10\\) conditions are more like general guidelines. We can check how consistent the results of the sampling distribution simulation (results of for loop stored in samp.prop) are with the CLT:\nmean(samp.prop)\n\n[1] 0.42013\nsd(samp.prop)\n\n[1] 0.1101817\nqqnorm(samp.prop)\nqqline(samp.prop)\nIn the statement of the Central Limit Theorem for proportions, we state the theorem applies as long as both \\(np \\geq 10\\) and \\(n(1-p) \\geq 10\\).\nIf our sample is not large enough (either \\(np &lt; 10\\) or \\(n(1-p) &lt; 10\\)) then sampling distribution of the sample proportions:\nWhen constructing a sampling distribution of sample proportions, we approximate the distribution of sample proportions with a continuous distribution, namely a normal distribution. However, the possible sample proportions we can get from a sample size \\(n\\) are not continuous. The possible sample proportions are a discrete set.\nIn the jury pool example, the discrete binomial distribution \\(X \\sim \\mbox{Binom}(20, 0.42)\\) is used to identify the probability of getting exactly \\(X\\) Independents in a jury of \\(n=20\\). From the CLT for proportions, we have \\(\\color{dodgerblue}{\\widehat{P} \\sim N \\left(p , \\sqrt{\\frac{p(1-p)}{n}} \\right) = N(0.42, 0.1104)}\\), which is also equivalent to the approximation \\(\\color{dodgerblue}{X \\sim N \\left( np , \\sqrt{np(1-p)} \\right) = N(8.4, 2.2073)}\\). The figure below illustrates and compares how we can use different distributions to estimate the same probabilities.\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nIn the previous plot, we note using a normal distribution to approximate a binomial distribution results in an underestimate since we miss capturing some of the rectangular area below the binomial distribution. We can improve the estimates we obtain using a normal distribution by including some additional area using a continuity correction as follows.\nTo more accurately calculate \\(P( a \\leq X \\leq b)\\) where \\(a &lt; b\\) are integers, then we apply the following correction:\nIf we use the CLT for proportions to approximate the distribution of sample proportions as \\(\\widehat{P} \\sim N \\left( p, \\sqrt{\\frac{p(1-p)}{n}} \\right)\\), then using proportions, the two corrected cutoffs will be:\nApplying a continuity correction to sample proportions, we have an improved estimate:\n\\[\\color{dodgerblue}{P( a \\leq X \\leq b) \\approx  P \\left( \\frac{a-0.5}{n} &lt; \\widehat{P} &lt; \\frac{b+0.5}{n} \\right)}.\\]"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-1",
    "href": "11-Sampling-Dist-Prop.html#question-1",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 1",
    "text": "Question 1\n\nWe randomly pick a pool of \\(20\\) people to fill a jury pool from a population where 42% of eligible jurors identify their political party as Independents. A jury pool is picked that has 8 out of 20 people in the jury pool say they are politically Independent\n\nQuestion 1a\n\nWhat is the value of \\(p\\)?\n\nSolution to Question 1a\n\n\n\n\n\n\n\n\nQuestion 1b\n\nWhat is the value of \\(\\hat{p}\\)?\n\nSolution to Question 1b\n\n\n\n\n\n\n\n\nQuestion 1c\n\nWill the value of \\(\\hat{p}\\) change when different jury pools are selected? If not, explain why not. If so, what would you expect the value of \\(\\hat{p}\\) to be?\n\nSolution to Question 1c\n\n\n\n\n\n\n\n\nQuestion 1d\n\nWill the value of \\(p\\) change when different jury pools are selected? If not, explain why not. If so, what would you expect the value of \\(p\\) to be?\n\nSolution to Question 1d\n\n\n\n\n\n\n\n\nQuestion 1e\n\nLet random variable \\(X\\) denote the number of people in the jury pool that identify politically as Independent. What random variable can we use to model the probabilities of picking different values of \\(X\\)?\n\nSolution to Question 1e"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#simulating-jury-selection-with-sample",
    "href": "11-Sampling-Dist-Prop.html#simulating-jury-selection-with-sample",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Simulating Jury Selection with sample()",
    "text": "Simulating Jury Selection with sample()\n\nWe first pick one random sample size \\(n=20\\) from the population \\(X \\sim \\mbox{Binom}(20, 0.42)\\). The code cell below uses the sample() function to simulate randomly selecting a pool of 20 jurors from a population where 42% of voters identify as Independents and the remaining 58% of voters do not identify as Independent.\n\nNote 5 out of the 20 people stored in jury.pool identify their political party as Independent.\n\n\n\n\n\n\n\nTip\n\n\n\nTip: The command set.seed(937) fixes the randomization used each time the sample() command is run so the same output is generated every time.\n\nFeel free to delete the set.seed() command to appreciate the variability in random sampling.\n\n\n\n\n# set seed for randomization so results do not change\nset.seed(937)  \n\njury.pool &lt;- sample(c(\"Indep\", \"Not\"),  # each time we select either Indep or Not\n                    size = 20,  #  pick 20 people for the pool\n                    replace = TRUE,  # replace the value each time before resampling\n                    prob = c(0.42,0.58))  # probabilities of success (\"Indep\") and failure (\"Not\")\n\njury.pool  # print results to screen\n\n [1] \"Not\"   \"Indep\" \"Indep\" \"Not\"   \"Not\"   \"Indep\" \"Not\"   \"Not\"   \"Not\"  \n[10] \"Not\"   \"Not\"   \"Not\"   \"Indep\" \"Not\"   \"Not\"   \"Not\"   \"Not\"   \"Not\"  \n[19] \"Indep\" \"Not\""
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#compute-sample-proportion-hatp-dfracxn",
    "href": "11-Sampling-Dist-Prop.html#compute-sample-proportion-hatp-dfracxn",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Compute Sample Proportion, \\(\\hat{P} = \\dfrac{X}{n}\\)",
    "text": "Compute Sample Proportion, \\(\\hat{P} = \\dfrac{X}{n}\\)\n\nThe command sum(jury.pool == \"Indep\") counts how many values in the vector jury.pool are equal to the string “Indep”.\n\nBased on the output stored in jury.pool from the previous code cell, the output \\(5\\) is stored in x.\nThus, the sample proportion stored in p.hat is \\(\\hat{p} = \\frac{5}{20} = 0.25\\).\n\n\n# out of the 20 values in jury.pool\n# count how many equal the string \"Indep\"\nx &lt;- sum(jury.pool == \"Indep\") \n\nn &lt;- 20  # jury pool is size n=20\np.hat &lt;- x / n  # compute p-hat  \np.hat\n\n[1] 0.25\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe command mean(jury.pool == \"Indep\") is a more concise way to calculate a sample proportion. The mean() function will both sum the number of “Indep” in jury.pool and divide by the length of the vector jury.pool, resulting in a sample proportion.\n\n\n\n# sample proportion with `mean()`\nmean(jury.pool == \"Indep\") \n\n[1] 0.25"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#constructing-a-distribution-of-sample-proportions",
    "href": "11-Sampling-Dist-Prop.html#constructing-a-distribution-of-sample-proportions",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Constructing a Distribution of Sample Proportions",
    "text": "Constructing a Distribution of Sample Proportions\n\nTo construct a sampling distribution of proportions, we repeat the two steps above over and over again.\n\nPick a random sample of \\(n=20\\) using the sample() function.\nCalculate \\(\\hat{p}\\) using a logical test and either a sum() or mean() function.\n\nThe for loop in the code cell below repeats the two previous steps 10,000 times and stores each sample proportion in a vector named samp.prop. Run the code cell below to generate a possible sampling distribution for proportions.\n\nThere is nothing to edit in the code cell.\nDo not worry, no output is printed to the screen since the output is being stored in samp.prop.\n\n\nsamp.prop &lt;- numeric(10000) # creates an empty vector to store results\nn &lt;- 20  # each random jury pool is size n=20\n\n# a for loop that generates 10,000 random samples \nfor (i in 1:10000)\n{\n  temp.pool &lt;- sample(c(\"Indep\", \"Not\"),  # pick jury pool\n                      size = 20,  \n                      replace = TRUE, \n                      prob = c(0.42,0.58))  \n  samp.prop[i] &lt;-sum(temp.pool == \"Indep\")/n  # calculate sample proportion\n}\n\nThe previous code cell generated many (10,000) random samples. We did not ensure that all possible samples are selected exactly once. Generating all possible random samples requires more intricate code that is time consuming to run.\n\nFor example, if the population of eligible voters is only 100 people, then there would be \\(\\begin{pmatrix} 100 \\\\ 10 \\end{pmatrix} = 5.4 \\times 10^{20}\\) distinct jury pools that could be selected!\n\n\n\n\n\n\n\nTip\n\n\n\nBy relaxing the condition to generate every possible sample exactly once, we can get a very good approximation of the sampling distribution with much simpler code that is easier to read, modify, and run!\n\n\nThe code cell below plots a histogram of the sampling distribution of the sample proportion of Independents on a jury pool size \\(n=20\\). - Comments are used to help explain hist options - There is nothing to edit in the code cell.\n\nhist(samp.prop,  # plot stored sample proportions\n     breaks = 20,  # use 20 breaks\n     xlab = \"Sample Proportion, P-Hat\",  # x-axis label\n     main = \"Sampling Distribution of Proportion of Independents, n=20\",  # main label\n     freq = FALSE,  # plot relative frequencies (density) on y-axis\n     xaxt='n')  # disable default x-axis\n\naxis(1, at=seq(0, 1, 0.05), pos=0)  # customize ticks on x-axis\n\nabline(v = 0.42, col = \"blue\", lwd = 2, lty = 1)  # draws vertical line at p\nabline(v = 5/20, col = \"red\", lwd = 2, lty = 2)  # draws vertical line at p-hat"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-2",
    "href": "11-Sampling-Dist-Prop.html#question-2",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 2",
    "text": "Question 2\n\nBased on the sampling distribution for the sample proportion of Independents plotted above, answer the following:\n\nWhat is significant about the location of blue vertical line plotted at \\(p=0.42\\) relative to the other sample proportions?\nWhat is significant about the location of red vertical line plotted at \\(\\hat{p}=\\frac{5}{20} = 0.25\\) relative to the other sample proportions?\nWhat is a “fair” amount of representation for Independents on a jury pool of size \\(n=20\\)?\nBased on the values stored in samp.prop, what is the probability of randomly selecting a jury pool of \\(n=20\\) that has a sample proportion of Independents that is at most \\(\\hat{p} = 0.25\\)?\n\nHint: Use a logical test involving samp.prop and a sum() or mean() command.\n\n\n\nSolution to Question 2\n\n\n\n\n\n\n# use samp.prop to approximate P( p-hat &lt;= 0.25)"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-3",
    "href": "11-Sampling-Dist-Prop.html#question-3",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 3",
    "text": "Question 3\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\), and consider the distribution of sample proportions \\(\\hat{P} = \\frac{X}{n}\\).\n\nQuestion 3a\n\nUsing properties of the binomial distribution, what are \\(E(X)\\) and \\(\\mbox{Var}(X)\\)? Your answers will depend on the parameters \\(n\\) and \\(p\\).\n\nSolution to Question 3a\n\n\n\n\n\n\n\n\nQuestion 3b\n\nLet \\(\\widehat{P} = \\frac{X}{n}\\) denote the distribution of sample proportions. Using formulas from part Question 3a and properties of expected value and variance, give formulas for \\(E( \\widehat{P} )\\) and \\(\\mbox{Var}( \\widehat{P} )\\). Your formulas should depend on parameters \\(n\\) and \\(p\\).\n\nSolution to Question 3b"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-4",
    "href": "11-Sampling-Dist-Prop.html#question-4",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 4",
    "text": "Question 4\n\nUse the CLT for proportions to describe the sampling distribution for the proportion of Independents on a jury pool of \\(n=20\\) people. As with earlier, assume 42% of the population of eligible jurors identify their political party as Independent.\n\nSolution to Question 4"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-5",
    "href": "11-Sampling-Dist-Prop.html#question-5",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 5",
    "text": "Question 5\n\nBased on your answer in Question 4, what is the probability of randomly picking a pool of 20 jurors that consists of at most 25% that identify politically as Independent.\n\nHint: Use the pnorm() function to help!\n\n\nSolution to Question 5\n\n\n# use CLT and pnorm()"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-6",
    "href": "11-Sampling-Dist-Prop.html#question-6",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 6",
    "text": "Question 6\n\nIn Question 2 and Question 5 we use two different methods to calculate \\(P( \\hat{P} \\leq 0.25)\\), the probability of picking a jury pool of 20 people that has at most 25% that identify politically as Independent:\n\nQuestion 2 uses a simulation to approximate the sampling distribution for the proportion of Independent.\nQuestion 5 uses the CLT to theoretically model the sampling distribution.\n\nCompare each of the values you obtained for \\(P( \\hat{P} \\leq 0.25)\\) in Question 2 and Question 5. Which do you believe is more accurate and why?\n\nSolution to Question 6"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-7",
    "href": "11-Sampling-Dist-Prop.html#question-7",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 7",
    "text": "Question 7\n\nConsider the same jury pool example where we randomly select a jury from a population where 42% of eligible jurors identify their political party as Independent. Let \\(X\\) denote the number of Independents on jury pool size \\(n=20\\). Use a binomial distribution to calculate the \\(P(X \\leq 5)\\), the probability that at most 5 out of 20 identify politically as Independents.\n\nSolution to Question 7\n\n\n\n\n# use a binomial function to answer question 7"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#plot-of-exact-area-under-binomial-distribution",
    "href": "11-Sampling-Dist-Prop.html#plot-of-exact-area-under-binomial-distribution",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Plot of Exact Area Under Binomial Distribution",
    "text": "Plot of Exact Area Under Binomial Distribution\n\nWe can use the binomial distribution \\(X \\sim \\mbox{Binom}(n, p)\\) to compute the exact value of \\(P(a \\leq X \\leq b)\\). We add up the areas of each of the shaded rectangles."
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#plot-of-normal-approximation-without-a-continuity-correction",
    "href": "11-Sampling-Dist-Prop.html#plot-of-normal-approximation-without-a-continuity-correction",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Plot of Normal Approximation Without a Continuity Correction",
    "text": "Plot of Normal Approximation Without a Continuity Correction\n\nIf we use the CLT for proportions we get \\(\\widehat{P} \\sim N \\left( p, \\sqrt{\\frac{p(1-p)}{n}} \\right)\\). Using the CLT for proportions without applying a continuity correction, we get an underestimate \\(P \\left( \\frac{a}{n} \\leq \\widehat{P} \\leq \\frac{b}{n} \\right) \\approx\\) that is shaded in red below. Notice there is gray shaded area under the binomial distribution that is missing from the shaded region under the normal distribution."
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#plot-of-normal-approximation-with-a-continuity-correction",
    "href": "11-Sampling-Dist-Prop.html#plot-of-normal-approximation-with-a-continuity-correction",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Plot of Normal Approximation With a Continuity Correction",
    "text": "Plot of Normal Approximation With a Continuity Correction\n\nUsing the CLT for proportions along with a continuity correction, we get an improved estimate \\(P \\left( \\frac{a-0.5}{n} \\leq \\widehat{P} \\leq \\frac{b+0.5}{n} \\right)\\) which is the red region plus the additional area in the two blue strips. The sum of the red and blue areas under the normal distribution is a much better approximation for the exact area under the binomial distribution.\n\nThere is a little extra area beneath the normal curve that is not included in any of the rectangular bars under the binomial distribution.\nThe extra area under the normal distribution approximately balances out the remaining gray rectangular areas missed above the normal distribution."
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-8",
    "href": "11-Sampling-Dist-Prop.html#question-8",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 8",
    "text": "Question 8\n\nUse the CLT for proportions along with a continuity correction to approximate the probability of randomly selecting a jury pool of \\(n=20\\) people that has at most 5 people that identify politically as Independent. Assume that 42% of all eligible jurors identify their political party as Independent.\n\nNote: Although our sample size does not technically satisfy the conditions for the CLT for proportions, we verified the results hold in this example, so we may go ahead and use the CLT.\nHint: There is no correction needed for a the lower cutoff since there is no lower cutoff. You only need to apply the correction to get a new upper cutoff.\n\n\nSolution to Question 8"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-9",
    "href": "11-Sampling-Dist-Prop.html#question-9",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 9",
    "text": "Question 9\n\nCensus Bureau data for 2017 shows nearly half (48 percent) of residents in United States five largest cities now speak a language other than English at home1. If a sample of 150 people are selected at random from the five largest cities in the US, what is the probability that between 44% and 48% of people in the sample speak a language other than English at home?\n\nQuestion 9a\n\nIs \\(n\\) large enough to use the CLT? Explain why or why not?\n\nSolution to Question 9a\n\n\n\n\n\n\n\nQuestion 9b\n\nUsing the CLT for a proportion, find the \\(z\\)-scores of the proportions \\(0.44\\) and \\(0.48\\).\n\nSolution to Question 9b\n\n\n\n\n\n\n\nQuestion 9c\n\nUsing the \\(z\\)-scores from Question 9b, what is the probability that between 44% and 48% (out of the random sample of 150 people) speak a language other than English at home?\n\nSolution to Question 9c\n\n\n\n\n\n\n\nQuestion 9d\n\nUsing the CLT for a proportion along with a continuity correction, give updated \\(z\\)-scores after applying a continuity correction to both endpoints of the interval.\n\nSolution to Question 9d\n\n\n\n\n\n\n\n\nQuestion 9e\n\nUsing the \\(z\\)-scores obtained from the continuity correction in Question 9d, what is the probability that between 44% and 48% (out of the random sample of 150 people) speak a language other than English at home?\n\nSolution to Question 9e"
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#question-10",
    "href": "11-Sampling-Dist-Prop.html#question-10",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "Question 10",
    "text": "Question 10\n\nIn Question 9 we calculated \\(P\\left( 0.44 \\leq \\widehat{P} \\leq 0.48 \\right)\\) using a normal distribution (using the CLT for proportions). We could equivalently rewrite this probability in terms of the discrete random variable \\(X \\sim \\mbox{Binom}(150,0.48)\\) as \\(P( 66 \\leq X \\leq 72\\)).\n\nQuestion 10a\n\nUsing a binomial distribution, calculate the exact value of \\(P( 66 \\leq X \\leq 72\\)).\n\nSolution to Question 10a\n\n\n# use binomial functions in R to solve 10a\n\n\n\n\n\n\n\n\nQuestion 10b\n\nCompare approximations from Question 9c and Question 9e with the exact calculation in Question 10a. Comment on whether or not the continuity correction improved the approximation or not.\n\nSolution to Question 10b\n\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "11-Sampling-Dist-Prop.html#footnotes",
    "href": "11-Sampling-Dist-Prop.html#footnotes",
    "title": "3.2: Sampling Distributions for Proportions",
    "section": "",
    "text": "Karen Zeigler and Steven A. Camarota Almost Half Speak a Foreign Language in America’s Largest Cities. Center for Immigration Studies, 2018 Sep 29.↩︎"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html",
    "href": "12-Sampling-Dist-Other.html",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "",
    "text": "Simulation for Sampling Distributions\nWe can apply the same method as we used with constructing sampling distributions for means and proportions to create a sampling distribution for any statistic. The general process is as follows:\nWe previsouly considered the sampling distribution for the mean wait time between trains. In that example, the wait time between trains was exponentially distributed with a mean of \\(4\\) minutes between trains. Thus, the rate parameter is \\(\\lambda = \\frac{1}{4}\\). We sample from the population of \\(Y \\sim \\mbox{Exp} \\left( \\frac{1}{4} \\right)\\) which is skewed to the right.\nRun the code cell below to create a sampling distribution for the mean time between trains with samples size \\(n=50\\).\n# creates an empty vector to store results\nsamp.mean &lt;- numeric(10000)\n\n# a for loop that generates 10000 random samples \n# each size n=50, and calculates the sample mean.\nfor (i in 1:10000)\n{\n  time.sample &lt;- rexp(50, 1/4)  # Step 1: Randomly picks 50 values from Exp(1/4)\n  samp.mean[i] &lt;- mean(time.sample)  # Step 2: calculate sample mean\n}\nIn the previous examples involving sample median and variance, we are able to analyze properties of the sampling distribution of those statistics by simulating the creation of a sampling distribution. We can always use simulations to approximate sampling distributions, even if we do not have a formal theorem or formulas that describe the distribution for those sample statistics. With some statistics, such as sample means and sample proportions, we can use properties of random variables to derive theory and formulas that describe properties of the sampling distribution. In situations where we can theoretically model sampling distributions, technology is not required to describe the sampling distribution. We explore some additional statistics, namely the maximum of a sample, and derive formulas we can use to describe a sampling distribution of maximum values.\nThe stop-and-frisk policy is the practice of temporarily stopping, questioning, and possibly searching civilians suspected of carrying illegal possessions such as drugs and weapons. The program has come under scrutiny in many cities regarding claims of racial-profiling. We investigate the following question:\nIn 2022, 15,102 stops were recorded by NYPD, and these stops are broken down by race in the table below.\nobserve &lt;- c(8863, 4477, 1077, 320, 365)  # vector of observed values\nprop &lt;- c(0.23, 0.29, 0.32, 0.14, 0.02)  # vector of population proportions\nIn the jury example from our exploration with sampling distributions for a single proportion, we considered whether people that identify their political party as Independent have fair representation. Political party was a binary variable (Independent or Not). Perhaps a more useful question to explore is whether the jury has a good representation of all political affiliations, not just Independents. When working with a categorical variable that has more than two categories, such as race or political affiliation, we need to consider more than a single proportion. Rather than consider a single proportion, we can conduct a chi-square goodness-of-fit test to measure how well the random sample resembles the population as far as what proportion of observations are in each category. We assume the categories are mutually exclusive (an observation cannot belong to multiple categories), and thus the sum of all proportions is 1."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#graphical-summary-of-results-of-simulation",
    "href": "12-Sampling-Dist-Other.html#graphical-summary-of-results-of-simulation",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Graphical Summary of Results of Simulation",
    "text": "Graphical Summary of Results of Simulation\n\nRun the code cell below to generate side-by-side plots of the pdf of the population \\(Y \\sim \\mbox{Exp}(1/4)\\) and a sampling distribution for the mean with \\(n=50\\).\n\npar(mfrow=c(1,2))  # arrange plots side by side\n\n# plot population\ny &lt;- seq(0, 6, 0.01) \nplot(y, dexp(y), type = \"l\",\n     xlab = \"Y, wait time (in min)\",\n     main = \"Population, Y ~ Exp(1/4)\")\n\n# plot the sampling distribution of means\nhist(samp.mean, \n     breaks = 20,\n     xlab = \"Mean Wait Time (n=50)\",\n     main = \"Dist. of Sample Means\")\nabline(v = mean(samp.mean), col = \"blue\", lwd = 2)  # center of sampling dist of mean"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#simulation-for-sampling-distribution-for-median",
    "href": "12-Sampling-Dist-Other.html#simulation-for-sampling-distribution-for-median",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Simulation for Sampling Distribution for Median",
    "text": "Simulation for Sampling Distribution for Median\n\nSince wait times between trains are skewed to the right, the sample means will be more heavily influenced by outliers that are biased to the right. The median is the 50th percentile of the data, and it is not as sensitive to outliers. We might want to consider a sampling distribution for the median if we want to consider a measure of center that is less influenced by outliers."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-1",
    "href": "12-Sampling-Dist-Other.html#question-1",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 1",
    "text": "Question 1\n\nComplete the partially completed code cell below to generate a sampling distribution for the median weight time between trains using samples size \\(n=50\\). You should continue to assume the time between successive trains is exponentially distributed with a mean time of 4 minutes between successive trains.\n\nSolution to Question 1\n\nReplace each of the three ?? in the code cell below with appropriate code. Then run the completed code to create and plot a sampling distribution for the sample median for samples size \\(n=50\\).\n\n# creates an empty vector to store results\nsamp.median &lt;- numeric(10000)\n\n# a for loop that generates 10000 random samples \n# each size n=50, and calculates the sample median\nfor (i in 1:10000)\n{\n##################################\n# Replace both ?? in the for loop\n##################################\n  time.sample &lt;- ??  # Step 1: Pick random sample from population\n  samp.median[i] &lt;- ??  # Step 2: Calculate sample statistic\n}\n\n\n# plot the sampling distribution\n#####################################\n# Replace the ?? in the hist command\n#####################################\nhist(??, \n     breaks = 20,\n     xlab = \"Median Wait Time (n=50)\",\n     main = \"Dist. of Sample Means\")\nabline(v = mean(samp.median), col = \"red\", lwd = 2, lty = 2)  # center of sampling dist of median\nabline(v = mean(samp.mean), col = \"blue\", lwd = 2, lty = 2)  # center of sampling dist of mean"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-2",
    "href": "12-Sampling-Dist-Other.html#question-2",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 2",
    "text": "Question 2\n\nBased on the plot of the sampling distribution for the median plotted above, comment on the significance of the red and blue vertical lines included in the plot.\n\nWhat quantity is the red line representing? What quantity is the blue line representing?\nCompare the locations of the two lines. Why is the blue line to the right of the red line? Explain why this makes sense in this context.\n\n\nSolution to Question 2"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-3",
    "href": "12-Sampling-Dist-Other.html#question-3",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 3",
    "text": "Question 3\n\nBased on the results of the sampling distribution for the median stored in samp.median in Question 2 answer the following questions.\n\nQuestion 3a\n\nCreate a qq-plot of the sampling distribution and comment on whether shape of the sampling distribution for median is normal or not.\n\nSolution to Question 3a\n\n\n# create a qq-plot of the sampling dist for median\n# interpret the output after writing/running code\n\n\n\n\n\n\n\n\nQuestion 3b\n\nCompute the standard error of the sampling distribution for medians and the standard error for the sampling distribution for means. Compare both values. Which is bigger and why does that make practical sense?\n\n\n\n\n\n\nTip\n\n\n\nResults for the sampling distribution of the mean wait time between trains were stored in samp.mean in an earlier code cell. Be sure you have run that code and stored those results.\n\n\n\nSolution to Question 3b\n\n\n# calculate both standard errors\n# interpret the output after writing/running code"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#simulation-for-sampling-distribution-for-variance",
    "href": "12-Sampling-Dist-Other.html#simulation-for-sampling-distribution-for-variance",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Simulation for Sampling Distribution for Variance",
    "text": "Simulation for Sampling Distribution for Variance\n\nVariance is one of the most frequently used measurements for the spread of a distribution. If we pick a random sample of values \\(X_1, X_2, \\ldots , X_n\\) from a population, it is natural to consider how much variation can we expect in the spread of values (measured by variance) for different random samples of size \\(n\\). The variance is not a linear combination of random variables \\(X_1, X_2, \\ldots , X_n\\), so we cannot use the same properties of expected value and variance that we applied to derive the Central Limit Theorem for means and proportions. Instead, we will use a simulation based method to investigate properties of a sampling distribution for sample variances."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-4",
    "href": "12-Sampling-Dist-Other.html#question-4",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 4",
    "text": "Question 4\n\nCholesterol is a fat-like substance present in all cells in your body. A blood test is used to measure cholesterol levels. The distribution of cholesterol levels for adults age 20 or older is approximately normally distributed with a mean of 200 mg/dL and standard deviation 40 mg/dL. Let \\(X\\) be the cholesterol level of a randomly selected adult age 20 or above. Thus, we have \\(X \\sim N(200, 40)\\).\n\nQuestion 4a\n\nComplete the partially completed code cell below to generate a sampling distribution for the sample variance of cholesterol level using samples size \\(n=25\\).\n\nSolution to Question 4a\n\nReplace each of the three ?? in the code cell below with appropriate code. Then run the completed code to create and plot a sampling distribution for the sample variance for samples size \\(n=25\\).\n\n# creates an empty vector to store results\nsamp.var &lt;- numeric(10000)\n\n# a for loop that generates 10000 random samples \n# and calculates the sample variance\nfor (i in 1:10000)\n{\n##################################\n# Replace both ?? in the for loop\n##################################\n  cholest.sample &lt;- ??  # Step 1: Pick random sample from population\n  samp.var[i] &lt;- ??  # Step 2: Calculate sample statistic\n}\n\n\n# plot the sampling distribution\n#####################################\n# Replace the ?? in the hist command\n#####################################\nhist(??, \n     breaks = 20,\n     xlab = \"Variance of Cholesterol (n=25)\",\n     main = \"Dist. of Sample Variance\")\nabline(v = mean(samp.var), col = \"red\", lwd = 2, lty = 2)  # center of sampling dist of variance\n\n\n\n\nQuestion 4b\n\nBased on the results of the sampling distribution for the sample variance stored in samp.var after running the code above:\n\nCalculate the mean and standard error of the sampling distribution for the sample variance when \\(n=25\\).\nCreate a qq-plot and comment on whether or not the sampling distribution for the sample variance appears to be normal or not.\n\n\nSolution to Question 4b\n\n\n# calculate mean and standard error of \n# sampling distribution of sample variance\n\n\n# create a qq-plot of the sampling dist for sample variance\n# interpret the output after writing/running code"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#sampling-distribution-for-the-maximum",
    "href": "12-Sampling-Dist-Other.html#sampling-distribution-for-the-maximum",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Sampling Distribution for the Maximum",
    "text": "Sampling Distribution for the Maximum\n\nFrequently we are interested in the sample maximum value, denoted \\(\\color{dodgerblue}{X_{\\rm{max}}}\\), of a random sample. For example, if we pick a random sample of five values, \\(\\left\\{ 11, 18, 2, 31, 25 \\right\\}\\), then we have the statistics \\(X_{\\rm{min}} = 2\\) and \\(X_{\\rm{max}} = 31\\). A sampling distribution for the sample maximum would be the distribution of maximum values obtained from samples when we independently pick many random samples, each size \\(n\\), from the same population."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-5",
    "href": "12-Sampling-Dist-Other.html#question-5",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(F\\) denote the cumulative distribution function (cdf) for a continuous random variable \\(X\\). Derive the cdf for \\(X_{\\rm{max}}\\), the maximum of a random sample \\(X_1,X_2, \\ldots, X_n\\), with each \\(X_i\\) independently picked from the same population \\(X\\) that has cdf \\(F(x)\\).\nAn outline of a proof is provided below. In the solution space below, fill in missing explanations for each step of the proof.\n\\[\\begin{aligned}\nF_{X_{\\rm{max}}} (a) &= P(X_1 \\leq a, X_2 \\leq a, \\ldots, X_n \\leq a) & \\mbox{Explanation 1} \\\\\n&= P\\big( (X_1 \\leq a) \\cap (X_2 \\leq a) \\cap \\ldots \\cap (X_n \\leq a) \\big) & \\mbox{Explanation 2}\\\\\n&= P(X_1 \\leq a) \\cdot P(X_2 \\leq a) \\cdot  \\ldots  \\cdot P(X_n \\leq a) & \\mbox{Explanation 3}\\\\\n&= F(a) \\cdot F(a) \\cdot \\ldots \\cdot F(a) & \\mbox{Explanation 4}\\\\\n&= \\big( F(a) \\big)^n\n\\end{aligned}\\]\n\nSolution to Question 5\n\nExplanation 1:\n\nExplanation 2:\n\nExplanation 3:\n\nExplanation 4:"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-6",
    "href": "12-Sampling-Dist-Other.html#question-6",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 6",
    "text": "Question 6\n\nUsing the result from the previous problem, find \\(\\displaystyle f_{X_{\\rm{max}}} (a)\\), the probability density function (pdf) for the maximum of a random sample. Your answer will depend on both \\(F\\) and \\(f\\), the corresponding cdf and pdf, respectively, of \\(X\\).\nHint: Recall the general relation between cdf’s and pdf’s of continuous random variables.\n\nSolution to Question 6\n\n\n\n\n\n\n\nSummarizing Results for \\(X_{\\rm{max}}\\)\n\nLet \\(F\\) denote the cumulative distribution function (cdf) for a continuous random variable \\(X\\), and let \\(X_{\\rm{max}}\\) denote the maximum of a random sample \\(X_1,X_2, \\ldots, X_n\\), where each \\(X_i\\) is independently picked from population \\(X\\). Then the sample maximum, \\(X_{\\rm{max}}\\), will have:\n\nCumulative distribution function \\(\\displaystyle F_{X_{\\rm{max}}} (a) = \\big( F(a) \\big)^n\\).\nProbability density function \\(\\displaystyle f_{X_{\\rm{max}}} (a) = n \\left( F(a) \\right)^{n-1} f(a)\\)."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-7",
    "href": "12-Sampling-Dist-Other.html#question-7",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 7",
    "text": "Question 7\n\nLet \\(X\\) be a random variable with continuous uniform distribution \\(\\mbox{Unif}(0,1)\\). Answer the questions below regarding the distribution \\(X\\) and the distribution of sample maximum, \\(X_{\\rm{max}}\\), for \\(n=10\\).\n\nQuestion 7a\n\nWhat are the cdf and pdf, \\(F(x)\\) and \\(f(x)\\) respectively, of \\(X \\sim \\mbox{Unif}(0,1)\\)? Hint: See useful properties of continuous uniform distributions.\n\nSolution to Question 7a\n\n\n\n\n\n\n\nQuestion 7b\n\nIf we pick a random sample of size \\(n=10\\), how likely is it that \\(X_{\\rm{max}}\\) is greater than or equal to \\(0.9\\)?\n\n\n\n\n\n\nTip\n\n\n\nUse the formulas from your answer to Question 7a and make use of the formulas for the cdf and/or pdf we derived for \\(X_{\\rm{max}}\\).\n\n\n\nSolution to Question 7b"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#sample-maximum-comparing-theory-with-simulation",
    "href": "12-Sampling-Dist-Other.html#sample-maximum-comparing-theory-with-simulation",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Sample Maximum: Comparing Theory with Simulation",
    "text": "Sample Maximum: Comparing Theory with Simulation\n\nIn Question 8 below, we check our analytic solutions for Question 7 using a statistical simulation."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-8",
    "href": "12-Sampling-Dist-Other.html#question-8",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 8",
    "text": "Question 8\n\nReplace each of the three ?? in the code cell below with appropriate code. Then run the completed code to create and plot a sampling distribution for the sample maximum for samples size \\(n=10\\) from \\(X \\sim \\mbox{Unif}(0,1)\\).\n\nSolution to Question 8\n\nReplace each of the three ?? in the code cell below and run to create and plot a sampling distribution for the sample maximum for samples size \\(n=10\\) picked from \\(X \\sim \\mbox{Unif}(0,1)\\).\n\n# creates an empty vector to store results\nsamp.max &lt;- numeric(10000)\n\n# a for loop that generates 10000 random samples \n# calculates the sample maximum\nfor (i in 1:10000)\n{\n##################################\n# Replace both ?? in the for loop\n##################################\n  temp.sample &lt;- ??  # Step 1: Pick random sample from population\n  samp.max[i] &lt;- ??  # Step 2: Calculate sample statistic\n}\n\n\n# plot the sampling distribution\n#####################################\n# Replace the ?? in the hist command\n#####################################\nhist(??, \n     breaks = 20,\n     xlab = \"Maximum of Sample (n=10)\",\n     main = \"Dist. of Sample Max\")\nabline(v = 0.9, col = \"red\", lwd = 2, lty = 2)  # marking x_max = 0.9"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-9",
    "href": "12-Sampling-Dist-Other.html#question-9",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 9",
    "text": "Question 9\n\nBased on the results of the sampling distribution for the sample maximum stored in samp.max after running the code above:\n\nUse your sampling distribution from Question 8 to approximate \\(P(X_{\\rm{max}} \\geq 0.9)\\).\n\nHint: Use a logical test involving samp.max and either a sum() or mean() command.\n\nCompare your approximation to the exact value you found in Question 7b.\n\n\nSolution to Question 9\n\n\n# use results in samp.max to\n# approx P(X_max &gt;= 0.9)"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#question-10",
    "href": "12-Sampling-Dist-Other.html#question-10",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Question 10",
    "text": "Question 10\n\nThis question guides us through the calculation of a statistic to measure how well racial demographics of the people stopped by NYPD in 2022 match the overall racial demographics of NYC in 2022.\n\nQuestion 10a\n\nFirst, we calculate the expected totals for each racial category. Each expected total is the proportion of the population in the group, \\(p_i\\), times the total sample size, \\(n\\).\n\nComplete the code cell below to calculate and store each expected value \\(\\mbox{Expected}_i = n p_i\\) in the vector expect.\nAfter running the code, make sure the output matches the values in expected column in the table below.\n\n\n\n\nRace\nObserved\nNYC Population\nExpected\n\n\n\n\nBlack\n8,863\n\\(0.23\\)\n\\(3473.46\\)\n\n\nLatinx\n4,477\n\\(0.29\\)\n\\(4379.58\\)\n\n\nWhite\n1,077\n\\(0.32\\)\n\\(4832.64\\)\n\n\nAsian/Pacific Islander\n320\n\\(0.14\\)\n\\(2114.28\\)\n\n\nMixed/Other\n365\n\\(0.02\\)\n\\(302.04\\)\n\n\nTotal\n15102\n1\n\\(15102\\)\n\n\n\n\nSolution to Question 10a\n\n\nn &lt;- sum(observe)  # sample size = total number of observed\nexpect &lt;- ??  # compute vector of expected totals\nexpect    # print values to screen\n\n\n\n\nQuestion 10b\n\nNext, we calculate the difference in the observed and expected totals for each category. Since we do not want positive and negative differences to cancel out, we square each distance, \\((O_i - E_i)^2\\).\n\nComplete the code cell below to calculate and store the each of the squared differences \\((O_i - E_i)^2\\) in the vector named sq.diff.\nRun the completed code cell before continuing to the next part.\n\n\nSolution to Question 10b\n\n\nsq.diff &lt;- ??  # compute vector of (O-E)^2 values\nsq.diff  # print values to screen\n\n\n\n\nQuestion 10c\n\nSince larger categories are going to have larger differences, we consider each squared distance \\((O_i - E_i)^2\\) relative to the expected size of the category, \\(E_i\\).\n\nComplete the code cell below to calculate and store each ratio of squared difference over expected count, \\(\\frac{(O_i - E_i)^2}{E_i}\\), in the vector named relative.diff.\nAfter running the code, you can check your output with the values in the \\((E-O)^2/E\\) column in the table below Question 10d.\n\n\nSolution to Question 10c\n\n\nrelative.diff &lt;- ??  # compute vector of (O-E)^2/E values\nrelative.diff  # print values to screen\n\n\n\n\nQuestion 10d\n\nFinally, sum all the ratios in relative.diff together to compute a single statistic called the chi-square test statistic that is denoted \\(\\color{dodgerblue}{\\chi^2}\\),\n\\[\\color{dodgerblue}{\\chi^2 = \\sum_{i} \\frac{(O_i - E_i)^2}{E_i}}.\\]\n\nComplete the code cell below to sum together all \\(\\frac{(O_i - E_i)^2}{E_i}\\) previously computed and stored in the vector relative.diff.\nAfter running the code, you can check your output with the total of the \\((E-O)^2/E\\) column in the table below.\n\n\nSolution to Question 10d\n\n\nnyc.chi.sq &lt;- ?? # sum together all (O-E)^2/E to compute chi-sq stat\nnyc.chi.sq   # print value to screen\n\n\n\n\nRace\nObserved\nNYC Population\nExpected\n\\((E-O)^2/E\\)\n\n\n\n\nBlack\n8,863\n\\(0.23\\)\n\\(3473.46\\)\n\\(8362.6\\)\n\n\nLatinx\n4,477\n\\(0.29\\)\n\\(4379.58\\)\n\\(2.167\\)\n\n\nWhite\n1,077\n\\(0.32\\)\n\\(4832.64\\)\n\\(2918.7\\)\n\n\nAsian/Pacific Islander\n320\n\\(0.14\\)\n\\(2114.28\\)\n\\(1522.7\\)\n\n\nMixed/Other\n365\n\\(0.02\\)\n\\(302.04\\)\n\\(13.124\\)\n\n\nTotal\n15102\n1\n\\(15102\\)\n\\(\\color{dodgerblue}{12819.26}\\)"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#simulating-a-distribution-of-sample-chi2",
    "href": "12-Sampling-Dist-Other.html#simulating-a-distribution-of-sample-chi2",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Simulating a Distribution of Sample \\(\\chi^2\\)",
    "text": "Simulating a Distribution of Sample \\(\\chi^2\\)\n\n\nPick a random sample size \\(n\\) from the population (NYC in 2022).\n\n\ntemp &lt;- sample(c(\"Black\", \"Latinx\", \"White\", \"Asian/Pacific Islander\", \"Mixed/Other\"),\n                      size = 15102,  \n                      replace = TRUE, \n                      prob = c(0.23, 0.29, 0.32, 0.14, 0.02))\n\n\nCalculate the chi-square test statistic of the sample as described in the previous section.\n\n\no1 &lt;- sum(temp == \"Black\")\no2 &lt;- sum(temp == \"Latinx\")\no3 &lt;- sum(temp == \"White\")\no4 &lt;- sum(temp == \"Asian/Pacific Islander\")\no5 &lt;- sum(temp == \"Mixed/Other\")\n\ntemp.obs &lt;- c(o1, o2, o3, o4, o5)\nrel.diff &lt;- (temp.obs - expect)^2 / expect\nsample.chi &lt;-sum(rel.diff)\n\n\nRepeat many (1,000) times.\n\n\ndist.chi &lt;- numeric(1000) # creates an empty vector to store results\nn &lt;- 15102  # total number of observed\nprop &lt;- c(0.23, 0.29, 0.32, 0.14, 0.02)  # nyc racial demographics\nexpect &lt;- n * prop  # sample size * population prop\n\n# a for loop that generates 1000 random samples \nfor (i in 1:1000)\n{\n  temp&lt;- sample(c(\"Black\", \"Latinx\", \"White\", \"Asian/Pacific Islander\", \"Mixed/Other\"),\n                      size = n,  \n                      replace = TRUE, \n                      prob = c(0.23, 0.29, 0.32, 0.14, 0.02))\n  o1 &lt;- sum(temp == \"Black\")\n  o2 &lt;- sum(temp == \"Latinx\")\n  o3 &lt;- sum(temp == \"White\")\n  o4 &lt;- sum(temp == \"Asian/Pacific Islander\")\n  o5 &lt;- sum(temp == \"Mixed/Other\")\n  temp.obs &lt;- c(o1, o2, o3, o4, o5)\n  rel.diff &lt;- (temp.obs - expect)^2 / expect\n  dist.chi[i] &lt;-sum(rel.diff)\n}\n\nhist(dist.chi,  # plot stored sample statistics\n     breaks = 20,  # use 20 breaks\n     xlab = \"Sample Goodness-of-Fit\",  # x-axis label\n     main = \"Sampling Distribution of Chi-Squared Statistics\",  # main label\n     freq = FALSE)  # plot relative frequencies (density) on y-axis\n\nabline(v = 12819.26, col = \"red\", lwd = 2, lty = 2)  # draws vertical line at observed"
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#interpreting-results",
    "href": "12-Sampling-Dist-Other.html#interpreting-results",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "Interpreting Results",
    "text": "Interpreting Results\n\nBased on the stop-and-frisk data from NYPD in 2022, the chi-square test statistic was \\(12,819.26\\). That is off the chart compared to the chi-square statistics we obtained from all the other random samples of the same size. The figure below shows just how extreme the chi-square test statistic and far removed it appears from samples that are randomly picked from the same population. This does not necessarily imply there is racial discrimination as that is a very complex question. There are other factors we need to consider, and this is an important question that requires more analysis and looking deeper into the data.\n\nhist(dist.chi,  # plot stored sample statistics\n     breaks = 20,  # use 20 breaks\n     xlab = \"Sample Goodness-of-Fit\",  # x-axis label\n     main = \"Sampling Distribution of Chi-Squared Statistics\",  # main label\n     freq = FALSE,  # plot relative frequencies (density) on y-axis\n     xlim = c(0, 14500),\n     xaxt = 'n')\naxis(1, at=seq(0, 13000, 1000), pos=0)  # customize ticks on x-axis\nabline(v = 12819.26, col = \"red\", lwd = 2, lty = 2)  # draws vertical line at observed\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "12-Sampling-Dist-Other.html#footnotes",
    "href": "12-Sampling-Dist-Other.html#footnotes",
    "title": "3.3: Sampling Distributions of Other Statistics",
    "section": "",
    "text": "Data from (NYPD Stop, Ask, and Frisk Data 2022↩︎\nData from US Census Data↩︎"
  },
  {
    "objectID": "13-Estimation-MLE.html",
    "href": "13-Estimation-MLE.html",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "",
    "text": "Case Study: Slot Machine Jackpots\nA strategic gambler believes they have identified a faulty slot machine which pays out significantly more money than the other slot machines. She and her friends watch the machine 24 hours a day for 7 days and observed the slot machine paid out the $\\(1,\\!000,\\!000\\) jackpot prize 10 times during the week. How can she figure out whether the machine is faulty or whether the number of jackpot prizes are within reason?\nWe motivate maximum likelihood estimation with the following question:\nThe likelihood function \\[\\color{dodgerblue}{L(\\theta)= L( \\theta \\mid x_1, x_2, \\ldots x_n)}\\] gives the likelihood of the parameter \\(\\theta\\) given the observed sample data. A maximum likelihood estimate (MLE), denoted \\(\\color{dodgerblue}{\\mathbf{\\hat{\\theta}_{\\rm MLE}}}\\), is the value of \\(\\theta\\) that gives the maximum value of the likelihood function \\(L(\\theta)\\).\nLet \\(f(x; \\theta)\\) denote the pdf of a random variable \\(X\\) with associated parameter \\(\\theta\\). Suppose \\(X_1, X_2, \\ldots , X_n\\) are random samples from this distribution, and \\(x_1, x_2, \\ldots , x_n\\) are the corresponding observed values.\n\\[\\color{dodgerblue}{\\boxed{L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta).}}\\]\nIn Question 2 we derived an expression for the likelihood function \\(L(\\lambda)\\) given the random sample of \\(n=4\\) values we picked from \\(X \\sim \\mbox{Pois}( \\lambda)\\) and stored in the vector x. Recall Poisson distributions have pmf\n\\[f(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\qquad \\mbox{for } x = 0, 1, 2, \\ldots .\\]\nIf we pick a sample of \\(n=4\\) values we denote \\(X_1 = x_1\\), \\(X_2 = x_2\\), \\(X_3 = x_3\\), and \\(X_4 = x_4\\), then the likelihood function is\n\\[L(\\lambda) = L(\\theta \\mid x_1, x_2, \\ldots , x_n) = \\left( \\frac{\\lambda^{x_1} e^{-\\lambda}}{x_1!} \\right) \\left( \\frac{\\lambda^{x_2} e^{-\\lambda}}{x_2!} \\right) \\left( \\frac{\\lambda^{x_3} e^{-\\lambda}}{x_3!} \\right) \\left( \\frac{\\lambda^{x_4} e^{-\\lambda}}{x_4!} \\right).\\]\nWe will use the random sample generated by the code cell below. Note x is a vector consisting of values x[1] \\(=9\\), x[2] \\(=8\\), x[3] \\(=6\\), and x[4] \\(=5\\).\nset.seed(612)\n\nx &lt;- rpois(4, true.mean)\nx\n\n[1] 9 8 6 5\nThe random sample \\((9,8,6,5)\\) picked from \\(X \\sim \\mbox{Pois}(\\lambda)\\) gave \\(\\hat{\\lambda}_{\\rm{MLE}} = 7\\). If we pick another random sample \\(n=4\\) from the population \\(X \\sim \\mbox{Pois}(\\lambda)\\), how much will our estimate for \\(\\hat{\\lambda}_{\\rm{MLE}}\\) change? Some desirable properties for the distribution of \\(\\hat{\\lambda}_{\\rm{MLE}}\\) values from different random samples would be:\nSteps for finding MLE, \\(\\hat{\\theta}_{\\rm MLE}\\):\n\\[L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\\]\nLogarithmic functions such as \\(y = \\ln{x}\\) are increasing functions. The larger the input \\(x\\), the larger the output \\(y = \\ln{x}\\) becomes. Thus, the value of \\(\\theta\\) that gives the maximum value of \\(L(\\theta)\\) will also correspond to the value of \\(\\theta\\) that gives the maximum value of the function \\(y = \\ln{(L(\\theta))}\\), and vice versa:\nWe call the the natural log of the likelihood function, \\(\\color{dodgerblue}{y=\\ln{(L(\\theta}))}\\), the log-likelihood function.\n(a) Maximum of Likelihood\n\n\n\n\n\n\n\n(b) Maximum of Log-Likelihood\n\n\n\n\nFigure 22.1: Comparing Maxima\nSteps for finding MLE, \\(\\hat{\\theta}_{\\rm MLE}\\), using a log-likelihood function:\n\\[L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\\] 2. Apply the natural log to \\(L(\\theta)\\) to derive the log-likelihood function \\(y = \\ln{(L(\\theta))}\\). Simplify using properties of the natural log before moving to the next step.\nIn Question 11, we found a general formula for \\(\\hat{\\lambda}_{\\rm{MLE}}\\), the MLE of exponential distributions in general. We cannot use R to numerically check our analytic results since our result is a formula that depends on the values of the \\(x_i\\)’s. We can test our formula on many different random samples and check to make sure our formula gives consistent answers with numeric solutions in R. Using calculus to derive the general formula for \\(\\hat{\\lambda}_{\\rm{MLE}}\\) in Question 11 is incredibly convenient since now we have a “shortcut” formula that we can use for finding MLE estimates for any random sample from an exponential distributions.\nSo far we have observed:\nFor normal distributions \\(X \\sim N(\\mu, \\sigma)\\), the maximum likelihood estimates of \\(\\mu\\) and \\(\\theta\\) are \\[\\hat{\\mu}_{\\rm{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x} \\quad \\mbox{ and } \\quad \\hat{\\sigma}_{\\rm{MLE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2}.\\]\nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "13-Estimation-MLE.html#question-1",
    "href": "13-Estimation-MLE.html#question-1",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 1",
    "text": "Question 1\n\nThey decide to compare the performance of the suspect slot machine to other slot machines. They pick a random sample of \\(n=4\\) other slot machines and record how many jackpot prizes each machine pays over a one week time frame. Let random variable \\(X\\) denote the number of jackpots a randomly selected slot machine pays out in week. What distribution do you think best models \\(X\\)? Give a corresponding formula for the probability mass function of \\(X\\).\n\nHint: Should we use a discrete or continuous random variable?\nHint: See either appendix of common discrete random variables or appendix of common continuous random variables for additional help.\n\n\nSolution to Question 1"
  },
  {
    "objectID": "13-Estimation-MLE.html#collecting-data",
    "href": "13-Estimation-MLE.html#collecting-data",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Collecting Data",
    "text": "Collecting Data\n\nWe will simulate collecting a data sample.\n\nRun the code cell below to “secretly” generate a population mean that we store in true.mean.\nThe command set.seed(827) will seed the randomization so we all have the same population mean.\nDo not print the output to screen. Keep true.mean secret for now!\n\n\n# set randomization for seeding population mean\nset.seed(827)  \n\n# pick a population mean that will be fixed but unknown to us\ntrue.mean &lt;- sample(3:8, size=1)\n\nNext, we generate a sample size \\(n=4\\).\n\nRun the code cell below to generate your random sample.\nEach observation \\(x_i\\) in the vector x corresponds to the number of jackpots a randomly selected slot machine paid out in one week.\nThe command set.seed(612) will seed the randomization so my sample x remains fixed.\n\nYou can delete the command set.seed(612) to generate a different random sample picked from the same population.\nThen you can compare the estimate obtained from your sample with the estimate based on the sample generated below.\n\nInspect the values in your sample after running.\n\n\nset.seed(612)\n\nx &lt;- rpois(4, true.mean)\nx\n\n[1] 9 8 6 5\n\n\n\nThe sample generated by set.seed(612) is\n\n\\[x_1=9 \\ ,\\  x_2=8\\ ,\\  x_3=6 \\ , \\ x_4=5.\\]"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-2",
    "href": "13-Estimation-MLE.html#question-2",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the probability mass function from Question 1 and your sample generated by the code cell above stored in x, what is the probability of picking the random sample \\(x_1\\), \\(x_2\\), \\(x_3\\), \\(x_4\\) stored in x? Your answer will be a formula that depends on the parameter \\(\\lambda\\).\n\nSolution to Question 2"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-3",
    "href": "13-Estimation-MLE.html#question-3",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 3",
    "text": "Question 3\n\nFind the value of \\(\\lambda\\) that maximizes the likelihood function from Question 2.\n\n\n\n\n\n\nTip\n\n\n\nRecall from calculus that global maxima occur at end points or critical points where \\(\\frac{d L}{d \\lambda} = 0\\) or is undefined.\n\n\n\nSolution to Question 3\n\n  \n\n\nPlotting the Likelihood Function for Question 3\n\nGiven the random sample \\(x_1=9, x_2=8, x_3=6, x_4=5\\), the resulting likelihood derived in Question 2 is\n\\[L({\\color{tomato}\\lambda}) = \\frac{{\\color{tomato}\\lambda}^{28} e^{-4{\\color{tomato}\\lambda}}}{(9!)(8!)(6!)(5!)}.\\]\nIn Question 3, we find the value of \\(\\lambda\\) that maximizes the likelihood function \\(L(\\lambda)\\) using optimization methods from calculus. It is always a good idea to check our work. If we have access to technology, we can plot the likelihood function and identify the approximate value of \\(\\lambda\\) that gives the maximum value of \\(L(\\lambda)\\).\n\nlam &lt;- seq(3, 11, 0.1)  # values of lambda on x-axis\nlike.est &lt;- lam^(sum(x)) * exp(-4*lam)/prod(factorial(x))  # values of L(lambda)\n\nplot(lam, like.est,  # plot lam and likelihood on x and y axes\n     type = \"l\",  # connect plotted points with a curve\n     ylab = \"L(lambda)\",  # y-axis label\n     xlab = \"lambda\",  # x-axis label\n     main = \"Plot of Likelihood Function\")  # main label\n\npoints(x = 7, y = 0.0002515952, cex = 2, pch = 20, col = \"tomato\")  # point at max\n\naxis(1, at=c(7), col.axis = \"tomato\")  # marking MLE estimate\nabline(v = 7, col = \"tomato\", lwd = 2, lty = 2)  # marking MLE estimate"
  },
  {
    "objectID": "13-Estimation-MLE.html#revealing-the-actual-value-of-lambda",
    "href": "13-Estimation-MLE.html#revealing-the-actual-value-of-lambda",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Revealing the Actual Value of \\(\\lambda\\)",
    "text": "Revealing the Actual Value of \\(\\lambda\\)\n\nWe picked a value for \\(\\lambda\\) and stored it in true.mean. We have not revealed what the actual value of \\(\\lambda\\) is. Run the code cell below to see that actual value of \\(\\lambda\\), and compare your answer for \\(\\hat{\\lambda}_{\\rm{MLE}}\\) in Question 3 with the actual value of \\(\\lambda\\).\n\ntrue.mean\n\n[1] 8"
  },
  {
    "objectID": "13-Estimation-MLE.html#defining-the-likelihood-function-as-product",
    "href": "13-Estimation-MLE.html#defining-the-likelihood-function-as-product",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Defining the Likelihood Function as Product",
    "text": "Defining the Likelihood Function as Product\n\nIn the code cell below, we input the likelihood function.\n\nTo define a symbolic function, we use the command function(lam) [expr].\n\nWe use lam to denote our variable, \\(\\lambda\\).\nWe enter an appropriate formula in place of [expr].\n\n[expr] is the product of the \\(4\\) pmf’s of the Poisson distribution.\nWe name the newly created function like.\nTo evaluate the function like at \\(\\lambda = 7\\), we use the command like(7).\n\n\nlike &lt;- function(lam) lam^x[1] * exp(-lam)/factorial(x[1]) *\n                      lam^x[2] * exp(-lam)/factorial(x[2]) *\n                      lam^x[3] * exp(-lam)/factorial(x[3]) *\n                      lam^x[4] * exp(-lam)/factorial(x[4])\n\nlike(7)\n\n[1] 0.0002515952"
  },
  {
    "objectID": "13-Estimation-MLE.html#improving-the-code-for-a-likelihood-function",
    "href": "13-Estimation-MLE.html#improving-the-code-for-a-likelihood-function",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Improving the Code for a Likelihood Function",
    "text": "Improving the Code for a Likelihood Function\n\nIf we have a sample size \\(n=100\\) instead of \\(n=4\\), we would not want to code the likelihood as we did in the previous code cell. We can streamline the process using a for loop that utilizes the structure of likelihood functions:\n\nEach term in the product uses the same formula for the pmf.\nThe likelihood function is a product of all the pmf’s.\n\nIn the slot machine example, we have \\(X \\sim \\mbox{Pois}(\\lambda)\\) and a sample \\(x_1=9\\), \\(x_2=8\\) , \\(x_3=6\\), and \\(x_4=5\\). The vectors x and pmf are therefore\n\\[ x = (9, 8, 6, 5) \\quad \\mbox{and} \\quad \\mbox{pmf} = \\left( \\frac{\\lambda^{9} e^{-\\lambda}}{9!} , \\frac{\\lambda^{8} e^{-\\lambda}}{8!} , \\frac{\\lambda^{6} e^{-\\lambda}}{6!} , \\frac{\\lambda^{5} e^{-\\lambda}}{5!} \\right).\\]\nThe likelihood function like is the product of the entries in the vector pmf. We can substitute different values for the parameter \\(\\lambda\\) into the function like and compute different values of the likelihood function.\n\nRun the code cell below to compute the likelihood that \\(\\lambda = 7\\) given the sample x.\n\n\nlike &lt;- function(lam){\n  pmf &lt;- lam^x * exp(-lam)/factorial(x)\n  prod(pmf)\n}\n\nlike(7)\n\n[1] 0.0002515952\n\n\n\nUsing Built-In Distribution Functions\n\nFor many common distributions, R has built in functions to compute the values of pmf’s for many discrete random variables and pdf’s for continuous random variables. For Poisson distributions, the function dpois(x, lam) calculates the value of \\(f(x; \\lambda) = \\frac{\\lambda^{x} e^{-\\lambda}}{x!}\\).\n\nTherefore, we can use dpois(x, lam) in place of the expression lam^x * exp(-lam)/factorial(x).\nThe code cell below makes use of the dpois(x, lam) function and saves us the trouble of typing the formula out ourselves!\nRun the code to evaluate the function at \\(\\lambda = 7\\) to make sure the result is consistent with our previous functions.\n\n\nlike &lt;- function(lam){\n  pmf &lt;- dpois(x, lam)\n  prod(pmf)\n}\n\nlike(7)\n\n[1] 0.0002515952"
  },
  {
    "objectID": "13-Estimation-MLE.html#sec-opt-r",
    "href": "13-Estimation-MLE.html#sec-opt-r",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Optimizing the Likelihood Function in R",
    "text": "Optimizing the Likelihood Function in R\n\nIn Question 3 we used methods from calculus to find the value of \\(\\theta\\) that maximizes the likelihood function \\(L(\\theta)\\). We can check those results using the command optimize(function, interval, maximum = TRUE).\n\nfunction is the name of the function where we stored the likelihood function.\ninterval is the interval of parameter values over which we maximize the likelihood function.\n\nUsing c(0,100) means we will find the maximum of \\(L(\\theta)\\) over \\(0 &lt; \\lambda &lt; 100\\).\nBased on the values in our sample, we can narrow the interval to save a little computing time.\n\nmaximum = TRUE option means optimize() will identify the maximum of the function.\n\nNote the default for optimize() is to find the minimum value.\n\nRun the command below to calculate \\(\\hat{\\lambda}_{\\rm{MLE}}\\) for the slot machine example.\n\n\noptimize(like, c(0,100), maximum = TRUE)\n\n$maximum\n[1] 7.000001\n\n$objective\n[1] 0.0002515952"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-4",
    "href": "13-Estimation-MLE.html#question-4",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 4",
    "text": "Question 4\n\nIf population \\(X \\sim \\mbox{Pois}(\\lambda)\\) is the number of jackpot payouts a randomly selected slot machine has in one week:\n\nWhat is the practical interpretation of the value of \\(\\lambda\\)?\nIf we pick a random sample of 4 slot machines and find \\(x_1=9\\), \\(x_2=8\\) , \\(x_3=6\\), and \\(x_4=5\\), explain why an estimate \\(\\hat{\\lambda}_{\\rm{MLE}} = 7\\) makes practical sense.\n\n\nSolution to Question 4"
  },
  {
    "objectID": "13-Estimation-MLE.html#picking-another-random-sample",
    "href": "13-Estimation-MLE.html#picking-another-random-sample",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Picking Another Random Sample",
    "text": "Picking Another Random Sample\n\nThe random sample \\((9,8,6,5)\\) picked from \\(X \\sim \\mbox{Pois}(\\lambda)\\) gave \\(\\hat{\\lambda}_{\\rm{MLE}} = 7\\). The actual value of \\(\\lambda\\) we revealed the true.mean we used was \\(\\lambda = 8\\). Below we simulate picking another random sample of \\(n=4\\) values from the same population, \\(X \\sim \\mbox{Pois}(8)\\). Then we will compute \\(\\hat{\\lambda}_{\\rm{MLE}}\\) for this sample and see if we can start to pick up on a pattern.\n\nRun the code cell below to generate a new random sample stored in new.x.\n\n\nset.seed(012)  # fixes randomization\n\nnew.x &lt;- rpois(4, 8)  # pick another random sample n=4 from Pois(8)\nnew.x  # print results\n\n[1]  4 11 13  6\n\n\nThe new sample is \\(x_1=4\\), \\(x_2=11\\) , \\(x_3=13\\), and \\(x_4=6\\).\n\nRun the code cell to compute the value of \\(\\hat{\\lambda}_{\\rm{MLE}}\\) for this new sample.\n\n\n# be sure to first run code cell above to create new.x\nnew.like &lt;- function(lam){\n  new.pmf &lt;- dpois(new.x, lam)\n  prod(new.pmf)\n}\n\noptimize(new.like, c(0,100), maximum = TRUE)\n\n$maximum\n[1] 8.500015\n\n$objective\n[1] 1.589466e-05\n\n\n\nComparing Estimates\n\nLet’s compare the two random samples and their corresponding values for the MLE estimate.\n\n\n\n\n\n\n\nSample           \nValue of MLE\n\n\n\n\n\\((9, 8, 6, 5)\\)\n\\(7\\)\n\n\n\\((4, 11, 13, 6)\\)\n\\(8.5\\)\n\n\n\n\nNeither gives the correct value for \\(\\lambda\\) which is actually 8.\n\nOne estimate is too small and the other is too large.\n\nWe hope if we average all such MLE estimates together, we get the actual value \\(\\lambda = 8\\).\nWe have some sense of the variation, but generating many (10,000) random samples and looking at the distribution of many more MLE estimates will tell us more information about the variability."
  },
  {
    "objectID": "13-Estimation-MLE.html#analyzing-a-distribution-of-mles",
    "href": "13-Estimation-MLE.html#analyzing-a-distribution-of-mles",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Analyzing a Distribution of MLE’s",
    "text": "Analyzing a Distribution of MLE’s\n\nThe for loop in the code cell below generates a distribution of MLE’s for \\(\\lambda\\) based on 10,000 random samples size \\(n=4\\) picked from \\(X \\sim \\mbox{Pois}(8)\\). Inside the for loop we:\n\nPick a random sample size \\(n=4\\) stored in temp.x.\nCalculate the MLE based on temp.x that we store in the vector pois.mle.\n\nThen we plot a histogram to display pois.mle, the distribution of MLE’s from the 10,000 random samples each size \\(n=4\\)\n\nRun the code cell below to generate and plot a distribution of MLE’s.\n\n\npois.mle &lt;- numeric(10000)\n\nfor (i in 1:10000)\n{\n  temp.x &lt;- rpois(4, 8)  # given random sample\n  like.pois &lt;- function(lam){  # define likelihood function\n    pmf.pois &lt;- dpois(temp.x, lam)  \n    prod(pmf.pois)  \n}\n  pois.mle[i] &lt;- optimize(like.pois, c(0,100), maximum = TRUE)$maximum  # find max of likelihood function\n}\n\nhist(pois.mle, \n     breaks = 20,\n     xlab = \"MLE\",\n     main = \"Dist. of MLE's for Poisson Dist\")\nabline(v = 8, col = \"blue\", lwd = 2)  # plot at actual value of lambda"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-5",
    "href": "13-Estimation-MLE.html#question-5",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 5",
    "text": "Question 5\n\nCalculate the mean and variance of the distribution of MLE’s stored in mle.pois and interpret the results. How would you describe the shape of the distribution? What would you expect to happen to the distribution as \\(n\\) gets larger?\n\nSolution to Question 5\n\n\n# use code cell to answer questions above"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-6",
    "href": "13-Estimation-MLE.html#question-6",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 6",
    "text": "Question 6\n\nA sample \\((x_1, x_2, x_3, x_4) = (1,3,3,2)\\) is randomly selected from \\(X \\sim \\mbox{Binom}(3,p)\\). Give a formula the likelihood function.\n\nSolution to Question 6"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-7",
    "href": "13-Estimation-MLE.html#question-7",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 7",
    "text": "Question 7\n\nGive a formula the likelihood function given the sample \\(x_1, x_2, x_3, \\ldots, x_n\\) is randomly selected from \\(X \\sim \\mbox{Exp}(\\lambda)\\).\n\nSolution to Question 7"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-8",
    "href": "13-Estimation-MLE.html#question-8",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 8",
    "text": "Question 8\n\nUsing your answer from Question 6, find the MLE for \\(p\\) when \\((x_1, x_2, x_3, x_4) = (1,3,3,2)\\) is randomly selected from \\(X \\sim \\mbox{Binom}(3,p)\\).\n\nSolution to Question 8"
  },
  {
    "objectID": "13-Estimation-MLE.html#plotting-the-likelihood-function-for-question-8",
    "href": "13-Estimation-MLE.html#plotting-the-likelihood-function-for-question-8",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Plotting the Likelihood Function for Question 8",
    "text": "Plotting the Likelihood Function for Question 8\n\nRunning the code cell below will generate a plot of the likelihood function from Question 8. We should verify the maximum of the graph coincides with our answer to Question 8. There is nothing to edit in the code cell below.\n\np &lt;- seq(0, 1, 0.01)  # values of p on x-axis\nlike.binom &lt;- 9 * p^9 * (1-p)^3  # values of L(p)\ncv &lt;- 9 * (0.75)^9 * (1-0.75)^3\n\nplot(p, like.binom,  # plot p and likelihood on x and y axes\n     type = \"l\",  # connect plotted points with a curve\n     ylab = \"L(p)\",  # y-axis label\n     xlab = \"p\",  # x-axis label\n     main = \"Plot of Likelihood Function\")  # main label\n\npoints(x = 0.75, y = cv, cex = 2, pch = 20, col = \"tomato\")  # point at max\n\naxis(1, at=c(0.75), label=\"theta = 0.75\", col.axis = \"tomato\", pos=0.0015, cex = 1.5)  # marking MLE estimate\nabline(v = 0.75, col = \"tomato\", lwd = 2, lty = 2)  # marking MLE estimate"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-9",
    "href": "13-Estimation-MLE.html#question-9",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 9",
    "text": "Question 9\n\nRecall the sample from Question 6. Complete the code cell below to build a formula for the likelihood function and find the value of \\(\\hat{p}_{\\rm{MLE}}\\). Run the completed code cell to check your answer in Question 8.\n\n\n\n\n\n\nTip\n\n\n\n\nSee earlier code for constructing the likelihood function and finding the maximum.\nWhen considering the interval option for optimize(), keep in mind we are estimating the value of a proportion, \\(p\\).\n\n\n\n\nSolution to Question 9\n\nReplace each of the four ?? in the code cell below with appropriate code. Then run the completed code to compute the MLE estimate \\(\\hat{p}_{\\rm{MLE}}\\) for the sample x picked from \\(X \\sim \\mbox{Binom}(3,p)\\).\n\nx &lt;- c(1, 3, 3, 2)  # given random sample\n\nlike.binom &lt;- function(p){\n  pmf.binom &lt;- ??  # replace ??\n  prod(??)  # replace ??\n}\n\noptimize(??, ??, maximum = TRUE)  # replace both ??"
  },
  {
    "objectID": "13-Estimation-MLE.html#why-maximize-ylnltheta-instead-of-ltheta",
    "href": "13-Estimation-MLE.html#why-maximize-ylnltheta-instead-of-ltheta",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Why Maximize \\(y=\\ln{(L(\\theta}))\\) Instead of \\(L(\\theta)\\)?",
    "text": "Why Maximize \\(y=\\ln{(L(\\theta}))\\) Instead of \\(L(\\theta)\\)?\n\nConsider the likelihood function from Question 7,\n\\[L({\\color{tomato}\\lambda}) = {\\color{\\tomato}\\lambda}^n e^{- {\\color{tomato}\\lambda} \\sum_i x_i}.\\] To find the critical values, we first need to find an expression for the derivative \\(\\frac{d L}{d \\lambda}\\).\n\nWe need to apply the product rule.\nWe need to apply the chain rule to compute the derivative of \\(e^{- {\\color{tomato}\\lambda} \\sum_i x_i}\\).\nAfter finding an expression for the derivative, we would then need to solve a complicated equation.\nWe can use key properties of the natural log to help make the differentiation easier!"
  },
  {
    "objectID": "13-Estimation-MLE.html#sec-log-prop",
    "href": "13-Estimation-MLE.html#sec-log-prop",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Useful Properties of the Natural Log",
    "text": "Useful Properties of the Natural Log\n\nThe four properties of natural logs listed below will be helpful to recall when working with log-likelihood functions.\n\n\\(\\ln{(A \\cdot B)} = \\ln{A} + \\ln{B}\\)\n\\(\\ln{\\left( \\frac{A}{B} \\right)} = \\ln{A} - \\ln{B}\\)\n\\(\\ln{(A^k)} = k \\ln{A}\\)\n\\(\\ln{e^k} = k\\)\n\nLikelihood functions are by definition a product of functions and often involve \\(e\\). Taking the natural log of the likelihood function converts a product to a sum. It is much easier to take the derivative of sums than products!"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-10",
    "href": "13-Estimation-MLE.html#question-10",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 10",
    "text": "Question 10\n\nGive a simplified expression for the log-likelihood function corresponding to the likelihood function from the exponential distribution in Question 7,\n\\[L({\\color{tomato}\\lambda}) = {\\color{\\tomato}\\lambda}^n e^{- {\\color{tomato}\\lambda} \\sum_i x_i}.\\]\n\nSolution to Question 10"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-11",
    "href": "13-Estimation-MLE.html#question-11",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 11",
    "text": "Question 11\n\nFind a general formula for the MLE of \\(\\lambda\\) when \\(x_1, x_2, x_3, \\ldots, x_n\\) comes from \\(X \\sim \\mbox{Exp}(\\lambda)\\). Your answer will depend on the \\(x_i\\)’s.\n\n\n\n\n\n\nTip\n\n\n\n\nMaximize the log-likelihood function from Question 10.\nBe sure you simplify the log-likelihood before taking the derivative.\nRecall \\(\\lambda\\) is the variable when differentiating, and treat each \\(x_i\\) as a constant."
  },
  {
    "objectID": "13-Estimation-MLE.html#solution-to-question-11",
    "href": "13-Estimation-MLE.html#solution-to-question-11",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Solution to Question 11",
    "text": "Solution to Question 11"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-12",
    "href": "13-Estimation-MLE.html#question-12",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 12",
    "text": "Question 12\n\nFind a general formula for the MLE of \\(\\lambda\\) when \\(x_1, x_2, x_3, \\ldots, x_n\\) comes from \\(X \\sim \\mbox{Pois}(\\lambda)\\). Your answer will depend on the \\(x_i\\)’s.\n\nSolution to Question 12"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-13",
    "href": "13-Estimation-MLE.html#question-13",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 13",
    "text": "Question 13\n\nSuppose a random variable with \\(X_1=5\\), \\(X_2=9\\), \\(X_3=9\\), and \\(X_4=10\\) is drawn from a distribution with pdf\n\\[f(x; \\theta) = \\frac{\\theta}{2\\sqrt{x}}e^{-\\theta \\sqrt{x}}, \\quad \\mbox{where x $&gt;0$}.\\]\nFind an MLE for \\(\\theta\\).\n\nSolution to Question 13"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-14",
    "href": "13-Estimation-MLE.html#question-14",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 14",
    "text": "Question 14\n\nConsider the random sample of \\(n=40\\) values picked from a geometric distribution \\(X \\sim \\mbox{Geom}(p)\\) that are stored in the vector x.geom. Note the proportion true.p is unknown for now.\n\nRun the code cell below to generate a random value for true.p (which is hidden) and create x.geom which is printed to the screen.\n\n\nset.seed(117)  # fixes randomization of true.p and x.geom\ntrue.p &lt;- sample(seq(0.1, 0.9, 0.1), size=1)  # true.p hidden for now\n\nx.geom &lt;- rgeom(40, true.p)  # generate a random sample n=40  \nx.geom\n\n [1]  8  0  5  2  1  1  4  2 12  3  0  1  7  2  0  0  3 14  2  3  1  0  2  5  2\n[26]  6  4  3  2  1  2  5  0  3  1  4  0  1  0  0\n\n\n\nQuestion 14a\n\nBased on the sample stored in x.geom in the previous code cell, find the MLE estimate for \\(\\hat{p}_{\\rm{MLE}}\\).\n\nComplete and run the partially completed R code cell below.\n\n\nSolution to Question 14a\n\nReplace each of the four ?? in the code cell below with appropriate code. Then run the completed code to compute the MLE estimate \\(\\hat{p}_{\\rm{MLE}}\\) for the sample (size \\(n=40)\\) x.geom randomly selected from \\(X \\sim \\mbox{Geom}(p)\\).\n\n# be sure you first run code cell above to define x.geom\nlike.geom &lt;- function(p){\n  pmf.geom &lt;- ??  # replace ??\n  prod(??)  # replace ??\n}\n\n\noptimize(??, ??, maximum = TRUE)  # replace both ??\n\n\n\n\nQuestion 14b\n\nComplete the partially completed code cell below to generate a plot of a distribution of MLE’s for \\(\\hat{p}_{\\rm{MLE}}\\) based on 10,000 randomly selected samples from \\(X \\sim \\mbox{Geom}(p)\\).\n\nSolution to Question 14b\n\nReplace each of the five ?? in the code cell below with appropriate code. Then run the completed code to create and plot a distribution of MLE’s for samples size \\(n=40\\) from \\(X \\sim \\mbox{Geom}(p)\\).\n\nmle.geom &lt;- numeric(10000)\n\nfor (i in 1:10000)\n{\n  x.temp &lt;- ??  # replace ??, pick random sample size n=40\n  geom.like &lt;- function(p){\n    geom.pmf &lt;- ??  # replace ??\n    prod(??)  # replace ??\n}\n  mle.geom[i] &lt;- optimize(??, ??, maximum = TRUE)$maximum  # replace both ??\n}\n\nhist(mle.geom, \n     breaks = 20,\n     xlab = \"MLE\",\n     main = \"Dist. of MLE's\")\nabline(v = true.p, col = \"dodgerblue\", lwd = 2)  # actual value of p\nabline(v = true.p, col = \"tomato\", lwd = 2)  # expected value of MLE\n\n\n\n\nQuestion 14c\n\nBased on the distribution of MLE’s in Question 14b, do you believe the estimator \\(\\hat{p}_{\\rm{MLE}}\\) is unbiased or biased? Explain why or why not.\n\n\nSolution to Question 14c"
  },
  {
    "objectID": "14-Estimation-MOM.html",
    "href": "14-Estimation-MOM.html",
    "title": "4.2: Method of Moments Estimates",
    "section": "",
    "text": "Building a Model for Bear Cub Weight\nA biologist is studying black bears. In particular, what distribution best fits the weight (in ounces) of newborn black bear cubs? Their sample data contains 10 observations, \\(x_1, x_2, \\ldots , x_{10}\\), corresponding the birth weight of 10 randomly selected black bear cubs.\nset.seed(113)  # fix randomization\n\nmu.cub &lt;- sample(seq(8.6, 9.8, 0.1), size=1)  # set value of mu\nsigma.cub &lt;- sample(seq(0.9, 1.3, 0.05), size=1)  # set value of sigma\n\npicked &lt;- rnorm(10, mu.cub, sigma.cub)  # pick a random sample n=10\ncub &lt;- data.frame(wt = picked)  # save sample to the cub data frame\nround(cub$wt, 2)  # print sample to screen\n\n [1]  9.71  7.77  8.47  7.35  7.83  9.06  8.66  8.74 10.82  8.27\nLet \\(X\\) be a random variable with pdf \\(f(x)\\). For a positive integer \\(k\\), the kth theoretical moment of \\(X\\) is \\(\\color{dodgerblue}{\\mu_k = E \\left( X^k \\right) }\\).\n\\[\\color{dodgerblue}{\\boxed{\\mu_k = E \\left( X^k \\right) = \\int_{-\\infty}^{\\infty} x^kf(x) \\, dx \\ \\ \\ \\mbox{(for continuous)} \\qquad \\mbox{or} \\qquad  \\mu_k = E \\left( X^k \\right) = \\sum_X x^kp(x) \\ \\ \\ \\mbox{(for discrete)}}},\\]\nLet \\(X\\) be a random variable with pdf \\(f(x; \\theta_1, \\theta_2, \\ldots, \\theta_k)\\) and let \\(X_1\\), \\(X_2\\), \\(\\ldots\\), \\(X_n\\) be a random sample.\n\\[\\begin{aligned}\n\\mu_1 = \\int_{-\\infty}^{\\infty} xf(x) \\, dx &= \\frac{1}{n} \\sum_{i=1}^n X_i = M_1\\\\\n\\mu_2 = \\int_{-\\infty}^{\\infty} x^2f(x) \\, dx &= \\frac{1}{n} \\sum_{i=1}^n X_i^2 = M_2\\\\\n& \\vdots \\\\\n\\mu_k = \\int_{-\\infty}^{\\infty} x^kf(x) \\, dx &= \\frac{1}{n} \\sum_{i=1}^n X_i^k = M_k\n\\end{aligned}\\]"
  },
  {
    "objectID": "14-Estimation-MOM.html#what-is-the-best-fitting-model",
    "href": "14-Estimation-MOM.html#what-is-the-best-fitting-model",
    "title": "4.2: Method of Moments Estimates",
    "section": "What is the Best Fitting Model?",
    "text": "What is the Best Fitting Model?\n\nFrom the code cell above, we have generated the following sample of cub birth weights (in ounces) that are stored in cub$wt,\n\\[x = (9.71, 7.77, 8.47, 7.35, 7.83, 9.06, 8.66, 8.74, 10.82, 8.27 ).\\]\nOur goal is to find the “best” description of the distribution ofOur goal is to find the “best” description of the distribution of all black bear cub birth weights. The interpretation of “best” depends on the context of the question and can mean different things to different statisticians."
  },
  {
    "objectID": "14-Estimation-MOM.html#question-1",
    "href": "14-Estimation-MOM.html#question-1",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 1",
    "text": "Question 1\n\nThe figure below shows a dot plot of the selected sample (size \\(n=10\\)) of cub birth weights along with the plots of 4 different models we could choose for our data. Answer the questions based on plot figure below.\n\n\n\n\n\n\nFigure 15.1: Comparing Models for Bear Cub Weights\n\n\n\n\n\n\nQuestion 1a\n\nWhich of the models labeled 1-4 in the plot above do you believe best fits the sample data cub birth weights?\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nWhat type of continuous distribution best matches the graph you selected? Explain why in terms of birth weights of black bear cubs this distribution is reasonable and makes practical sense.\n\nHint: See the appendix of common continuous random variables section for some options.\n\n\nSolution to Question 1b\n\n\n\n\n\n\n\nQuestion 1c\nUsing the sample of birth weights cub$wt, give estimates for each of the parameter(s) in the distribution you identified in Question 1b.\n\nSolution to Question 1c\n\n\n# be sure you have already run the first code cell and \n# stored sample weights to variable `wt` in data frame `cub`\n\nBased on your code above, what are the values of the parameters of the distribution in Question 1b?"
  },
  {
    "objectID": "14-Estimation-MOM.html#identifying-key-properties-for-our-model",
    "href": "14-Estimation-MOM.html#identifying-key-properties-for-our-model",
    "title": "4.2: Method of Moments Estimates",
    "section": "Identifying Key Properties for Our Model",
    "text": "Identifying Key Properties for Our Model\n\nLet \\(X\\) be a random variable with pdf \\(f(x; \\theta_1, \\theta_2, \\ \\ldots \\theta_k)\\) that depends on parameters \\(\\theta_1, \\theta_2, \\ldots , \\theta_k\\). If we independently pick a random variables \\(X_1, X_2, \\ldots X_n\\) from population \\(X\\), we can determine the values of \\(\\theta_1, \\theta_2, \\ldots , \\theta_k\\) that best fit the data in the following sense:\n\nThe mean \\(\\mu_X = E(X)\\) of the population \\(X\\) equals the sample mean, \\(\\bar{x}\\).\nThe variance \\(\\sigma^2_X = \\mbox{Var}(X)\\) of the population equals the variance of the sample, \\(s^2\\).\nThe skewness of the population equals the skewness of the sample.\nThe “peakiness” (kurtosis) of the population equals the “peakiness” of the sample.\n\\(\\ldots\\) and so on.\n\nFor example, in the birth weight of black bear cubs example, we assumed the population of birth weights \\(X\\) is normally distributed. Normal distributions are determined by two parameters, \\(\\mu\\) and \\(\\sigma\\).\n\nThe random sample has \\(\\bar{x} = 8.668\\). We estimate \\(E(X) = \\mu=\\bar{x} = 8.668\\).\nThe random sample has \\(s^2 = 1.032\\). We estimate \\(\\mbox{Var(X)} = \\sigma^2=s^2=1.032\\).\n\n\nWe find values of the parameters so the properties of random variable \\(X\\) are equal to corresponding statistics from our sample."
  },
  {
    "objectID": "14-Estimation-MOM.html#interpretation-of-theoretical-moments",
    "href": "14-Estimation-MOM.html#interpretation-of-theoretical-moments",
    "title": "4.2: Method of Moments Estimates",
    "section": "Interpretation of Theoretical Moments",
    "text": "Interpretation of Theoretical Moments\n\n\nThe first moment is \\(\\color{dodgerblue}{\\mu_1 = E \\left( X \\right) }\\).\n\n\\(\\mu_1\\) is the mean.\n\nThe second moment is \\(\\color{tomato}{\\mu_2 = E \\left( X^2 \\right) }\\).\n\n\\(\\mu_2\\) is related (but not equal) to the variance.\nIf we can find \\(\\mbox{Var}(X)\\) and have computed the first theoretical moment, \\(\\mu_1\\), we have:\n\n\n\\[{\\color{tomato}{\\mu_{2}}} = \\mbox{Var}(X) + \\mu_1^2 \\qquad \\mbox{since} \\qquad \\mbox{Var}(X) = E \\big( (X-\\mu_1)^2 \\big) = {\\color{tomato}{E(X^2)}} - \\mu_1^2 = {\\color{tomato}{\\mu_{2}}} - \\mu_1^2.\\]\n\nThe third moment is \\(\\color{mediumseagreen}{\\mu_3 = E \\left( X^3 \\right) }\\).\n\n\\(\\mu_3\\) is related to the skewness of \\(X\\) which is defined as \\(E \\big( (X-\\mu_1)^3 \\big)\\)\n\n\n\n\n\nCredit: Diva Jain, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\nThe fourth moment is \\(\\color{mediumpurple}{\\mu_4 = E \\left( X^4 \\right) }\\).\n\n\\(\\mu_4\\) is related to the kurtosis of \\(X\\) which is defined as \\(E \\big( (X-\\mu_1)^4 \\big)\\).\n\nInformally, the kurtosis measures how “peaky” or flat the distribution is.\n\n\n\n\n\n\n\n\n\nFigure 16.1: A Graphical Overview of Kurtosis"
  },
  {
    "objectID": "14-Estimation-MOM.html#sample-moments",
    "href": "14-Estimation-MOM.html#sample-moments",
    "title": "4.2: Method of Moments Estimates",
    "section": "Sample Moments",
    "text": "Sample Moments\n\nFor a sample \\(X_1=x_1, X_2=x_2, \\ldots , X_n=x_n\\), we can calculate the corresponding sample moments.\n\nThe first sample moment is \\(\\displaystyle M_1 = \\frac{1}{n} \\sum_{i=1}^n x_i\\).\nThe second sample moment is $M_2 = _{i=1}^n x_i^2 $.\nThe kth sample moment is \\(\\color{dodgerblue}{\\displaystyle M_k = \\frac{1}{n} \\sum_{i=1}^n x_i^k }\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWe use Latin letters \\(\\color{dodgerblue}{M_k}\\) to denote sample moments and Greek letters \\(\\color{tomato}{\\mu_k}\\) to denote theoretical moments for the population."
  },
  {
    "objectID": "14-Estimation-MOM.html#question-2",
    "href": "14-Estimation-MOM.html#question-2",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 2",
    "text": "Question 2\n\nLet \\(X\\) be a random variable with pdf\n\\[f(x; \\lambda, \\delta)=\\lambda e^{-\\lambda(x-\\delta)}\\]\nfor \\(x &gt; \\delta\\) with parameters \\(\\lambda, \\delta &gt;0\\).\n\nQuestion 2a\n\nWrite out (but do not evaluate) integrals that represent the first and second theoretical moments.\n\nSolution to Question 2a\n\n\\[\\mu_1 = E(X) = \\int_{\\delta}^{\\infty} \\left(   ?? \\right)  \\, dx\\] \\[\\mu_2 = E(X^2) = \\int_{\\delta}^{\\infty} \\left(   ?? \\right)  \\, dx\\]\n\n\n\n\n\nQuestion 2b\n\nApplying integration methods, we can evaluate the integrals in Question 2a to get the following expressions for the first and second theoretical moments.\n\\[\\mu_1 = E(X) = \\delta + \\frac{1}{\\lambda}.\\]\n\\[\\mu_2 = E(X^2) = \\left( \\delta + \\frac{1}{\\lambda} \\right)^2 + \\frac{1}{\\lambda^2}.\\]\nWhat integration methods do you believe will be useful to integrate the formulas in Question 2a? Explain in words how you could evaluate each of the integrals.\n\nTime permitting: Refresh your integration skills by verifying the formulas for \\(\\mu_1\\) and \\(\\mu_2\\).\n\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nLet \\(X_1=3\\), \\(X_2=4\\), \\(X_3 = 5\\), and \\(X_4 = 8\\) be a random sample from the random variable \\(X\\) from Question 2. Find the first and second sample moments.\n\nSolution to Question 2c"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-3",
    "href": "14-Estimation-MOM.html#question-3",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 3",
    "text": "Question 3\n\nLet \\(X\\) be a random variable from Question 2 with pdf \\(\\displaystyle f(x; \\lambda, \\delta)=\\lambda e^{-\\lambda(x-\\delta)}\\) for \\(x &gt; \\delta\\) with parameters \\(\\lambda, \\delta &gt;0\\). The first and second theoretical moments (see Question 2a and Question 2b) are\n\\[\\mu_1 = E(X) = \\delta + \\frac{1}{\\lambda} \\qquad \\mbox{and} \\qquad\n\\mu_2 = E(X^2) = \\left( \\delta + \\frac{1}{\\lambda} \\right)^2 + \\frac{1}{\\lambda^2}.\\]\nLet \\(X_1=3\\), \\(X_2=4\\), \\(X_3 = 5\\), and \\(X_4 = 8\\) be a random sample from \\(X\\). Find \\(\\hat{\\lambda}_{\\rm{MoM}}\\) and \\(\\hat{\\delta}_{\\rm{MoM}}\\), the MoM estimates for parameters \\(\\lambda\\) and \\(\\delta\\). Hint: Use the sample moments from Question 2c.\n\nSolution to Question 3"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-4",
    "href": "14-Estimation-MOM.html#question-4",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 4",
    "text": "Question 4\n\nLet \\(X_1=1, X_2=3, X_3=7, X_4=10\\) be four numbers picked at random from a continuous uniform distribution on \\(\\lbrack \\alpha , \\beta \\rbrack\\). Find the MoM estimates of \\(\\alpha\\) and \\(\\beta\\).\n\n\n\n\n\n\nTip\n\n\n\nYou do not need to evaluate any integrals to find expressions for \\(\\mu_1\\) and \\(\\mu_2\\). Recall if \\(X\\) is a continuous uniform distribtion, we have\n\\[E(X) = \\frac{\\alpha + \\beta}{2}  \\qquad \\mbox{and} \\qquad \\mbox{Var}(X) = \\frac{\\beta- \\alpha}{12}.\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\\(\\mu_2 \\ne \\mbox{Var}(X)\\). However, you can derive a formula for \\(\\mu_2 = E(X^2)\\) from formulas for both \\(\\mbox{Var}(X)\\) and \\(E(X)\\).\n\n\n\nSolution to Question 4"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-5",
    "href": "14-Estimation-MOM.html#question-5",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(X_1=1, X_2=3, X_3=3, X_4=2\\) be four values picked at random from a binomial distribution \\(X \\sim \\mbox{Binom}(n,p)\\). Find the MoM estimates of \\(n\\) and \\(p\\).\n\nSolution to Question 5"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-6",
    "href": "14-Estimation-MOM.html#question-6",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 6",
    "text": "Question 6\n\nLet \\(X_1=x_1, X_2=x_2, \\ldots X_n=x_n\\) denote a random sample size \\(n\\) from the continuous uniform distribution on \\(\\lbrack \\alpha , \\beta \\rbrack\\).\n\nQuestion 6a\n\nDerive the following general formulas for the MoM estimates of \\(\\alpha\\) and \\(\\beta\\):\n\\[\\hat{\\alpha}_{\\rm{MoM}} = M_1 - \\sqrt{3} \\left( \\sqrt{ M_2 - M_1^2} \\right)\\] \\[\\hat{\\beta}_{\\rm{MoM}} = M_1 + \\sqrt{3} \\left( \\sqrt{ M_2 - M_1^2} \\right)\\]\nwhere \\(M_1=\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) and \\(M_2= \\frac{1}{n} \\sum_{i=1}^n x_i^2\\) denote the first and second sample moments, respectively. Find the MoM estimates of \\(\\alpha\\) and \\(\\beta\\).\n\nSolution to Question 6a\n\n\n\n\n\n\n\nQuestion 6b\n\nVerify your solution to Question 4 using the formulas for the MoM estimates for \\(\\alpha\\) and \\(\\beta\\) in Question 6a.\n\nComplete and run the partially completed R code cell below.\n\n\nSolution to Question 6b\n\nReplace each of the two ?? in the code cell below with appropriate code. Then run the completed code to check the MoM estimates for \\(\\hat{\\alpha}_{\\rm{MoM}}\\) and \\(\\hat{\\beta}_{\\rm{MoM}}\\) obtained Question 4.\n\nx.unif &lt;- c(1, 3, 7, 10)  # sample from question 4\nn &lt;- length(x.unif)  # length of sample\n\nm1 &lt;- sum(x.unif)/n  # first sample moment\nm2 &lt;- sum(x.unif^2)/n  # second sample moment\n\nalpha.hat &lt;- ??  # enter formula for MoM estimate for alpha\nbeta.hat &lt;- ??   # enter formula for MoM estimate for beta\n\n# print results to screen\nalpha.hat\nbeta.hat"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-7",
    "href": "14-Estimation-MOM.html#question-7",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 7",
    "text": "Question 7\n\nThe code below generates sampling distributions for MoM estimates for the parameters \\(\\alpha\\) and \\(\\beta\\) for random variable \\(X \\sim \\mbox{Unif}(\\alpha, \\beta)\\) using sample size \\(n=4\\).\n\nA total of 1,000 random samples each of size \\(n\\) are generated in the for loop.\nThe distribution of \\(\\hat{\\alpha}_{\\rm{MoM}}\\) values are stored in the vector mom.alpha.\nThe distribution of \\(\\hat{\\beta}_{\\rm{MoM}}\\) values are stored in the vector mom.beta.\n\n\n#############################\n# do not edit\n# run the code cell as is\n#############################\nn &lt;- 4  # sample size\n\nmom.alpha &lt;- numeric(1000)\nmom.beta &lt;- numeric(1000)\n\nfor (i in 1:1000)\n{\n  x.temp &lt;- runif(n, 0, 11)  # generate random sample\n  m1 &lt;- sum(x.temp)/n  # first sample moment\n  m2 &lt;- sum(x.temp^2)/n  # second sample moment\n  k &lt;- sqrt(3) * sqrt(m2 - m1^2)  # compute sqrt(3)*(m2 - m1^2)\n  mom.alpha[i] &lt;- m1 - k  # enter formula for MoM estimate for alpha\n  mom.beta[i] &lt;- m1 + k  # enter formula for MoM estimate for beta\n}\n\nThe distribution of \\(\\hat{\\alpha}_{\\rm{MoM}}\\) values generated by the code above is plotted in the histogram below.\n\nA blue vertical line is drawn at the actual value of \\(\\color{dodgerblue}{\\alpha=0}\\).\nA red vertical line is drawn at the value of \\(\\color{tomato}{\\hat{\\alpha}_{\\rm{MoM}}=-0.797}\\) we found for the sample in Question 4.\n\n\n#############################\n# do not edit\n# run the code cell as is\n#############################\nhist(mom.alpha, \n     breaks = 20,\n     xlab = \"MoM for alpha\",\n     main = \"Dist. of MoM's for alpha\")\nabline(v = 0, col = \"dodgerblue\", lwd = 2)  # plot at actual value of alpha\nabline(v = -0.797, col = \"tomato\", lwd = 2)  # plot at estimated value of alpha\n\n\n\n\nThe distribution of \\(\\hat{\\beta}_{\\rm{MoM}}\\) values generated by the code above is plotted in the histogram below.\n\nA blue vertical line is drawn at the actual value of \\(\\color{dodgerblue}{\\beta=11}\\).\nA red vertical line is drawn at the value of \\(\\color{tomato}{\\hat{\\beta}_{\\rm{MoM}}=11.297}\\) we found for the sample in Question 4.\n\n\n#############################\n# do not edit\n# run the code cell as is\n#############################\nhist(mom.beta, \n     breaks = 20,\n     xlab = \"MoM for beta\",\n     main = \"Dist. of MoM's for beta\")\nabline(v = 11, col = \"dodgerblue\", lwd = 2)  # plot at actual value of beta\nabline(v = 11.297, col = \"tomato\", lwd = 2)  # plot at estimated value of alpha\n\n\n\n\n\nQuestion 7a\n\nBased on inspecting the the sampling distributions plotted for \\(\\hat{\\alpha}_{\\rm{MoM}}\\) and \\(\\hat{\\beta}_{\\rm{MoM}}\\) in Question 7:\n\nDo you believe the MoM estimator for \\(\\alpha\\) is biased? Explain why or why not.\nDo you believe the MoM estimator for \\(\\beta\\) is biased? Explain why or why not.\nBase your answers on the distribution of all estimates, not just the red vertical lines corresponding to the sample from Question 4.\n\n\nSolution to Question 7a\n\n\n\n\n\n\n\nQuestion 7b\n\nCheck your answers in Question 7a more carefully using the MoM estimates stored in mom.alpha and mom.beta.\n\nHint: Recall an estimator \\(\\hat{\\theta}\\) is unbiased if \\(E(\\hat{\\theta}) = \\theta\\).\n\n\nSolution to Question 7b\n\n\n# check whether or not each estimator is biased\n\n\n\n\n\n\nQuestion 7c\n\nWhich estimator, \\(\\hat{\\alpha}_{\\rm{MoM}}\\) or \\(\\hat{\\beta}_{\\rm{MoM}}\\), is more precise?\n\nHint: Recall the precision of an estimator \\(\\hat{\\theta}\\) is often measured by \\(\\mbox{Var}(\\hat{\\theta}) = \\theta\\).\nHint: Use R code and the MoM estimates stored in mom.alpha and mom.beta.\n\n\nSolution to Question 7c\n\n\n# check which estimator is more precise\n\n\n\n\n\n\nQuestion 7d\n\nAdjust the sample size in the first line in the first code cell below to investigate what happens to the distributions of estimators \\(\\hat{\\alpha}_{\\rm{MoM}}\\) and \\(\\hat{\\beta}_{\\rm{MoM}}\\). In particular, as \\(n\\) gets larger and larger:\n\nDoes each estimator seem to get more, less, or no change in bias?\nDoes each estimator get more, less, or no change in variability?\nDoes the shape of each distribution change?\n\n\nSolution to Question 7d\n\n\n\nExperiment with different sample sizes, \\(n\\).\n\n\n#######################\n# adjust sample size\n#######################\nn &lt;- 4  # sample size\n\n#####################################\n# do not edit the rest of the code\n#####################################\n\nmom.alpha &lt;- numeric(1000)\nmom.beta &lt;- numeric(1000)\n\nfor (i in 1:1000)\n{\n  x.temp &lt;- runif(n, 0, 11)  # generate random sample\n  m1 &lt;- sum(x.temp)/n  # first sample moment\n  m2 &lt;- sum(x.temp^2)/n  # second sample moment\n  k &lt;- sqrt(3) * sqrt(m2 - m1^2)  # compute sqrt(3)*(m2 - m1^2)\n  mom.alpha[i] &lt;- m1 - k  # enter formula for MoM estimate for alpha\n  mom.beta[i] &lt;- m1 + k  # enter formula for MoM estimate for beta\n}\n\n\n##########################################\n# sampling distribution for MoM of alpha\n# do not edit cell, just run\n##########################################\nhist(mom.alpha, \n     breaks = 20,\n     xlab = \"MoM for alpha\",\n     main = \"Dist. of MoM's for alpha\")\nabline(v = 0, col = \"dodgerblue\", lwd = 2)  # plot at actual value of alpha\n\n\n##########################################\n# sampling distribution for MoM of beta\n# do not edit cell, just run\n##########################################\nhist(mom.beta, \n     breaks = 20,\n     xlab = \"MoM for beta\",\n     main = \"Dist. of MoM's for beta\")\nabline(v = 11, col = \"dodgerblue\", lwd = 2)  # plot at actual value of beta\n\n\n\n\nExploring Bias of Each Estimator\n\n\nAs \\(n\\) gets larger, does each estimator seem to get more, less, or no change in bias?\n\n\n# check for change to bias\n\n\n\n\n\nExploring Variability of Estimators\n\n\nAs \\(n\\) gets larger, does each estimator get more, less, or no change in variability?\n\n\n# check for change in variability\n\n\n\n\n\nExploring the Shape of Sampling Distributions\n\n\nDoes the shape of each distribution change as \\(n\\) gets larger?\n\n\n# check for change in shape\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "15-Properties-Estimators.html",
    "href": "15-Properties-Estimators.html",
    "title": "4.3: Properties of Estimators",
    "section": "",
    "text": "Comparing Estimators\nAt first glance, the question of which estimator is best may seem like a simple question.\nHowever, choosing the “best” estimator is not as straightforward as simply choosing the estimator that gives the value closest to the actual value. The parameters we are estimating are unknown values! We do not know where the center of the dart board is located. We cannot be certain which estimator leads to the closest estimate. Different samples will give different estimates even if we use the same formula for the estimator, and each estimate has some uncertainty due to sampling.\nWe can still choose a method that is more likely to give a better estimate (such as MLE) and/or minimizes the effect of the uncertainty due to sampling. Which properties are most important to consider depend on many factors. For example:\nThere are many properties of estimators worth considering when deciding between different estimators. In this section, we explore properties relating to accuracy (bias), precision (variability), and the mean squared error (MSE) which takes both bias and variability into consideration.\nNo matter what formula we choose as an estimator, the estimate we obtain will vary from sample to sample. We like an estimator to be, on average, equal to the parameter it is estimating. The bias of an estimator \\(\\hat{\\theta }\\) for parameter \\(\\theta\\) is defined as the difference in the average (expected) value of the estimator and the parameter \\(\\theta\\),\n\\[{\\large \\color{dodgerblue}{\\boxed{ \\mbox{Bias} = E(\\hat{\\theta}) - \\theta.}}}\\]\nEstimates that are perfectly unbiased may be impossible or unreasonable at times. In practice, we are satisfied when estimates are approximately unbiased, or when the bias gets closer and closer to 0 as the sample size, \\(n\\), gets larger.\nLet \\(\\hat{\\theta}\\) be an estimator for a parameter \\(\\theta\\). We can measure how precise \\(\\hat{\\theta}\\) is by considering how “spread out” the estimates obtained by selecting many random samples (each size \\(n\\)) and calculating an estimate \\(\\hat{\\theta}\\). The variance of the sampling distribution, \\(\\mbox{Var}(\\hat{\\theta})\\), measures the variability in estimates due to the uncertainty in random sampling. The standard error of \\(\\hat{\\theta}\\) is the standard deviation of the sampling distribution for \\(\\hat{\\theta}\\) and also commonly used.\nWe have explored bias and variability of estimators. It is not always possible or reasonable to use an unbiased estimator. Moreover, in some cases an estimator with a little bit of bias and very little variability might be preferred over an unbiased estimator that has a lot of variability. Choosing which estimator is preferred often involves a trade-off between bias and variability.\nThe Mean Squared Error (MSE) of an estimator \\(\\hat{\\theta}\\) measures the average squared distance between the estimator and the parameter \\(\\theta\\),\n\\[{\\color{dodgerblue}{\\mbox{MSE} \\big[ \\hat{\\theta} \\big] = E \\big[ (\\hat{\\theta}-\\theta)^2 \\big]}}.\\]\n\\[\\boxed{\\large {\\color{dodgerblue}{ \\mbox{MSE} \\big[ \\hat{\\theta} \\big] }} = {\\color{tomato}{\\mbox{Var} \\big[ \\hat{\\theta} \\big]}} + {\\color{mediumseagreen}{\\left( \\mbox{Bias}(\\hat{\\theta}) \\right)^2. }}}\\]"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-1",
    "href": "15-Properties-Estimators.html#question-1",
    "title": "4.3: Properties of Estimators",
    "section": "Question 1",
    "text": "Question 1\n\nSuppose our population parameter of interest is the center of a dart board. We use four different methods for throwing darts and the results of those four different methods are displayed in Figure 15.1.\n\n\n\n\n\n\n\nDart Method 1\n\n\n\n\n\n\n\nDart Method 2\n\n\n\n\n\n\n\nDart Method 3\n\n\n\n\n\n\n\nDart Method 4\n\n\n\n\nFigure 15.1: Comparing the results of four different methods for throwing darts.  Credit: Arbeck, CC BY 4.0, via Wikimedia Commons\n\n\n\nQuestion 1a\n\nRank the results of the four dart methods in terms of accuracy, from most to least accurate. Explain your reasoning.\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nRank the results of the four dart methods in terms of precision, from most to least precise. Explain your reasoning.\n\nSolution to Question 1b\n\n\n\n\n\n\n\nQuestion 1c\n\nRank the results of the four dart methods from best to worst overall. Explain your reasoning.\n\nSolution to Question 1c\n\n\n\n\n\n\n\nQuestion 1d\n\nFour different sampling distributions of the results of the four dart throwing methods are plotted in Figure 15.2. The location of the population parameter (the center of the dart board) is indicated by the dashed red line. The mean of the sampling distribution is indicated by the solid blue vertical line. Match each of the distributions labeled A-D below to one of the four dart boards displayed in Question 1.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\nFigure 15.2: Matching Distributions to Dart Boards\n\n\n\nSolution to Question 1d\n\n\nDart Distribution A matches dart method ??.\nDart Distribution B matches dart method ??.\nDart Distribution C matches dart method ??.\nDart Distribution D matches dart method ??."
  },
  {
    "objectID": "15-Properties-Estimators.html#question-2",
    "href": "15-Properties-Estimators.html#question-2",
    "title": "4.3: Properties of Estimators",
    "section": "Question 2",
    "text": "Question 2\n\nLet \\(X\\) denote the diastolic blood pressure (in mm Hg) of a randomly selected woman from the Pima Indian Community. The Pima Indian Community is mostly located outside of Phoenix, Arizona. The data1 used in this example is from the data frame Pima.tr in the MASS package.\nIf we suppose blood pressure is normally distributed, then we have \\(X \\sim N(\\mu, \\sigma)\\). We would like to decide which estimator is best for the population mean \\(\\mu\\). We pick a random sample of \\(n=20\\) women in the code cell below.\n\nlibrary(MASS)  # load MASS package to access data\n\n\nset.seed(20)  # fix randomization of sample\nx &lt;- sample(Pima.tr$bp, size=20)  # pick a random sample of 20 blood pressures\nx\n\n [1]  58  64  64  80  68  76  72 102  68  52  90  70  56  62  74  74  76  82  85\n[20]  52\n\n\nThe random sample of diastolic blood pressure values is \\[\\mathbf{x} = (58, 64, 64, 80, 68, 76, 72, 102, 68, 52, 90, 70, 56, 62, 74, 74, 76, 82, 85, 52).\\] Consider the following estimates for the population mean \\(\\mu\\):\n1. The sample mean: \\(\\hat{\\mu}_1 = \\bar{X} = \\dfrac{\\sum_{i=1}^n X_i}{n}\\).\n\n# 1. calculate sample mean\nmu.hat1 &lt;- mean(x)\nmu.hat1\n\n[1] 71.25\n\n\n2. The sample median, denoted \\(\\hat{\\mu}_2 = \\mbox{median}\\).\n\n# 2. calculate sample median\nmu.hat2 &lt;- median(x)\nmu.hat2 \n\n[1] 71\n\n\n3. The mid-range of the sample, \\(\\hat{\\mu}_3 = \\dfrac{X_{\\rm{max}} + X_{\\rm{min}}}{2}\\).\n\n# 3. calculate sample mid range\nmu.hat3 &lt;- (max(x) + min(x)) / 2\nmu.hat3\n\n[1] 77\n\n\n4. The sample trimmed (10%) mean, \\(\\bar{x}_{\\rm{tr}(10)}\\).\n\nWe exclude the smallest 10% of the values. The smallest 2 values, 52 and 52, are excluded.\nWe exclude the largest 10% of the values. The largest 2 values, 102 and 90, are excluded.\nWe compute the mean of the remaining 16 values.\n\n\n# 4. calculate sample trimmed (10%) mean\nmu.hat4 &lt;- mean(x, trim = 0.1)\nmu.hat4\n\n[1] 70.5625\n\n\n\nQuestion 2a\n\nWhich estimator (mean, median, mid-range, or trimmed mean) do you believe is best? Which estimator do you believe is the worst? Explain your reasoning.\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nTo help decide which estimator performs best, we can consider the sampling distribution of estimates obtained from many different random samples (each of size \\(n=20\\)) chosen independently from the same population. Based on the sampling distributions for \\(\\hat{\\mu}_1\\), \\(\\hat{\\mu}_2\\), \\(\\hat{\\mu}_3\\), and \\(\\hat{\\mu}_4\\) (mean, median, mid-range, and trimmed mean, respectively) in Figure 16.1, rank the four estimators from least to most bias and from most to least precise. Explain your reasoning.\n\n\n\n\n\n\n\n(a) Distribution of Sample Means\n\n\n\n\n\n\n\n(b) Distribution of Sample Medians\n\n\n\n\n\n\n\n\n\n(c) Distribution of Sample Mid-ranges\n\n\n\n\n\n\n\n(d) Distribution of Sample Trimmed Means\n\n\n\n\nFigure 16.1: Comparing Estimators for Mean Blood Pressure\n\n\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nWe still let \\(X\\) denote the diastolic blood pressure (in mm Hg) of a randomly selected woman from the Pima Indian Community. However, now we suppose blood pressure is uniformly distributed over the interval \\(\\lbrack 41.26, 101.26 \\rbrack\\). Although a uniform distribution would not make practical sense for blood pressure, we make this assumption in order to investigate how the shape of the population may affect which estimator works best.\nConsider the sampling distribution of estimates obtained from many different random samples (each of size \\(n=20\\)) chosen independently from a uniformly distributed population. Based on the sampling distributions for \\(\\hat{\\mu}_1\\), \\(\\hat{\\mu}_2\\), \\(\\hat{\\mu}_3\\), and \\(\\hat{\\mu}_4\\) (mean, median, mid-range, and trimmed mean, respectively) in Figure 16.2, rank the four estimators again from least to most bias and from most to least precise. Compare your updated rankings to those from Question 2b. Did your rankings change?\n\n\n\n\n\n\n\n(a) Distribution of Sample Means\n\n\n\n\n\n\n\n(b) Distribution of Sample Medians\n\n\n\n\n\n\n\n\n\n(c) Distribution of Sample Mid-ranges\n\n\n\n\n\n\n\n(d) Distribution of Sample Trimmed Means\n\n\n\n\nFigure 16.2: Comparing Estimators for a Uniform Population\n\n\n\nSolution to Question 2c"
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-prop-bias",
    "href": "15-Properties-Estimators.html#sec-prop-bias",
    "title": "4.3: Properties of Estimators",
    "section": "Question 3",
    "text": "Question 3\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown. Consider the following two estimators for parameter \\(p\\):\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\). This is equivalent to adding 4 more trials to the sample, 2 of which are successes.\n\n\n\n\n\n\n\nTip\n\n\n\nUse properties of expected value and recall these useful formulas for \\(X \\sim \\mbox{Binom}(n,p)\\),\n\\[E(X) = np \\quad \\mbox{and} \\quad \\mbox{Var}(X) = np(1-p).\\]\n\n\n\nQuestion 3a\n\nDetermine whether the estimator \\(\\hat{p} = \\frac{X}{n}\\) is biased or unbiased.\n\nSolution to Question 3a\n\n\n\n\n\n\n\nQuestion 3b\n\nDetermine whether the estimator \\(\\tilde{p} = \\frac{X+2}{n+4}\\) is biased or unbiased.\n\nSolution to Question 3b"
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-var-bias",
    "href": "15-Properties-Estimators.html#sec-var-bias",
    "title": "4.3: Properties of Estimators",
    "section": "Question 4",
    "text": "Question 4\n\nLet \\(X \\sim N(\\mu, \\sigma)\\). In Question 2, we consider several estimators for the parameter \\(\\mu\\). We now consider two possible estimators for the parameter \\(\\sigma^2\\), the variance of the population.\n\nUsing the estimator \\(\\displaystyle s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}\\).\nUsing the estimator \\(\\displaystyle \\hat{\\sigma}^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}\\).\n\n\nQuestion 4a\n\nProve the following statement:\nIf \\(X_1\\), \\(X_2\\), \\(\\ldots\\) , \\(X_n\\) are independently and identically distributed random variables with \\(E(X_i) = \\mu\\) and \\(\\mbox{Var}(X_i) = \\sigma^2\\), then\n\\[{\\color{dodgerblue}{\\boxed{E \\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] = (n-1)\\sigma^2.}}}\\]\n\n\n\n\n\n\nTip\n\n\n\nUse the result of Theorem 15.1 that states the following:\nIf \\(X_1\\), \\(X_2\\), \\(\\ldots\\) , \\(X_n\\) are independently and identically distributed random variables with \\(\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\), then\n\\[\\boxed{ E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n  E \\big[ X_i^2 \\big] - n E \\big[ \\overline{X}^2 \\big]}\\]\n\n\n\nSolution to Question 4a\n\nProof:\nWe first apply Theorem 15.1 to begin simplifying the expected value of the sum of the squared deviations,\n\\[E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}}\\]\nNext we simplify using properties of random variables and summations as follows,\n\\[\\begin{aligned}\nE\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] &= \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}} & \\mbox{by Theorem 15.1}\\\\\n&=  \\sum_{i=1}^n \\bigg( {\\color{dodgerblue}{ \\mbox{Var} \\big[ X_i \\big] + \\left( E \\big[ X_i \\big] \\right)^2 }} \\bigg) - n \\left( {\\color{tomato}{\\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2}} \\right) & \\mbox{Justification 1 ??}\\\\\n&=  \\sum_{i=1}^n {\\color{dodgerblue}{ \\left( \\sigma^2 + \\mu^2 \\right)}} - n \\left( \\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2 \\right) & \\mbox{Justification 2 ??}\\\\\n&=  \\sum_{i=1}^n\\left( \\sigma^2 + \\mu^2 \\right) - n \\left( {\\color{tomato}{\\frac{\\sigma^2}{n}}} + \\left( {\\color{tomato}{ \\mu }}\\right)^2 \\right) & \\mbox{Justification 3 ??} \\\\\n&= {\\color{dodgerblue}{n(\\sigma^2 + \\mu^2)}} - \\sigma^2 - n\\mu^2 & \\mbox{Justification 4 ??}\\\\\n&= (n-1) \\sigma^2. & \\mbox{Algebraically simplify}\n\\end{aligned}\\]\nThis concludes our proof!\n\nJustifications for Proof\n\n\nJustification 1:\nJustification 2:\nJustification 3:\nJustification 4:\n\n\n\n\n\n\n\nQuestion 4b\n\nDetermine whether the estimator \\(\\displaystyle s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}\\) is biased or unbiased.\n\n\n\n\n\n\nTip\n\n\n\nIf we apply the theorem we proved in Question 4a, this question should not require much more additional work!\n\n\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nDetermine whether the estimator \\(\\displaystyle \\hat{\\sigma}^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}\\) is biased or unbiased.\n\nSolution to Question 4c"
  },
  {
    "objectID": "15-Properties-Estimators.html#estimating-variance-and-standard-deviation",
    "href": "15-Properties-Estimators.html#estimating-variance-and-standard-deviation",
    "title": "4.3: Properties of Estimators",
    "section": "Estimating Variance and Standard Deviation",
    "text": "Estimating Variance and Standard Deviation\n\nThe variance of random variable \\(X\\) is defined as \\(\\sigma^2=\\mbox{Var} (X) = E\\big[ (X- \\mu)^2 \\big]\\). If we pick a random sample \\(X_1, X_, \\ldots , X_n\\) and want to approximate \\(\\sigma^2\\), then a reasonable recipe for estimating \\(\\sigma^2\\) could be to approximate \\(E \\big[ (X - \\mu)^2 \\big]\\) using the following process:\n\nUse the sample mean \\({\\color{tomato}{\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i}}\\) in place of the unknown value of the parameter \\({\\color{tomato}{\\mu}}\\).\nBased on the sample data, calculate the average value of \\((X_i - {\\color{tomato}{\\overline{X}}})^2\\).\n\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n \\left( X_i- \\overline{X} \\right)^2}{n}.\\]\nHowever, in Question 4 we showed this estimator is biased. For this reason:\n\nThe unbiased estimator \\(s^2\\) (that has \\(n-1\\) in the denominator) is usually used to estimate the population variance \\(\\sigma^2\\).\n\n\\[s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}.\\]\n\nAnd the estimator \\(s\\) (that also has \\(n-1\\) in the denominator) is usually used to estimate the population standard deviation \\(\\sigma\\).\n\n\\[s = \\sqrt{ \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}}.\\]\n\nThe R commands sd(x) and var(x) use the formulas for \\(s\\) and \\(s^2\\), respectively, with \\(n-1\\) in the denominator.\nFor large samples, the difference is very minimal whether we use the estimator with \\(n\\) or \\(n-1\\) in the denominator.\n\n\n\n\n\n\n\nWarning\n\n\n\nAlthough the sample variance \\(s^2\\) is an unbiased estimator for the population variance \\(\\sigma^2\\), the sample standard deviation \\(s\\) is in general a biased estimator for the population variance \\(\\sigma\\) since it is not true that \\(\\sqrt{E(X)} = E(\\sqrt{X})\\)."
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-mean-eff",
    "href": "15-Properties-Estimators.html#sec-mean-eff",
    "title": "4.3: Properties of Estimators",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(X_1, X_2, X_3\\) be independent random variables from an identical distribution with mean and variance \\(\\mu\\) and \\(\\sigma^2\\), respectively, and consider two possible estimators for \\(\\mu\\):\n\nThe usual sample mean, \\(\\hat{\\mu}_1 = \\overline{X} = \\frac{X_1 + X_2 + X_3}{3}\\).\nA weighted sample mean, \\(\\hat{\\mu}_2 = \\frac{1}{6}X_1 + \\frac{1}{3}X_2 + \\frac{1}{2} X_3\\).\n\n\nQuestion 5a\n\nProve both estimators \\(\\hat{\\mu}_1\\) and \\(\\hat{\\mu}_2\\) are unbiased estimators of \\(\\mu\\).\n\nSolution to Question 5a\n\n\n\n\n\n\n\nQuestion 5b\n\nCalculate \\(\\mbox{Var}( \\hat{\\mu}_1)\\) and \\(\\mbox{Var}( \\hat{\\mu}_2)\\), the variances of the estimators \\(\\hat{\\mu}_1\\) and \\(\\hat{\\mu}_2\\). Which estimator is more precise?\n\n\n\n\n\n\nTip\n\n\n\nRecall properties we can apply when finding the variance of a linear combination of independent random variables, and note the variances will depend on the unknown value of the population variance, \\(\\mathbf{\\sigma^2}\\).\n\n\n\nSolution to Question 5b"
  },
  {
    "objectID": "15-Properties-Estimators.html#efficiency-of-unbiased-estimators",
    "href": "15-Properties-Estimators.html#efficiency-of-unbiased-estimators",
    "title": "4.3: Properties of Estimators",
    "section": "Efficiency of Unbiased Estimators",
    "text": "Efficiency of Unbiased Estimators\n\nIf \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) are both unbiased estimators of \\(\\theta\\), then \\(\\hat{\\theta}_1\\) is said to be more efficient than \\(\\hat{\\theta}_2\\) if \\({\\color{dodgerblue}{\\mbox{Var} ( \\hat{\\theta}_1) &lt; \\mbox{Var} (\\hat{\\theta}_2)}}\\). For example, in Question 5 we show the usual sample mean \\(\\hat{\\mu}_1=\\overline{X}\\) is a more efficient estimator than the weighted mean \\(\\hat{\\mu}_2\\)."
  },
  {
    "objectID": "15-Properties-Estimators.html#question-6",
    "href": "15-Properties-Estimators.html#question-6",
    "title": "4.3: Properties of Estimators",
    "section": "Question 6",
    "text": "Question 6\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown. Consider the following two estimators for parameter \\(p\\):\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\).\n\nRecall in Question 3 we determined \\(\\hat{p}\\) is an unbiased estimator for \\(p\\) while \\(\\tilde{p}\\) is a slightly biased estimator.\n\n\n\n\n\n\nTip\n\n\n\nUse properties of variance and recall these useful formulas for \\(X \\sim \\mbox{Binom}(n,p)\\),\n\\[E(X) = np \\quad \\mbox{and} \\quad \\mbox{Var}(X) = np(1-p).\\]\n\n\n\nQuestion 6a\n\nFind \\(\\mbox{Var}(\\hat{p}) = \\mbox{Var} \\left( \\frac{X}{n} \\right)\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 6a\n\n\n\n\n\n\n\nQuestion 6b\n\nFind \\(\\mbox{Var}(\\tilde{p}) = \\mbox{Var} \\left( \\frac{X+2}{n+4} \\right)\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 6b"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-7",
    "href": "15-Properties-Estimators.html#question-7",
    "title": "4.3: Properties of Estimators",
    "section": "Question 7",
    "text": "Question 7\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown. Consider the following two estimators for parameter \\(p\\):\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\).\n\nUsing previous results regarding the bias and variability of the estimators we derived in Question 3 and Question 6, respectively, answer Question 7a and Question 7b to compare the MSE of the estimators.\n\nQuestion 7a\n\nGive a formula for \\(\\mbox{MSE}(\\hat{p})\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 7a\n\n\n\n\n\n\n\nQuestion 7b\n\nGive a formula for \\(\\mbox{MSE}(\\tilde{p})\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 7b"
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-compare-prop",
    "href": "15-Properties-Estimators.html#sec-compare-prop",
    "title": "4.3: Properties of Estimators",
    "section": "Choosing a Sample Proportion",
    "text": "Choosing a Sample Proportion\n\nIn the case of \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown, we have considered two possible estimators for the population proportion \\(p\\).\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\n\nThis estimator makes the most practical sense.\nThis is the estimator obtained using MLE or MoM.\n\\(\\hat{p}\\) is unbiased.\nBut \\(\\hat{p}\\) can be less precise depending on the value of \\(p\\).\n\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\).\n\n\\(\\tilde{p}\\) is biased, but this may not be a problem:\n\nAs \\(n\\) gets larger and larger, the bias of this estimator gets smaller and smaller.\nThe bias is towards \\(0.5\\), so if \\(p\\) is close to \\(0.5\\) this is not a big issue.\n\n\\(\\tilde{p}\\) is a more precise estimator when \\(p\\) is not close to 0 or 1.\n\n\nFor example if \\(n=16\\), we have \\(X \\sim \\mbox{Binom}(16,p)\\). Figure 19.1 compares the values of \\(\\mbox{MSE}(\\hat{p})\\) and \\(\\mbox{MSE}(\\tilde{p})\\) for \\(n=16\\).\n\np &lt;- seq(0, 1, length.out = 100)  # values of p\nn &lt;- 16  # sample size\n\n# formula for MSE of p-hat\nmse.phat &lt;- (p * (1 - p)) / n\n\n# formula for MSE of p-tilde\nmse.ptilde &lt;- (n * p * (1 - p))/(n + 2)^2 + (1 - 2*p)^2/(n + 2)^2\n\n# plot of MSE(p-hat)\nplot(p, mse.phat,  \n     type = \"l\",\n     lwd =2,\n     col = \"firebrick2\",\n     main = \"Comparing MSE of Estimators for p when n=16\",\n     ylab = \"MSE\")\n\n# add plot of MSE(p-tilde)\nlines(p, mse.ptilde,  \n      lty=2, \n      lwd =2,\n      col = \"blue\")\n\n# add legend to plot\nlegend(0.37, 0.005, \n       legend=c(\"MSE(p.hat)\",\"MSE(p.tilde\"), \n       col=c(\"firebrick2\",\"blue\"), \n       lty=c(1,2), \n       ncol=1)\n\n\n\n\nFigure 19.1: Comparing MSE of p-hat and p-tilde for X ~ Binom(16,p)"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-8",
    "href": "15-Properties-Estimators.html#question-8",
    "title": "4.3: Properties of Estimators",
    "section": "Question 8",
    "text": "Question 8\n\nUsing the plots of \\({\\color{\\tomato}{\\mbox{MSE}(\\hat{p})}}\\) and \\({\\color{dodgerblue}{\\mbox{MSE}(\\tilde{p})}}\\) in Figure 19.1, identify the interval of \\(p\\) values where the MSE of \\(\\tilde{p}\\) is less than the MSE of \\(\\hat{p}\\).\n\nSolution to Question 8"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-9",
    "href": "15-Properties-Estimators.html#question-9",
    "title": "4.3: Properties of Estimators",
    "section": "Question 9",
    "text": "Question 9\n\nRun run the code below for different values of \\(n\\) and say what happens to your choice of estimator as \\(n\\) gets larger.\n\n#########################################\n# adjust sample size n and run again\n# what happens to mse as n gets larger?\n#########################################\nn &lt;- ??  # sample size\n\n####################################################\n# you do not need to edit the rest of the code cell\n####################################################\n\np &lt;- seq(0, 1, length.out = 100)  # values of p\n\n# formula for MSE of p-hat\nmse.phat &lt;- (p * (1 - p)) / n\n\n# formula for MSE of p-tilde\nmse.ptilde &lt;- (n * p * (1 - p))/(n + 2)^2 + (1 - 2*p)^2/(n + 2)^2\n\n# plot of MSE(p-hat)\nplot(p, mse.phat,  \n     type = \"l\",\n     lwd =2,\n     col = \"firebrick2\",\n     main = \"Comparing MSE of Estimators for p\",\n     ylab = \"MSE\")\n\n# add plot of MSE(p-tilde)\nlines(p, mse.ptilde,  \n      lty=2, \n      lwd =2,\n      col = \"blue\")\n\n# add legend to plot\nlegend(0.37, 0.005, \n       legend=c(\"MSE(p.hat)\",\"MSE(p.tilde\"), \n       col=c(\"firebrick2\",\"blue\"), \n       lty=c(1,2), \n       ncol=1)\n\n\nSolution to Question 9\n\n\nInterpret the plots generated by the code above and answer the question."
  },
  {
    "objectID": "15-Properties-Estimators.html#footnotes",
    "href": "15-Properties-Estimators.html#footnotes",
    "title": "4.3: Properties of Estimators",
    "section": "",
    "text": "Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. and Johannes, R. S. (1988) “Using the ADAP learning algorithm to forecast the onset of diabetes mellitus”. In Proceedings of the Symposium on Computer Applications in Medical Care (Washington, 1988), ed. R. A. Greenes, pp. 261–265. Los Alamitos, CA: IEEE Computer Society Press.↩︎"
  },
  {
    "objectID": "16-Bootstrap-Dist.html",
    "href": "16-Bootstrap-Dist.html",
    "title": "5.1: Bootstrap Distributions",
    "section": "",
    "text": "Sampling From a Population\nRarely, we have access to data from the entire population of interest, in which case we are able to calculate the actual value(s) of population parameter(s). We can generate a sampling distribution by simulating the selection of many different random samples from the population data, and we can compute the standard error of the sampling distribution to measure how much uncertainty we can expect due to the randomness of sampling. If we have access to data from the entire population, there is no need for statistics to estimate parameters since we know the values of the parameters! Recall the distinction and connection and between parameters and statistics:\nIn some situations, we have a known probability distribution that we can use to build a model and make predictions. For example, characteristics such as height (normal), time between successive events (exponential), and counting the number of times an event occurs over an interval of time all behave predictably (Poisson). We can pick random sample and use point estimators such as MLE and MoM to estimate unknown population parameters. What happens if the data does not follow a known distribution?\nSuppose we would like to estimate the value of a parameter for a population about which we know very little information (this is often the case). We collect data from a single random sample of size \\(n\\), and then we can use statistics from the sample to make predictions about the population:\nIn any of these cases, how certain can/should we be in our estimate? In practice, we do not repeatedly pick 1000’s of random samples from the population. That is likely impractical, expensive, and time consuming. We have only collected data from a single random sample.\nThe data frame1 jackal in the permute package contains a sample of \\(n=20\\) mandibles from male and female golden jackals. For each of the 20 observations, two variables are recorded:\nWe have explored the sampling distributions of sample means, proportions, medians, variances and other estimators as a tool to assess the variability in those statistics and measure the level of uncertainty or precision in the estimate we obtain from the sample. In particular, the variance of a sampling distribution or the standard error (which is the square root of the variance of a sampling distribution) are commonly used to assess the variability in sample statistics.\nIn the case of the mean mandible length of all golden jackals, we have collected one sample of \\(n=20\\) adult golden jackals. We do not have access to data from the entire population, so we cannot construct a sampling distribution by picking many different random samples each size \\(n=20\\). Collecting unbiased samples can be quite expensive, time-consuming, and logistically difficult. If we only have one sample and know very little about the population, how can we generate a sampling distribution from this limited information?\nMonte Carlo methods are computational algorithms that rely on repeated random sampling. A bootstrap distribution is one example of a Monte Carlo method. A bootstrap distribution theoretically would contain the sample statistics from all possible bootstrap resamples. If we pick an initial sample size \\(n\\), then there exists a total of \\(n^n\\) possible bootstrap resamples. In the case of \\(n=20\\), we have \\(20^{20} \\approx 1.049 \\times 10^{26}\\) possible resamples. If we ignore the ordering in which we pick the sample, when \\(n=20\\), we have a total of \\(68,\\!923,\\!264,\\!410\\) (almost 69 billion!) distinct bootstrap resamples.\nFor small samples, we could write out all possible bootstrap resamples. For larger values of \\(n\\) (and we see \\(n=20\\) is already extremely large), it is really not practical or feasible to generate all possible bootstrap resamples while avoiding duplicates. Instead, we use Monte Carlo methods to repeatedly pick random samples that we use to approximate a sampling distribution. The Monte Carlo method of generating many (but necessarily all) bootstrap resamples introduces additional uncertainty and variability into the analysis. The more bootstrap resamples we choose, the less uncertainty we have.\nMonte Carlo methods were first explored by the Polish mathematician Stanislow Ulam in the 1940s while working on the initial development of nuclear weapons at Los Alamos National Lab in New Mexico. The research required evaluating extremely challenging integrals. Ulam devised a numerical algorithm based on resampling to approximate the integrals. The method was later named “Monte Carlo”, a gambling region in Monaco, due to the randomness involved in the computations.\nRecall if \\({\\color{tomato}{\\widehat{\\theta}}}\\) is an estimator for the parameter \\({\\color{mediumseagreen}{\\theta}}\\), then we define the bias of an estimator as\n\\[{\\large \\color{dodgerblue}{ \\boxed{\\mbox{Bias}(\\widehat{\\theta}) = {\\color{tomato}{\\widehat{\\theta}}} - {\\color{mediumseagreen}{\\theta}}}.}}\\]\nIn the case of bootstrapping:\n\\[{\\large \\color{dodgerblue}{ \\boxed{\\mbox{Bias}_{\\rm{boot}} \\big( \\hat{\\mu}_{\\rm{boot}} \\big) = {\\color{tomato}{\\hat{\\mu}_{\\rm{boot}}}} - {\\color{mediumseagreen}{\\bar{x}}}.}}}\\]\nLet \\(X\\) be the mandible length (in mm) of a randomly selected adult golden jackal. Based on the sample data in jaw.sample, we could come up with unbiased estimates for the population mean and population variance using:\nWe now have an estimate for the population, namely \\(X \\sim N(111, 3.88)\\). We can apply the Central Limit Theorem (CLT) for Means to construct another estimate for the sampling distribution. Although our sample size is relatively small (\\(n=20 &lt; 30\\)), we can apply CLT in this situation since the population is assumed to be symmetric (normally distributed).\nConsider the theoretical population \\(X \\sim N(23,7)\\). Below we compare the sampling distribution for the mean obtained using the central limit theorem on the top row with one random sample and a corresponding bootstrap distribution for the sample mean on the bottom row.\n(a) Distribution of Population\n\n\n\n\n\n\n\n(b) Sampling Distribution using CLT\n\n\n\n\n\n\n\n\n\n(c) Distribution of Sample\n\n\n\n\n\n\n\n(d) Bootstrap Dist Approximation of the Sampling Dist\n\n\n\n\nFigure 23.1: Comparing Bootstrap Approximation of Sampling Distribution to CLT"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-1",
    "href": "16-Bootstrap-Dist.html#question-1",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 1",
    "text": "Question 1\n\n\n\n\n\n\n\n\nCredit: Вых Пыхманн, CC BY-SA 3.0, Wikimedia Commons\n\n\n\n\n\n\n\nMariomassone, CC BY-SA 4.0, Wikimedia Commons\n\n\n\n\nFigure 17.1: Left: Golden Jackal (Canis aureus) Right: Mandible bones of wolf and jackal\n\n\nA zoologist would like to answer the following question?\n\nWhat is the average mandible (jaw) length of all golden jackals (Canis aureus)?\n\nDevise a method for collecting and analyzing data to help them answer this question.\n\nSolution to Question 1"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#loading-the-data",
    "href": "16-Bootstrap-Dist.html#loading-the-data",
    "title": "5.1: Bootstrap Distributions",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nIt is very likely you do not have the package permute installed. You will need to first install the permute package.\n\nGo to the R console window.\nRun the command &gt; install.packages(\"permute\").\n\nYou will only need to run the install.package() command one time. You can now access permute anytime you like! However, you will need to run the command library(permute) during any R session in which you want to access data from the permute package. Be sure you have first installed the permute package before executing the code cell below.\n\n# be sure you have already installed the permute package\nlibrary(permute)  # loading permute package\n\n\nSummarizing and Storing the Data\n\nIn the code cell below we load the jackal data from the permute package and provide a numerical summary of the two variables in the sample.\n\ndata(jackal)  # load jackal data\nsummary(jackal)  # numerical summary of each variable\n\n     Length          Sex    \n Min.   :105.0   Male  :10  \n 1st Qu.:107.8   Female:10  \n Median :111.0              \n Mean   :111.0              \n 3rd Qu.:113.2              \n Max.   :120.0              \n\n\nThe code cell below displays the distribution of mandible lengths separately for males and females.\n\n# side by side box plots\nplot(Length ~ Sex, data = jackal, \n     col = c(\"dodgerblue\", \"mediumseagreen\"),\n     main = \"Mandible Length of Golden Jackals\",\n     ylab = \"Length (in mm)\")  \n\n\n\n\nWe will be analyzing mandible lengths for both adult male and female golden jackals. In the code cell below, we save the \\(n=20\\) mandible lengths to a vector called jaw.sample.\n\njaw.sample &lt;- jackal$Length  # store mandible lengths to vector\njaw.sample  # print sample to screen\n\n [1] 120 107 110 116 114 111 113 117 114 112 110 111 107 108 110 105 107 106 111\n[20] 111"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-2",
    "href": "16-Bootstrap-Dist.html#question-2",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 2",
    "text": "Question 2\n\nBased on the sample above, what is your estimate for \\(\\mu\\), the mean mandible length of all adult golden jackals?\n\nSolution to Question 2\n\n\n# use jaw.sample to estimate population mean"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-3",
    "href": "16-Bootstrap-Dist.html#question-3",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 3",
    "text": "Question 3\n\nHow much confidence do you have in your estimate in Question 2? Any suggestions on how we can measure the uncertainty in our estimate due to the randomness of sampling?\n\nSolution to Question 3"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#what-is-a-statistical-question",
    "href": "16-Bootstrap-Dist.html#what-is-a-statistical-question",
    "title": "5.1: Bootstrap Distributions",
    "section": "What is a Statistical Question?",
    "text": "What is a Statistical Question?\n\nA statistical question is one that can be answered by collecting data and where there will be variability in that data.\n\nBased on a random sample of \\(n=20\\) adult golden jackals, what is the mean mandible length of all adult golden jackals?\n\n\nEach time we pick a different sample we have a different subset of data.\nDifferent samples have different sample means, leading to different estimates.\nThis is a statistical question!\nHow can we account for this variability in our estimate?\n\n\nUsing a database that contains information on all registered voters in Colorado, what proportion of all Colorado voters are over 50 years old?\n\n\nThe database includes information from the population of all registered voters in Colorado.\nWe can use the population data to calculate the proportion.\nThe population data does not change, so there is no variability in the value of the proportion.\nThis is not an example of a statistical question."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#bootstrap-distributions",
    "href": "16-Bootstrap-Dist.html#bootstrap-distributions",
    "title": "5.1: Bootstrap Distributions",
    "section": "Bootstrap Distributions",
    "text": "Bootstrap Distributions\n\nBootstrapping is the process of generating many different random samples from one random sample to obtain an estimate for a population parameter. For each randomly selected resample, we calculate a statistic of interest. Then we construct a new distribution of bootstrap statistics that approximates a sampling distribution for some sample statistic (such as a mean, proportion, variance, and others). We can use bootstrapping with any sample, even small ones. We can bootstrap any statistic. Thus, bootstrapping provides a robust method for performing statistical inference that we can adapt to many different situations in statistics and data science.\n\nA Bootstrapping Algorithm\n\nGiven an original sample of size \\(n\\) from a population:\n\nDraw a bootstrap resample of the same size, \\(n\\), with replacement from the original sample.\nCompute the relevant statistic (mean, proportion, max, variance, etc) of that sample.\nRepeat this many times (say \\(100,\\!000\\) times).\n\n\nA distribution of statistics from the bootstrap samples is called a bootstrap distribution.\nA bootstrap distribution gives an approximation for the sampling distribution.\nWe can inspect the center, spread and shape of the bootstrap distribution and do statistical inference."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-4",
    "href": "16-Bootstrap-Dist.html#question-4",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 4",
    "text": "Question 4\n\nConsider a random sample of 4 golden jackal mandible lengths (in mm):\n\\[120, 107, 110, \\mbox{ and } 116.\\]\nWhich of the following could be a possible bootstrap resample? Explain why or why not.\n\nQuestion 4a\n\n120, 107, 116\n\nSolution to Question 4a\n\n\n\n\n\n\n\nQuestion 4b\n\n110, 110, 110, 110\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\n120, 107, 110, 116\n\nSolution to Question 4c\n\n\n\n\n\n\n\nQuestion 4d\n\n120, 107, 110, 116, 120\n\nSolution to Question 4d\n\n\n\n\n\n\n\nQuestion 4e\n\n110, 130, 120, 107\n\nSolution to Question 4e"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-5",
    "href": "16-Bootstrap-Dist.html#question-5",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 5",
    "text": "Question 5\n\nHow many possible bootstrap resamples can be constructed from an original sample that has \\(n=20\\) values?\n\nSolution to Question 5\n\n\n# How many possible resamples are there for n=20?"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#sec-jawboot",
    "href": "16-Bootstrap-Dist.html#sec-jawboot",
    "title": "5.1: Bootstrap Distributions",
    "section": "Creating a Bootstrap Distribution in R",
    "text": "Creating a Bootstrap Distribution in R\n\nLet’s return to our statistical question:\n\nWhat is the average mandible (jaw) length of all golden jackals?\n\nWe have already picked one random sample of \\(n=20\\) adult golden jackals. The mandible lengths of our sample are stored in the vector jaw.sample.\n\nStep 1: Pick a Bootstrap Resample\n\nWe use the sample() function in R to pick a random sample of values out of the values in jaw.sample.\n\nNotice the resample has size \\(n=20\\), the same as the original sample.\nWe use the option replace = TRUE since we want to sample with replacement.\nRunning the code cell below creates one bootstrap resample stored in temp.samp.\n\n\ntemp.samp &lt;- sample(jaw.sample, size=20, replace = TRUE)  # sample with replacement\ntemp.samp  # print sample to screen\n\n [1] 111 107 111 110 107 116 107 117 112 111 107 117 107 113 116 107 110 112 120\n[20] 116\n\n\n\n\nStep 2: Calculate Statistic(s) from the Bootstrap Sample\n\nIn the golden jackal mandible length example, we want to use information about the distribution of sample means to estimate a population mean. Thus, we calculate the mean of the bootstrap resample temp.samp that we picked in the previous code cell.\n\nmean(temp.samp)  # mean of bootstrap resample\n\n[1] 111.7\n\n\n\n\nStep 3: Repeat Over and Over Again\n\nIn the code cell below, we repeat steps 1 and 2 over and over again. The sample means we calculate from each bootstrap resample are stored in a vector named boot.dist. Run the code cell below to generate a bootstrap distribution for the sample mean.\n\nA solid red line marks the location of the sample mean from the original sample.\nA dashed blue line marks the location of the mean of the bootstrap distribution.\nA solid green line marks the location of another published estimate for the population mean2.\n\n\n##########################\n# cell is ready to run\n# no need for edits\n##########################\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dist &lt;- numeric(N)  # create vector to store bootstrap means\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(jaw.sample, 20, replace = TRUE)  # pick a bootstrap resample\n  boot.dist[i] &lt;- mean(x)  # compute mean of bootstrap resample\n}\n\n# plot bootstrap distribution\nhist(boot.dist,  \n     breaks=20, \n     xlab = \"x-bar, mandible length (in mm)\",\n     main = \"Bootstrap Distribution for Sample Mean (n=20)\")\n\n# red line at the observed sample mean\nabline(v = mean(jaw.sample), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = mean(boot.dist), col = \"blue\", lwd = 2, lty = 2)\n\n# green line at the population mean, 112 mm\nabline(v = 112, col = \"mediumseagreen\", lwd = 2, lty = 1)"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-6",
    "href": "16-Bootstrap-Dist.html#question-6",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 6",
    "text": "Question 6\n\nWhat are the mean and standard error of the bootstrap distribution? Use the code below to compute each value.\n\n# calculate center of bootstrap dist\n\n\n# calculate bootstrap standard error\n\n\nSolution to Question 6"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-7",
    "href": "16-Bootstrap-Dist.html#question-7",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 7",
    "text": "Question 7\n\nCompute the bootstrap estimate of bias if we use the mean of the bootstrap distribution from Question 6 as our estimate for the mean mandible length of all adult golden jackals.\n\nSolution to Question 7"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-8",
    "href": "16-Bootstrap-Dist.html#question-8",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 8",
    "text": "Question 8\n\nWhat common distribution do you believe is the best model for mandible lengths of all golden jackals? Explain your reasoning.\n\nSolution to Question 8"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-9",
    "href": "16-Bootstrap-Dist.html#question-9",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 9",
    "text": "Question 9\n\nUsing the CLT with the population model \\(X \\sim N(111, 3.88)\\), we can derive a theoretical model for the distribution of sample means for \\(n=20\\). Using the CLT for means, give the mean and standard error for the sampling distribution for \\(\\overline{X}\\). How do your answers compare to approximations you found in Question 6 using the bootstrap distribution to estimate the sampling distribution for sample means?\n\nSolution to Question 9"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-10",
    "href": "16-Bootstrap-Dist.html#question-10",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 10",
    "text": "Question 10\n\n\nCompare the population and sample distributions. What is similar about the two distributions? What are the differences?\n\n\n\nCompare the CLT sampling distribution and bootstrap sampling distribution. What is similar about the two distributions? What are the differences?\n\n\nSolution to Question 10\n\nSolution to part a:\n\n\nSolution to part b:"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#sec-plugin",
    "href": "16-Bootstrap-Dist.html#sec-plugin",
    "title": "5.1: Bootstrap Distributions",
    "section": "The Plug-in Principle",
    "text": "The Plug-in Principle\n\nThe Plug-in Principle: If something (such as a characteristic of a population) is unknown, substitute (plug-in) an estimate. For example, if we do not know the population mean \\(\\mu\\), the sample mean \\(\\bar{x}\\) is a nice, unbiased substitute. If a population standard deviation \\(\\sigma\\) is unknown, we can use substitute the sample standard deviation, \\(s\\).\n\nBootstrapping is an extreme application of this principle.\nWe replace the entire population (not just one parameter) by the entire set of data from the sample.\n\nEach bootstrap resample is picked from the same “population”, the original sample, to generate a bootstrap distribution that can be used to estimate a sampling distribution constructed from the entire population."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-11-arsenic-case-study",
    "href": "16-Bootstrap-Dist.html#question-11-arsenic-case-study",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 11: Arsenic Case Study",
    "text": "Question 11: Arsenic Case Study\n\nArsenic is a naturally occurring element in the groundwater in Bangladesh. Much of this water is used for drinking in rural areas, so arsenic poisoning is a serious health issue. The data set Bangladesh in the resampledata package3 contains measurements on arsenic, chlorine, and cobalt levels (in parts per billion, ppb) present in each of 271 groundwater samples.\n\nLoading the Data\n\nIt is very likely you do not have the package resampledata installed. You will need to first install the resampledata package.\n\nGo to the R console.\nRun the command &gt; install.packages(\"resampledata\").\n\nYou will only need to run the install.package() command one time. You can now access resampledata anytime you like! However, you will need to run the command library(resampledata) during any R session in which you want to access data from the resampledata package. Be sure you have first installed the resampledata package before executing the code cell below.\n\n# be sure you have already installed the resampledata package\nlibrary(resampledata)  # loading resampledata package\n\n\n\nQuestion 11a\n\nComplete the code cell below to calculate the mean and standard deviation and size of the arsenic level of the sample.\n\nSolution to Question 11a\n\n\n# be sure you have already installed the resampledata package\narsenic &lt;- Bangladesh$Arsenic  # store arsenic data in vector\nn.arsenic &lt;- length(arsenic)  # how many observations in arsenic\n\n################################\n# complete each command below\n################################\nmean.arsenic &lt;- ??  # sample mean\nsd.arsenic &lt;- ??  # sample standard deviation\nmean.arsenic  # print result to screen\nsd.arsenic  # print result to screen\n\n\n\n\n\n\nQuestion 11b\n\nCreate a histogram to show the shape of the distribution of the sample data. How would you describe the shape?\n\nSolution to Question 11b\n\n\n# create a histogram of the sample arsenic data\nhist(??)\n\n\n\n\n\n\nQuestion 11c\n\nComplete the code cell below to generate a bootstrap distribution for the sample mean. What are the mean and standard error of the bootstrap distribution?\n\nSolution to Question 11c\n\nReplace all eight ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution.\n\nN &lt;- 10^5  # number of bootstrap samples\nboot.arsenic &lt;- numeric(N)  # create vector to store bootstrap means\n\n# Set up a for loop!\n\nfor (i in 1:N)\n{\n  x &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.arsenic[i] &lt;- ??  # calculate relevant sample statistic\n}\n\nboot.mean &lt;- mean(??)  # calculate center of bootstrap dist\nboot.se &lt;- sd(??)  # calculate bootstrap standard error\n\n# plot bootstrap distribution\nhist(boot.arsenic,  xlab = \"xbar\",\n     main = \"Bootstrap Distribution\")\n\n# add a red line at the observed sample mean\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# add a blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n# print bootstrap estimate and standard error to screen\nboot.mean  # mean (center) of bootstrap dist\nboot.se  # standard error (spread) of bootstrap dist\n\n\n# compare bootstrap dist to normal dist\n# run to create a qq-plot\nqqnorm(boot.dist)\nqqline(boot.dist)\n\n\n\n\n\n\nQuestion 11d\n\nCalculate the bootstrap estimate for bias.\n\nSolution to Question 11d\n\n\n# calculate bootstrap estimate of bias\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#footnotes",
    "href": "16-Bootstrap-Dist.html#footnotes",
    "title": "5.1: Bootstrap Distributions",
    "section": "",
    "text": "Manly, B.F.J. (2007) Randomization, bootstrap and Monte Carlo methods in biology. Third Edition. Chapman & Hall/CRC, Boca Raton.↩︎\nAli Louei Monfared, “Macro-Anatomical Investigation of the Skull of Golden Jackal (Canis aureus) and its Clinical Application during Regional Anesthesia”, Global Veterinaria 10 (5): 547-550, 2013.↩︎\nLaura M. Chihara and Tim C. Hesterberg (2019) Mathematical Statistics with Resampling and R. Second Edition. John Wiley & Sons, Hoboken, NJ.↩︎"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html",
    "href": "17-Bootstrap-Confidence-Int.html",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "",
    "text": "Case Study: Ozone Concentration in New York City\nAccoording to the Unites States Environmental Protection Agency (EPA)1:\nWhen the ozone concentration is greater, respiratory illnesses such as asthma, pneumonia, and bronchitis can become exacerbated. While the effects of short-term exposure to high ozone concentration are reversible, long-term exposure may not be reversible. The EPA sets Ozone National Ambient Air Quality Standards (NAAQS)2. “The existing primary and secondary standards, established in 2015, are 0.070 parts per million (ppm)”, or equivalently 70 parts per billion (pbb).\nWe will begin todays work with bootstrap distributions investigating the following question:\nThe sample mean for ozone concentration is below the 70 ppb limit. However, from the plot in Question 1a, we see there are number of observations in the sample with an ozone concentration over 70 ppb.\nsum(nyc.oz &gt; 70)\n\n[1] 24\nThe sample proportion of observations with an ozone concentration greater than 70 pbb is \\(24/111 \\approx 0.2162\\). We calculate the sample proportion in two different ways below. In both methods, we use the logical test nyc.oz &gt; 70 to help count the number of observations greater than 70 pbb.\n# method 1\nn &lt;- length(nyc.oz)  # number of observations in sample\nsum(nyc.oz &gt; 70) / n  # sample proportion over 70 ppb\n\n[1] 0.2162162\n# method 2\nmean(nyc.oz &gt; 70)  # sample proportion over 70 ppb\n\n[1] 0.2162162\nBoth methods are equivalent and we see that\n\\[\\hat{p} = 0.2162 = 21.62 \\%.\\] Thus use the plug-in principle, a reasonable estimate for the proportion of all time that the ozone concentration in NYC is over 70 ppb (we denote the population proportion \\(p\\)) is\n\\[ p \\approx \\hat{p} = 0.2612.\\] How certain can we be in this estimate? If we pick another sample of observations, would we get a similar estimate for \\(p\\), or should we expect a lot of variability?\nRecall the Central Limit Theorem (CLT) for Proportions,\n\\[\\widehat{P} \\sim N \\left( \\mu_{\\hat{P}}, \\mbox{SE}(\\widehat{P}) \\right) = \\left( {\\color{tomato}{p}}, \\sqrt{\\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}} \\right).\\]\nThe population proportion \\(p\\) is unknown, but we can use the plug-in principle and use the sample proportion \\({\\color{tomato}{\\hat{p} = 0.2162}}\\) in place to estimate the sampling distribution for the sample proportion:\n\\[\\begin{aligned}\n\\widehat{P} \\sim N \\left( \\mu_{\\hat{P}}, \\mbox{SE}(\\widehat{P}) \\right) &= \\left( {\\color{tomato}{p}}, \\sqrt{\\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}} \\right) \\\\\n& \\approx N\\left( {\\color{tomato}{0.2162}}, \\sqrt{\\frac{{\\color{tomato}{0.2162}}(1-{\\color{tomato}{0.2162}})}{111}} \\right) \\\\\n& \\approx N( 0.2162, 0.0391).\n\\end{aligned}\\]\nThe two methods, bootstrap distribution and the estimate from using the CLT, give us consistent results!\nUsually when estimating an unknown population parameter, we give an interval estimate that gives range of plausible values for the parameter by accounting for the uncertainty due to the variability in sampling.\n\\[ p_{\\rm over} \\approx \\hat{p}_{\\rm boot} \\color{dodgerblue}{\\pm \\mbox{SE}_{\\rm boot} \\left( \\widehat{P} \\right)} = 0.216 \\color{dodgerblue}{\\pm 0.039}.\\]\nThe interval between the \\(2.5\\) and \\(97.5\\) percentiles (or \\(0.025\\) and \\(0.975\\) quantiles) of the bootstrap distribution of a statistic is a 95% bootstrap percentile confidence interval for the corresponding parameter.\nFigure 22.1: Finding Cutoffs for a 95% Bootstrap Percentile Confidence Interval\nWe would expect the actual value of the unknown population parameter to equal the corresponding statistic calculated from one of the 100,000 bootstrap resamples in our distribution. Since 95% of the bootstrap statistics are inside the confidence interval:\nThe confidence level is the success rate of success of the interval estimate. We can choose differenet confidence levels for an interval estimate:\nOften in a study, we may be interested in determining whether there is an association between different variables. For example, we can ask:\nTo help explore this question, we will use a sample of data in the data frame5 birthwt that is in the package MASS which should already be installed. In the code cell below, the MASS package is loaded and numerical summaries for all variables in birthwt are computed and displayed.\nGiven independent samples of sizes \\(m\\) and \\(n\\) from two independent populations:\nm.non &lt;- length(non$bwt)  # m, size of sample 1\ntemp.non &lt;- sample(non$bwt, size = m.non, replace = TRUE)\nn.smoker &lt;- length(smoker$bwt)  # n, size of sample 2\ntemp.smoker &lt;- sample(smoker$bwt, size = n.smoker, replace = TRUE)\ndiff.resample &lt;- mean(temp.non) - mean(temp.smoker)\ndiff.resample\n\n[1] 209.3382\nGiven matched samples each of size \\(n\\):\nWe do a resample of the differences of each pair as opposed to two resamples from each individual sample.\nThe data frame darwin.maize in the agridat package contains results from experiment6 by Charles Darwin in 1876."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#loading-nyc-ozone-data",
    "href": "17-Bootstrap-Confidence-Int.html#loading-nyc-ozone-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Loading NYC Ozone Data",
    "text": "Loading NYC Ozone Data\n\nIt is very likely you do not have the package lattice installed. You will need to first install the lattice package.\n\nGo to the R Console window.\nRun the command &gt; install.packages(\"lattice\").\n\nYou will only need to run the install.package() command one time. You can now access lattice anytime you like! However, you will need to run the command library(lattice) during any R session in which you want to access data from the lattice package. Be sure you have first installed the lattice package before executing the code cell below.\n\n# be sure you have already installed the lattice package\nlibrary(lattice)  # loading permute package\n\n\nSummarizing and Storing the Data\n\nIn the code cell below we summarize a data frame3 named environmental from the lattice package and store the ozone concentration data environmental$ozone to a vector named nyc.oz.\n\nsummary(environmental)  # numerical summary of each variable\n\n     ozone         radiation      temperature         wind       \n Min.   :  1.0   Min.   :  7.0   Min.   :57.00   Min.   : 2.300  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.:71.00   1st Qu.: 7.400  \n Median : 31.0   Median :207.0   Median :79.00   Median : 9.700  \n Mean   : 42.1   Mean   :184.8   Mean   :77.79   Mean   : 9.939  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:84.50   3rd Qu.:11.500  \n Max.   :168.0   Max.   :334.0   Max.   :97.00   Max.   :20.700  \n\nnyc.oz &lt;- environmental$ozone  # store ozone data to a vector"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-1",
    "href": "17-Bootstrap-Confidence-Int.html#question-1",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 1",
    "text": "Question 1\n\nWe will use the sample data stored in nyc.oz to construct a bootstrap distribution that we can use to make predictions about the population of all times in New York City. Answer the questions below to get acquainted with the sample data.\n\nQuestion 1a\n\nHow many observations are in the sample stored in nyc.oz? Describe the shape of the data in nyc.oz.\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nBased on the sample data in nyc.oz, give an estimate for the mean ozone concentration in New York City (over all days and times).\n\nSolution to Question 1b"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-2",
    "href": "17-Bootstrap-Confidence-Int.html#question-2",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 2",
    "text": "Question 2\n\nAnswer each part below to construct a bootstrap distribution for the sample proportion. Then use the result to answer the questions that follow.\n\nQuestion 2a\n\nComplete the code cell below to construct a bootstrap distribution for the sample proportion of observations with ozone concentration greater than 70 ppb.\n\n\n\n\n\n\nTip\n\n\n\nThere are two operations to complete inside the for loop:\n\nPick a bootstrap resample from the observed sample nyc.oz.\nCalculate the proportion of observations in the bootstrap resample with ozone concentration greater than 70 ppb.\n\n\n\n\nSolution to Question 2a\n\nReplace all six ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed sample proportion (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.prop &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.prop[i] &lt;- ??  # compute bootstrap sample proportion\n}\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 2b\n\nBased on your answer to Question 2a, calculate bootstrap estimate for bias. Note answers will vary due to the randomness of bootstrapping in Question 2a.\n\nSolution to Question 2b\n\n\n# calculate bootstrap estimate of bias\n\n\n\n\n\n\nQuestion 2c\n\nBased on your answer to Question 2a, calculate the bootstrap estimate for the standard error of the sampling distribution for sample proportions. Note answers will vary due to the randomness of bootstrapping in Question 2a.\n\nSolution to Question 2c\n\n\n# calculate bootstrap standard error"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-3",
    "href": "17-Bootstrap-Confidence-Int.html#question-3",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 3",
    "text": "Question 3\n\nIn Question 2, we created a bootstrap distribution (stored in the vector boot.prop) for the sample proportion of times the ozone concentration exceeds 70 ppb. One possible bootstrap distribution is plotted below. Bootstrap distributions will vary slightly depending on the 100,000 bootstrap resamples that are randomly selected.\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = mean(boot.prop), col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nQuestion 3a\n\nUsing the bootstrap statistics stored in boot.prop, find the lower and upper cutoffs for a 95% bootstrap percentile confidence interval for the proportion of all times the ozone concentration in NYC exceeds 70 ppb.\n\n\n\n\n\n\nTip\n\n\n\nRecall the quantile() function in R. Run the command ?quantile for a refresher!\n\n\n\nSolution to Question 3a\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 95% bootstrap percentile confidence interval.\n\n# find cutoffs for 95% bootstrap CI\nlower.boot.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.boot.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print values to screen\nlower.boot.95\nupper.boot.95\n\nBased on the output above, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n\n\nQuestion 3b\n\nThe code cell below plots a bootstrap distribution corresponding of the sample proportions stored in boot.prop along with two blue vertical lines to mark the lower and upper cutoffs for a 95% bootstrap percentile confidence interval. A red vertical line marks the value of the sample proportion we calculated from the original sample.\nRun the code cell below to illustrate the confidence interval on the bootstrap distribution. There is nothing to edit in the code cell. Then in the space below, explain the practical meaning of the interval to a person with little to no background in statistics.\n\n#################################\n# code is ready to run!\n# no need to edit the code cell\n#################################\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.95, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.95, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 3b\n\nInterpret the confidence interval from Question 3a.\n\n\n\n\n\n\nQuestion 3c\n\nSometimes, it is desirable to describe the interval as some value plus or minus some margin of error. Construct a symmetric 95% bootstrap confidence interval for the proportion of time the ozone concentration in NYC exceeds 70 ppb. Compare the symmetric confidence interval to your percentile confidence interval in Question 3a.\n\n\n\n\n\n\nTip\n\n\n\nRecall for normal distributions, approximately 95% of the data is within 2 standard deviations of center of the distribution.\n\n\n\nSolution to Question 3c\n\nReplace each ?? in the code cell below with appropriate code.\n\n?? - 2*??  # going 2 SE's below the observed sample proportion\n?? + 2*??  # going 2 SE's above the observed sample proportion\n\nBased on the output above, a symmetric 95% bootstrap confidence interval is from ?? to ??."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-4",
    "href": "17-Bootstrap-Confidence-Int.html#question-4",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 4",
    "text": "Question 4\n\nIn Question 2, we created a bootstrap distribution (stored in the vector boot.prop) for the sample proportion of times the ozone concentration exceeds 70 ppb. In Question 3, we used the bootstrap distribution to construct a 95% bootstrap percentile confidence interval. In this question, we will investigate what happens when we change the confidence level.\n\nQuestion 4a\n\nComplete the first code cell below to give a 90% bootstrap percentile confidence interval to estimate the proportion of all time in NYC when the ozone concentration exceeds 70 ppb.\nThen complete the second code cell to plot a histogram of the bootstrap distribution from with the upper and lower confidence interval cutoffs marked with vertical lines similar to the plot in Question 3b.\n\nSolution to Question 4a\n\nBased on the output below, a 90% bootstrap percentile confidence interval is from ?? to ??.\n\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 90% bootstrap percentile confidence interval.\n\n# find cutoffs for 90% bootstrap CI\nlower.prop.90 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.prop.90 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.prop.90\nupper.prop.90\n\nNothing to edit in the code cell below. Just be sure you first run the code cell above to calculate and store the cutoffs lower.prop.90 and upper.prop.90.\n\n##################################\n# code is ready to run!\n# no need to edit the code cell\n##################################\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.90, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.90, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 4b\n\nComplete the first code cell below to give a 99% bootstrap percentile confidence interval to estimate the proportion of all time in NYC when the ozone concentration exceeds 70 ppb.\nThen complete the second code cell to plot a histogram of the bootstrap distribution from with the upper and lower confidence interval cutoffs marked with vertical lines similar to the plot in Question 3b.\n\nSolution to Question 4b\n\nBased on the output below, a 99% bootstrap percentile confidence interval is from ?? to ??.\n\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 90% bootstrap percentile confidence interval.\n\n# find cutoffs for 99% bootstrap CI\nlower.prop.99 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.prop.99 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.prop.99\nupper.prop.99\n\nNothing to edit in the code cell below. Just be sure you first run the code cell above to calculate and store the cutoffs lower.prop.99 and upper.prop.99.\n\n##################################\n# code is ready to run!\n# no need to edit the code cell\n##################################\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.99, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.99, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 4c\n\nWhen we decreased the confidence level from 95% in Question 3a to 90% in Question 4a, did the interval estimate get wider or more narrow? Explain why this makes practical sense. Feel free to explain using the fishing analogy from earlier.\n\nSolution to Question 4c\n\n\n\n\n\n\n\nQuestion 4d\n\nExplain the trade-off between choosing a confidence level and the precision of the interval estimate. In particular, why would choose a 95% confidence interval over an interval that has a greater chance of success, such as a 99% confidence interval?\n\nSolution to Question 4d"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-5",
    "href": "17-Bootstrap-Confidence-Int.html#question-5",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 5",
    "text": "Question 5\n\nExplain how you could design a study to collect data that could help determine whether smoking during pregnancy has an affect on the weight of the baby at birth.\n\nSolution to Question 5"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#loading-the-birth-weight-sample",
    "href": "17-Bootstrap-Confidence-Int.html#loading-the-birth-weight-sample",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Loading the Birth Weight Sample",
    "text": "Loading the Birth Weight Sample\n\n\nlibrary(MASS)  # load MASS package\nsummary(birthwt)  # summary of data frame\n\n      low              age             lwt             race      \n Min.   :0.0000   Min.   :14.00   Min.   : 80.0   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:19.00   1st Qu.:110.0   1st Qu.:1.000  \n Median :0.0000   Median :23.00   Median :121.0   Median :1.000  \n Mean   :0.3122   Mean   :23.24   Mean   :129.8   Mean   :1.847  \n 3rd Qu.:1.0000   3rd Qu.:26.00   3rd Qu.:140.0   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :45.00   Max.   :250.0   Max.   :3.000  \n     smoke             ptl               ht                ui        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.3915   Mean   :0.1958   Mean   :0.06349   Mean   :0.1481  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :3.0000   Max.   :1.00000   Max.   :1.0000  \n      ftv              bwt      \n Min.   :0.0000   Min.   : 709  \n 1st Qu.:0.0000   1st Qu.:2414  \n Median :0.0000   Median :2977  \n Mean   :0.7937   Mean   :2945  \n 3rd Qu.:1.0000   3rd Qu.:3487  \n Max.   :6.0000   Max.   :4990"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-6",
    "href": "17-Bootstrap-Confidence-Int.html#question-6",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 6",
    "text": "Question 6\n\nHow many observations are in the data set? How many variables? Which variables are categorical and which are quantitative? Which variables are most important in helping determine whether smoking during pregnancy has an affect on the weight of the baby at birth?\n\nSolution to Question 6"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#cleaning-the-birth-weight-data",
    "href": "17-Bootstrap-Confidence-Int.html#cleaning-the-birth-weight-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Cleaning the Birth Weight Data",
    "text": "Cleaning the Birth Weight Data\n\nThe variable smoke is being stored as a quantitative variable.\n\nPregnant parents that did not smoke have a smoke value equal to 0.\nPregnant parents that were smokers have a smoke value equal to 1.\nRun the code cell below to make these categories more clearly labeled.\n\nNon-smokers are assigned a value of no.\nSmokers are assigned a value of smoker.\nWe use the factor() command to convert the smoke variable to a categorical variable.\nThe output tells us out of 189 parents, 115 self-identified as non-smokers and 74 as smokers.\n\n\n\nbirthwt$smoke[birthwt$smoke == 0]  &lt;- \"no\"\nbirthwt$smoke[birthwt$smoke == 1]  &lt;- \"smoker\"\nbirthwt$smoke &lt;- factor(birthwt$smoke)\nsummary(birthwt$smoke)\n\n    no smoker \n   115     74"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-7",
    "href": "17-Bootstrap-Confidence-Int.html#question-7",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 7",
    "text": "Question 7\n\nComplete the code cell below to create a side-by-side box plots to compare the distribution of weights for smokers and non-smokers.\n\nSolution to Question 7\n\nReplace each ?? in the code cell below to generate side-by-side box plots for comparison.\n\n# create side by side box plots\nplot(?? ~ ??, data = ??,\n     col = c(\"springgreen4\", \"firebrick2\"),\n     main = \"Comparison of Birth Weights from Smokers and Non-Smokers\",\n     xlab = \"Smoking Status of Pregnant Parent\",\n     ylab = \"Birth Weight (in grams)\",\n     names = c(\"Non-smoker\", \"Smoker\"))"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#difference-in-two-independent-means",
    "href": "17-Bootstrap-Confidence-Int.html#difference-in-two-independent-means",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Difference in Two Independent Means",
    "text": "Difference in Two Independent Means\n\nOur statistical question is:\n\nDoes smoking during pregnancy have an affect on the weight of the baby at birth?\n\nIn this example, we have two independent populations of parents to consider.\n\nAll parents that did no smoke during pregnancy.\nAll parents that smoke during pregnancy.\nYou are either in one group or the other, not both!\n\nIdeally, if we had access to data on every baby that has been born, then we could:\n\nCalculate \\(\\mu_{\\rm{non}}\\), the mean birth weight of all children of non-smokers.\nCalculate \\(\\mu_{\\rm{smoker}}\\), the mean birth weight of all children of smokers.\nConsider how large is the difference in the two means, \\(\\mu_{\\rm{non}} - \\mu_{\\rm{smoker}}\\).\n\nIf the difference in populations means is 0, there is no difference in mean birth weights.\nIf the difference is not 0, then there is a difference in mean birth weights.\n\n\nWe have data from a random sample, but we plug that data in place of the population and perform an equivalent analysis.\n\nCalculate \\(\\bar{x}_{\\rm{non}}\\), the sample mean birth weight of children of non-smokers in the sample.\nCalculate \\(\\bar{x}_{\\rm{smoker}}\\), the sample mean birth weight of children of smokers in the sample.\nConsider how large is the difference in the two means, \\(\\bar{x}_{\\rm{non}} - \\bar{x}_{\\rm{smoker}}\\).\n\nIf the difference is close to zero, this indicates there is likely no difference in the population means.\nIf the difference is not close to zero, this indicates there likely is a difference in the population means."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#subsetting-the-sample",
    "href": "17-Bootstrap-Confidence-Int.html#subsetting-the-sample",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Subsetting the Sample",
    "text": "Subsetting the Sample\n\nAt the moment, we have all of the sample data for smokers and non-smokers stored in the same data frame named birthwt. How can we calculate the sample mean birth weights for the non-smoking group separate from the smoking group? One way to compare the two samples is to split the sample into two independent subsets based on whether or not the child was birthed by a smoker or not."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-8",
    "href": "17-Bootstrap-Confidence-Int.html#question-8",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 8",
    "text": "Question 8\n\nAnswer the questions to find an initial estimate for the difference in the mean birth weights of all children born to a non-smoking parent compared to the mean birth weight of all children born to a parent that did smoke while pregnant.\n\nQuestion 8a\n\nComplete each of the subset() commands below to subset the data into two independent samples: parent was a smoker and parent was a non-smoker.\n\nSolution to Question 8a\n\n\n# subset the sample into two independent samples\nnon &lt;- subset(??, smoke == ??)\nsmoker &lt;- subset(??, smoke == ??)\n\n\n\n\n\n\nQuestion 8b\n\nComplete the code cell below to calculate, store, and print the difference in sample means based on the data in our original sample.\n\nSolution to Question 8b\n\n\n# calculate difference in sample means\nobs.diff &lt;- ??\nobs.diff  # print observed difference to screen\n\n\n\n\n\n\nQuestion 8c\n\nBased on your answer to [Quesiton 8b], given an estimate for the difference in the mean birth weights of all children born to a non-smoking parent compared to the mean birth weight of all children born to a parent that did smoke while pregnant. Include units in your answer.\n\nSolution to Question 8c\n\n\n\n\n\n\n\nQuestion 8d\n\nBased on your estimate in Question 8c, do you believe there is a difference in the mean birth weight of all babies whose parent smoked while pregnant compared to the mean birth weight of all babies whose parent did not smoke while pregnant?\n\nSolution to Question 8d"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#accounting-for-uncertainty-in-sampling",
    "href": "17-Bootstrap-Confidence-Int.html#accounting-for-uncertainty-in-sampling",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Accounting for Uncertainty in Sampling",
    "text": "Accounting for Uncertainty in Sampling\n\nIn the case of comparing samples, we do need to be mindful of the randomness involved in the sampling process. If we pick another sample of 189 babies and compare the difference in sample means, we will likely get another value for the difference in sample means. How can we determine whether the difference in sample means is larger than the variability we might expect due to sampling?"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-9",
    "href": "17-Bootstrap-Confidence-Int.html#question-9",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 9",
    "text": "Question 9\n\nFollow the steps below to generate a bootstrap distribution for the difference in sample means and obtain a 95% bootstrap percentile confidence interval for the difference in population means.\n\nQuestion 9a\n\nComplete the code cell below to construct a bootstrap distribution for the difference in the sample mean birth weights of of babies born to non-smokers compared to smokers.\n\nSolution to Question 9a\n\nReplace all six ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed sample proportion (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.diff.mean &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x.non &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  x.smoker &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.diff.mean[i] &lt;- ??  # compute difference in sample means\n}\n\n# plot bootstrap distribution\nhist(boot.diff.mean,  \n     breaks=20, \n     xlab = \"x.bar.non - x.bar.smoker (in grams)\",\n     main = \"Bootstrap Distribution for Difference in Means\")\n\n# red line at the observed  difference in sample means\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 9b\n\nComplete the code cell below to give a 95% bootstrap percentile confidence interval to estimate the difference in the mean birth weight of all babies born to non-smokers compared the to mean birth of all babies born to smokers.\n\nSolution to Question 9b\n\nBased on the output below, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n# find cutoffs for 95% bootstrap CI\nlower.bwt.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.bwt.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.bwt.95\nupper.bwt.95\n\n\n\n\nQuestion 9c\n\nInterpret the practical meaning of your interval estimate in Question 9b. Do you think it is plausible to conclude smoking does have an effect on the weight of a newborn? Explain why or why not.\n\nSolution to Question 9c\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#loading-the-data",
    "href": "17-Bootstrap-Confidence-Int.html#loading-the-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nIt is very likely you do not have the package agridat installed. You will need to first install the agridat package.\n\nGo to the R Console window.\nRun the command &gt; install.packages(\"agridat\").\n\nYou will only need to run the install.package() command one time. You can now access agridat anytime you like! However, you will need to run the command library(agridat) during any R session in which you want to access data from the agridat package. Be sure you have first installed the agridat package before executing the code cell below.\n\n# be sure you have already installed the agridat package\nlibrary(agridat)  # loading agridat package\n\n\nSummarizing and Storing the Data\n\nThe the help manual for the data frame darwin.maize has a nice summary of the experiment and the data. Run the code cell below to learn about the data and context of the experiment.\n\n# be sure you have already loaded the agridat package\n?darwin.maize\n\nBelow is an excerpt from the help manual:\n\nCharles Darwin, in 1876, reported data from an experiment that he had conducted on the heights of corn plants. The seeds came from the same parents, but some seeds were produced from self-fertilized parents and some seeds were produced from cross-fertilized parents. Pairs of seeds were planted in pots. Darwin hypothesized that cross-fertilization produced produced more robust and vigorous offspring.\n\nIn the code cell below we provide a numerical summary of the two variables in the sample.\n\nsummary(darwin.maize)  # numerical summary of each variable\n\n  pot          pair       type        height     \n I  : 6   a      : 2   cross:15   Min.   :12.00  \n II : 6   b      : 2   self :15   1st Qu.:17.53  \n III:10   c      : 2              Median :18.88  \n IV : 8   d      : 2              Mean   :18.88  \n          e      : 2              3rd Qu.:21.38  \n          f      : 2              Max.   :23.50  \n          (Other):18                             \n\n\n\npot is a categorical variable with 4 levels: I, II, III, and IV.\n\npair is a categorical variable with 15 levels: a, b, \\(\\ldots\\) , n, o.\ntype cagtegorical with 2 levels: cross and self.\nheight plant height in inches\n\nThe code cell below displays the distribution of mandible lengths separately for males and females.\n\n# side by side box plots\nplot(height ~ type, data = darwin.maize, \n     col = c(\"dodgerblue\", \"mediumseagreen\"),\n     main = \"Darwin Cross Fertilization Results\",\n     ylab = \"Hength (in inches)\")"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#wranginling-the-data",
    "href": "17-Bootstrap-Confidence-Int.html#wranginling-the-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Wranginling the Data",
    "text": "Wranginling the Data\n\nIn the code cell below, we reorganize the data from corn.maize so it is structured in a way that will be more convenient for our analysis. The result is stored in a new data frame named corn, and the first 6 rows are printed to the screen. In total, there are 18 pairs of data.\n\nself &lt;- subset(darwin.maize, select = c(pair, type, height), type == \"self\")\ncross &lt;- subset(darwin.maize, select = c(pair, type, height), type == \"cross\")\ncorn &lt;- data.frame(letters[1:15], self$height, cross$height)\nnames(corn) &lt;- c(\"pair\", \"self\", \"cross\")\nkbl(head(corn, 18), format = \"html\")\n\n\n\nTable 28.1: Corn Heights\n\n\npair\nself\ncross\n\n\n\n\na\n17.375\n23.500\n\n\nb\n20.375\n12.000\n\n\nc\n20.000\n21.000\n\n\nd\n20.000\n22.000\n\n\ne\n18.375\n19.125\n\n\nf\n18.625\n21.500\n\n\ng\n18.625\n22.125\n\n\nh\n15.250\n20.375\n\n\ni\n16.500\n18.250\n\n\nj\n18.000\n21.625\n\n\nk\n16.250\n23.250\n\n\nl\n18.000\n21.000\n\n\nm\n12.750\n22.125\n\n\nn\n15.500\n23.000\n\n\no\n18.000\n12.000"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-4-wetsuit-case-study",
    "href": "17-Bootstrap-Confidence-Int.html#question-4-wetsuit-case-study",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 4: Wetsuit Case Study",
    "text": "Question 4: Wetsuit Case Study\n\nExample 20.5. Researchers are concerned about the impact of vitamin C content reduction due to storage and ship- ment. To test this, researchers randomly chose a collection of bags of wheat soy blend bound for Haiti, marked them, and measured vitamin C from a sample of the contents. Five months later, the bags were opened and a second sample was measured for vitamin C content. The units are milligrams of vitamin C per 100g of wheat soy blend. - Watkins page 366\n\n\n\nPhelps Wetsuit\n\n\nIn the 2008 Olympics there was a lot of controversy over new swimsuits that possibly provided an unfair advantage to swimmers which led to new international rules regarding swimsuit materials and coverage. Can a swimsuit really make a swimmer faster?7\nA study8 tested whether wearing wetsuits influences swimming velocity. Twelve competitive swimmers swam 1500 meters at maximum speed twice each.\n\nOnce wearing a wetsuit and once wearing a regular bathing suit.\nThe order of the trials was randomized.\nEach time, the maximum velocity in meters/sec of the swimmer was recorded.\nThe max velocity with and without the wetsuit are recorded in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwimmer\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nNew Wetsuit\n\\(1.57\\)\n\\(1.47\\)\n\\(1.42\\)\n\\(1.35\\)\n\\(1.22\\)\n\\(1.75\\)\n\\(1.64\\)\n\\(1.57\\)\n\\(1.56\\)\n\\(1.53\\)\n\\(1.49\\)\n\\(1.51\\)\n\n\nRegular Swimsuit\n\\(1.49\\)\n\\(1.37\\)\n\\(1.35\\)\n\\(1.27\\)\n\\(1.12\\)\n\\(1.64\\)\n\\(1.59\\)\n\\(1.52\\)\n\\(1.50\\)\n\\(1.45\\)\n\\(1.44\\)\n\\(1.41\\)\n\n\nDifference\n\\(0.08\\)\n\\(0.10\\)\n\\(0.07\\)\n\\(0.08\\)\n\\(0.10\\)\n\\(0.11\\)\n\\(0.05\\)\n\\(0.05\\)\n\\(0.06\\)\n\\(0.08\\)\n\\(0.05\\)\n\\(0.10\\)\n\n\n\n\nQuestion 4a\n\nThe code cell below contains ordered vectors of maximum velocities (with the wetsuit and with a regular swimsuit) for each of the 12 swimmers. The ordering in each vector must be consistent since we have natural pairing of values between the two samples. Compute the observed mean matched-pair difference.\n\nSolution to Question 4a\n\nComplete the code cell below by replacing each ?? with appropriate code.\n\n# Vectors containing the times of each swimmer. Ordering is critical\nwetsuit &lt;- c(1.57, 1.47, 1.42, 1.35, 1.22, 1.75, 1.64, 1.57, 1.56, 1.53, 1.49, 1.51)\nnone &lt;- c(1.49, 1.37, 1.35, 1.27, 1.12, 1.64, 1.59, 1.52, 1.50, 1.45, 1.44, 1.41)\nDiff &lt;- ??  # compute vector of matched-pair differences\nobs.match &lt;- ??  # compute mean of matched-pair differences\n\n# Print to screen\nobs.match\n\n\n\n\nQuestion 4b\n\nGenerate one possible bootstrap resample and print the values in your resample to the screen.\n\nSolution to Question 4b\n\nComplete the code cell below by replacing each ?? with appropriate code.\n\nsample(??, ??, replace = ??)\n\n\n\n\nQuestion 4c\n\nConstruct a 95% bootstrap percentile confidence interval to estimate this difference.\n\nSolution to Question 4c\n\nComplete the code cell below by replacing each ?? with appropriate code.\n\nn &lt;- length(Diff) # number of matched-pairs in sample\n\nN &lt;- 10^5\nboot.match &lt;- numeric(N)\n\n# Create bootstrap samples by picking n difference from original sample\n# With replacement, and then find mean of those differences.\nfor (i in 1:N)\n{\n  samp.diff &lt;- ??  # pick a bootstrap resample\n  boot.match[i] &lt;- ??  # compute mean of resampled matched-pair differences\n}\n\n\n# Bootstrap mean and bootstrap standard error\nmean.match &lt;- ??  # compute bootstrap mean\nse.match &lt;- ??  # compute bootstrap standard error\n\n\n# Calculate lower and upper cutoffs\n# for the 95% Bootstrap CI\nlower.match &lt;- quantile(boot.match, probs = ??)\nupper.match &lt;- quantile(boot.match, probs = ??)\nlower.match\nupper.match\n\n# Display bootstrap distribution\nhist(boot.match, xlab = \"difference in max velocity in m/sec (wetsuit - none)\",\n      main = \"Bootstrap Dist for Mean Matched-Pair Difference (n=12)\")\n\n\n# Add a red line at the observed mean matched-pair difference\nabline(v = ??, col = \"red\", lwd = 2, lty = 2)\n\n# Add blue lines at cutoffs for 95 percentile\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 4d\n\nInterpret the practical meaning of your answer to Question 4c. Do you think it is plausible to conclude the wetsuit gives a competitive advantage? Explain why or why not.\n\nSolution to Question 4d"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#footnotes",
    "href": "17-Bootstrap-Confidence-Int.html#footnotes",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "",
    "text": "https://www.epa.gov/ozone-pollution-and-your-patients-health/what-ozone accessed June 26, 2023↩︎\nOzone Air Quality Standards acccessed from EPA June 26, 2023↩︎\nBruntz, Cleveland, Kleiner, and Warner. (1974). “The Dependence of Ambient Ozone on Solar Radiation, Wind, Temperature, and Mixing Height”. In Symposium on Atmospheric Diffusion and Air Pollution. American Meterological Society, Boston.↩︎\nAnswers from Question 2 may vary↩︎\nBaystate Medical Center, Springfield, Massachusetts↩︎\nDarwin, C. R. 1876. “The effects of cross and self fertilisation in the vegetable kingdom”. London: John Murray.↩︎\nBased on content from Statistics: Unlocking the Power of Statistics by R. Lock, P. Lock, K. Lock, E. Lock, and D. Lock.↩︎\nde Lucas, Balidan, Neiva, Grecco, and Denadai. “The effects of wetsuits on physiological and biomechanical indices during swimming’’, Journal of Science and Medicine in Sport.↩︎"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html",
    "href": "18-Bootstrap-Other-Stats.html",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "",
    "text": "A Recap of Bootstrapping\nIn this section, we will continue exploring bootstrap distributions. Thus far, we have analyzed statistical questions where bootstrapping different statistics has been constructive, such as a single sample mean and a sample proportion. In many situations, we wish to investigate the possible association between two or more variables. In these cases, comparing two or more sample statistics is more useful than a single mean or a single proportion.\nFor example, we investigated whether there is an association between the smoking during pregnancy and birth weight of the baby.\nWe used a bootstrap distribution on a difference of two independent sample means to construct a bootstrap confidence interval to estimate the difference in mean birth weights to help determine whether a difference of \\(0\\) is plausible or not.\nLet’s recap and compare data sets from two different studies investigating potential side-effects of smoking during pregnancy:\nStudy A: There is a sample of 189 observations corresponding to 189 different pregnancies. For each observation, we have two values corresponding to smoking status and birth weight of baby. The sample is split based on whether or not the parent smoked during their pregnancy. For any single observation in the non-smoking group, there is no inherent connection to any of the observations in the smoking group. The non-smoker and smoker groups are independent samples.\nStudy B: There is a sample of \\(n=10\\) observations corresponding to \\(10\\) different people who each gave birth to two children. During one of the pregnancies they smoked, and they did not smoke during the other. For each observation, we have two variables: birth weight of baby when they smoked and the birth weight of the baby during the non-smoking pregnancy. For any single observation in the non-smoker group of birth weights, there is exactly one observation (their sibling) in the smoker group of birth weights that naturally form a pair of observations. This is an example of a matched pairs design since observations from each sample are matched based on a key variable. In this case, we pair non-smoking and smoking birth weights based on parent of the baby.\nWhen we bootstrap matched pairs, we do not want to randomness to affect the natural pairing between the two samples! Below is an algorithm for bootstrapping matched pairs. In order to preserve the pairing of the smoking and non-smoking birth weights, we do a single resample from the original sample of differences as opposed to two resamples from two independent samples.\nGiven a matched pairs sample of size \\(n\\):\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking birth weights\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight\n\n\ndiff &lt;- no - smoker  # storing the original sample of differences\nobs.mean &lt;- mean(diff)  # observed mean difference between matched pairs\n\n# print observed mean to screen\nobs.mean\n\n[1] 394\nresamp &lt;- sample(diff, size = 10, replace = TRUE)\nresamp\n\n [1] 610 960 960 960 -80  14  85 960 -80 -80\nmean(resamp)\n\n[1] 430.9\nWhen constructing sampling distributions for with means and proportions, in addition to bootstrapping, we also have a theoretical model available with the Central Limit Theorem (CLT). For example, we when working with golden jackal data, we constructed a bootstrap distribution to approximate the sampling distribution for the sample mean mandible length. Then we compared the bootstrap results to those obtained using the CLT for means. However, not all statistics have a Central Limit Theorem or formulas we can use to model a sampling distribution.\nWhen investigating the possible effect of smoking during pregnancy on birth weight, we initially used the birthwt data frame in the MASS package to construct a bootstrap distribution for the difference of means (DoM). From the bootstrap distribution, we obtained a confidence interval to estimate the difference in the mean birth weights between children of those that smoked and those that did not smoke during pregnancy. One possible 95% bootstrap percentile confidence interval for the difference of means is \\(80 \\mbox{ g} &lt; \\mu_{\\rm{non}} - \\mu_{s} &lt; 486 \\mbox{g }\\).\nAnother comparative analysis we can do is compare the ratio of means (RoM) instead of the difference of means. Let \\(R = \\dfrac{\\mu_{\\rm{non}}}{\\mu_{\\rm{s}}}\\) denote the ratio of population means. \\(R\\) is unknown population parameter that we can analyze as follows:\nWhether we use a difference or ratio of means can highlight or minimize certain characteristics of the data. With ratios, we generally get sample ratios close 1. With differences, we can get a large spread of values in the differences of sample means. Thus, one potentially nice outcome of using ratios is the magnitude of the sample statistics will be smaller and easier to compare with other ratios from other contexts that will also have values close to 1. Another nice advantage is the results are independent of the units we choose to measure birth weight. If we choose to measure birth weights in pounds instead of grams, birth weights will be closer to 7, 8, 9 pounds as opposed to values such as 2000 or 3000 grams. We get different confidence intervals for the difference in mean birth weights depending on whether we use grams or pounds. The bootstrap confidence intervals we get for a ratio of means will be the same regardless of the units for weight.\nRecall the data set birthwt from the MASS package we worked with earlier. In the code cell below, we load and clean the data in birthwt.\nlibrary(MASS)  # load MASS package\nbirthwt$smoke[birthwt$smoke == 0]  &lt;- \"no\"  # non-smokers assigned \"no\"\nbirthwt$smoke[birthwt$smoke == 1]  &lt;- \"smoker\"  # smokers assigned \"smoker\"\nbirthwt$smoke &lt;- factor(birthwt$smoke)  # convert smoke variable to categorical factor\nsummary(birthwt$smoke)  # summary of data frame\n\n    no smoker \n   115     74\nBefore we can bootstrap, we next subset the original sample into two independent samples based on smoking status. The output is stored in data frames non and smoker, and no output will be printed to the screen.\n# subset the sample into two independent samples\nnon &lt;- subset(birthwt, smoke == \"no\")\nsmoker &lt;- subset(birthwt, smoke == \"smoker\")\nRecall if \\(\\widehat{\\theta}\\) is an estimator for a parameter \\(\\theta\\), then we define the bias of the estimator as\n\\[\\mbox{Bias}(\\widehat{\\theta}) = (\\mbox{Estimated Value}) - (\\mbox{Actual Value}) = \\widehat{\\theta} - \\theta.\\]\nIn the case of bootstrapping, we used the following estimate:\n\\[\\mbox{Bias}_{\\rm{boot}} \\big( \\hat{\\theta}_{\\rm{boot}} \\big) = {\\color{dodgerblue}{\\hat{\\theta}_{\\rm{boot}}}} - {\\color{tomato}{(\\mbox{observed statistic})}}.\\]"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-1",
    "href": "18-Bootstrap-Other-Stats.html#question-1",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 1",
    "text": "Question 1\n\nThe data used in the smoking and birth weight analysis is from an observational study. We determined the observed difference in sample means is significant, and thus concluded smoking during pregnancy is likely associated with lower baby birth weights. However, with observational studies we should be mindful of confounding variables. Based on our analysis we can conclude smoking is associated with a change in birth weight, but not whether the smoking itself is the cause of the change in birth weight. What are some possible confounding variables?\n\nSolution to Question 1"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#designing-a-randomized-controlled-experiment",
    "href": "18-Bootstrap-Other-Stats.html#designing-a-randomized-controlled-experiment",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Designing a Randomized Controlled Experiment",
    "text": "Designing a Randomized Controlled Experiment\n\nIn order to see whether smoking during pregnancy causes lower birth weights, we could try to design a randomized controlled experiment. Designing such an experiment that is humane in this context might not be possible. For example, here is an example of a highly unethical experiment we would not implement in practice:\n\nCollect volunteers early in their pregnancy who agree to the conditions of the experiment.\nRandomly assign each pregnant person to a smoking or non-smoking group.\n\nEach person in the smoking group is required to smoke 1 pack of cigarettes per day.\nEach person in the non-smoking group is forbidden from smoking any tobacco during pregnancy.\n\nRecord the birth weight of each person’s baby.\nCompare mean birth weights of the two groups (smokers and non-smokers).\n\nAlthough such an experiment does eliminate the impact of many confounding variables, it is not possible to conduct this experiment due to ethical concerns."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#collecting-pairs-of-data",
    "href": "18-Bootstrap-Other-Stats.html#collecting-pairs-of-data",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Collecting Pairs of Data",
    "text": "Collecting Pairs of Data\n\nLet’s consider one more study that we could use to help determine whether smoking during pregnancy is associated with lower birth weight. In this study, we solicit volunteers that have already given birth to two babies. During one of the pregnancies, the parent smoked. During the other pregnancy, they did not smoke. This study is ethical and controls for some confounding variables though not all. Below is hypothetical data from such a study. A sample of \\(n=10\\) people volunteer to share their data with the researchers from which we have 10 different pairs of birth weights (in grams) summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-2",
    "href": "18-Bootstrap-Other-Stats.html#question-2",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the data from this study, give a possible sample statistic that can be used to determine if smoking is associated with lower birth weight?\n\nSolution to Question 2\n\n\n# data from study\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking births weights\n\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-3",
    "href": "18-Bootstrap-Other-Stats.html#question-3",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 3",
    "text": "Question 3\n\nDevise a bootstrapping method that could be used to construct a bootstrap distribution for the comparison statistic you identified in Question 2.\n\nSolution to Question 3"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-4",
    "href": "18-Bootstrap-Other-Stats.html#question-4",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 4",
    "text": "Question 4\n\nFollow the steps below to generate a bootstrap distribution for the mean difference between matched pairs of smoking and non-smoking birth weights. Then use the bootstrap distribution to obtain a 95% bootstrap percentile confidence interval for the mean difference between all matched pairs in the population.\n\nQuestion 4a\n\nComplete the code cell below to construct a bootstrap distribution for the sample mean difference between matched pairs.\n\nSolution to Question 4a\n\nReplace the six ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed mean difference between matched pairs (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.matched &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.matched[i] &lt;- ??  # compute sample mean of matched pair differences\n}\n\n# plot bootstrap distribution\nhist(boot.matched,  \n     breaks=20, \n     xlab = \"Sample Mean of Matched Pair Differenes\",\n     main = \"Bootstrap Distribution for Matched Pairs\")\n\n# red line at the observed mean difference between matched pairs\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\n\nQuestion 4b\n\nComplete the code cell below to give a 95% bootstrap percentile confidence interval to estimate the mean difference between all matched pairs in the population. Include units in your answer.\n\nSolution to Question 4b\n\nBased on the output below, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n# find cutoffs for 95% bootstrap CI\nlower.matched.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.matched.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.matched.95\nupper.matched.95\n\n\n\n\n\n\nQuestion 4c\n\nInterpret the practical meaning of your interval estimate in Question 4b. Do you think it is plausible to conclude smoking does have an effect on the weight of a newborn? Explain why or why not.\n\nSolution to Question 4c"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-5",
    "href": "18-Bootstrap-Other-Stats.html#question-5",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 5",
    "text": "Question 5\n\nA random sample of \\(n=20\\) mandible jaw lengths (in mm) of golden jackals is stored in the vector jaw.sample. Run the code cell below to load the sample data, and then answer the questions that follow.\n\njaw.sample &lt;- c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112,\n                110, 111, 107, 108, 110, 105, 107, 106, 111, 111)\n\n\nQuestion 5a\n\nBelow is code we previously used to create bootstrap distribution for the sample mean mandible length. Adjust the code to create a bootstrap distribution for the sample median mandible length. Then run the updated code cell to store the bootstrap medians in the vector named boot.dist. No output will printed to the screen since the output is being stored in boot.dist.\n\nSolution to Question 5a\n\nAdjust the code cell below to create a bootstrap distribution for the sample median.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dist &lt;- numeric(N)  # create vector to store bootstrap stats\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(jaw.sample, 20, replace = TRUE)  # pick a bootstrap resample\n  boot.dist[i] &lt;- mean(x)  # compute mean of bootstrap resample\n}\n\n\n\n\n\n\nQuestion 5b\n\nUsing the bootstrap distribution boot.dist created in Question 5a, give a 90% bootstrap percentile confidence interval for the median mandible length of all golden jackals. Include units in your answer.\n\nSolution to Question 5b\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 90% bootstrap percentile confidence interval.\n\n# find cutoffs for 90% bootstrap CI\nlower.median.90 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.median.90 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.median.90\nupper.median.90\n\nBased on the output above, a 90% bootstrap percentile confidence interval for the median is from ?? to ??."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-6",
    "href": "18-Bootstrap-Other-Stats.html#question-6",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 6",
    "text": "Question 6\n\nCalculate the observed ratio of sample mean birth weights of smokers and non-smokers and store the value as obs.ratio.\n\nSolution to Question 6\n\n\nobs.ratio &lt;- ??  # calculate the observed ratio of sample means\nobs.ratio  # print observed ratio to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-7",
    "href": "18-Bootstrap-Other-Stats.html#question-7",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 7",
    "text": "Question 7\n\nFollow the steps below to generate a bootstrap distribution for the ratio of sample mean birth weights for babies born to non-smokers compared to babies born to smokers. Then use the bootstrap distribution to obtain a 95% bootstrap percentile confidence interval to estimate the ratio in population means.\n\nQuestion 7a\n\nComplete the code cell below to construct a bootstrap distribution for the ratio of the sample mean birth weights of babies born to non-smokers compared to babies born to smokers.\n\nSolution to Question 7a\n\nReplace all nine ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed ratio of sample means (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.rom &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x.non &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample of non-smokers\n  x.smoker &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample of smokers\n  boot.rom[i] &lt;- ??  # compute ratio of sample means\n}\n\n# plot bootstrap distribution\nhist(boot.rom,  \n     breaks=20, \n     xlab = \"Ratio = x.bar.non/x.bar.smoker\",\n     main = \"Bootstrap Distribution for Ratio of Means\")\n\n# red line at the observed  difference in sample means\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\n\nQuestion 7b\n\nComplete the code cell below to give a 95% bootstrap percentile confidence interval to estimate the ratio of the mean birth weight of all babies born to non-smokers compared the to mean birth of all babies born to smokers.\n\nSolution to Question 7b\n\nBased on the output below, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n# find cutoffs for 95% bootstrap CI\nlower.rom.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.rom.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.rom.95\nupper.rom.95\n\n\n\n\n\n\nQuestion 7c\n\nInterpret the practical meaning of your interval estimate in Question 7b. Do you think it is plausible to conclude smoking does have an effect on the weight of a newborn? Explain why or why not.\n\nSolution to Question 7c\n\n\n\n\n\n\n\nQuestion 7d\n\nIs the shape of the bootstrap distribution for the ratio of means normal? Create a QQ-Plot to compare the bootstrap distribution to a standard normal distribution.\n\nSolution to Question 7d\n\nReplace each ?? with appropriate code.\n\nqqnorm(??)\nqqline(??)\n\n\n\nInterpret the plot above to answer the question."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-8",
    "href": "18-Bootstrap-Other-Stats.html#question-8",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 8",
    "text": "Question 8\n\nThe code cell below constructs a bootstrap distribution for the sample mean from the sample data stored in jaw.sample. The corresponding bootstrap sample means are stored in the vector boot.dist.\n\nRead the code cell below.\nAfter interpreting the code, run the code cell. No edits are needed.\nIn the code cell provided in the solution space, calculate the bootstrap estimate of bias.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the case of bootstrap estimate for a mean, we have \\({\\displaystyle \\mbox{Bias}_{\\rm{boot}} \\big( \\hat{\\mu}_{\\rm{boot}} \\big) = {\\color{dodgerblue}{\\hat{\\mu}_{\\rm{boot}}}} - {\\color{tomato}{\\bar{x}}}}\\). Use the output from the code below to identify the values of \\({\\color{dodgerblue}{\\hat{\\mu}_{\\rm{boot}}}}\\) and \\({\\color{tomato}{\\bar{x}}}\\).\n\n\n\n##############################\n# no edits needed\n# read and run the code\n#############################\njaw.sample &lt;- c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112,\n                110, 111, 107, 108, 110, 105, 107, 106, 111, 111)\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dist &lt;- numeric(N)  # create vector to store bootstrap stats\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(jaw.sample, 20, replace = TRUE)  # pick a bootstrap resample\n  boot.dist[i] &lt;- mean(x)  # compute mean of bootstrap resample\n}\n\n# plot bootstrap distribution\nhist(boot.dist,  \n     breaks=20, \n     xlab = \"x-bar, mandible length (in mm)\",\n     main = \"Bootstrap Distribution for Sample Mean (n=20)\")\n\n# bootstrap mean and standard error\nboot.dist.mean &lt;- mean(boot.dist)  # mean of bootstrap dist\nboot.dist.se &lt;- sd(boot.dist)  # SE of bootstrap dist\n\n\n# red line at the observed sample mean\nabline(v = mean(jaw.sample), col = \"firebrick2\", lwd = 2, lty = 1)\n# blue line at the center of bootstrap dist\nabline(v = boot.dist.mean, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 8\n\nReplace the ?? in the code cell below to calculate and store bootstrap estimate of bias to bias.jaw.\n\n# calculate bootstrap estimate of bias\nbias.jaw &lt;- ?? \nbias.jaw  # print to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-9",
    "href": "18-Bootstrap-Other-Stats.html#question-9",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 9",
    "text": "Question 9\n\nRecall the data set environmental from the lattice package we worked with earlier to estimate the proportion of all time in New York City when the ozone concentration exceeds 70 ppb. In the code cell below, we load the lattice package and store the sample of ozone concentrations to a vector named nyc.oz.\n\n\n\n\n\n\nNote\n\n\n\nIf you receive an error message when running the code cell below, then you do not have the lattice package installed.\n\nIn the R console, run the command install.packages(\"lattice\").\nThen rerun the code cell below.\n\n\n\n\n# be sure you have already installed the lattice package\nlibrary(lattice)  # loading lattice package\nnyc.oz &lt;- environmental$ozone  # store ozone data to a vector\n\nThe code cell below constructs a bootstrap distribution for the sample proportion of time the ozone concentration exceeds 70 ppb using the original sample nyc.oz. The corresponding bootstrap sample proportions are stored in the vector boot.prop.\n\nRead the code cell below.\nAfter interpreting the code, run the code cell. No edits are needed.\nIn the code cell provided in the solution space, calculate the bootstrap estimate of bias.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the case of bootstrap estimate for a proportion, we have \\({\\displaystyle\\mbox{Bias}_{\\rm{boot}} \\big( \\hat{p}_{\\rm{boot}} \\big) = {\\color{dodgerblue}{\\hat{p}_{\\rm{boot}}}} - {\\color{tomato}{\\hat{p}}}}\\).\n\n\n\n##############################\n# no edits needed\n# read and run the code\n#############################\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.prop &lt;- numeric(N)  # create vector to store bootstrap proportions\nn.oz &lt;- length(nyc.oz)\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(nyc.oz, size = n.oz, replace = TRUE)  # pick a bootstrap resample\n  boot.prop[i] &lt;- sum(x &gt; 70)/n.oz  # compute bootstrap sample proportion\n}\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# bootstrap mean and standard error\nboot.prop.mean &lt;- mean(boot.prop)  # mean of bootstrap dist\nboot.prop.se &lt;- sd(boot.prop)  # SE of bootstrap dist\n\n# red line at the observed sample proportion\nabline(v = sum(nyc.oz &gt; 70)/n.oz, col = \"firebrick2\", lwd = 2, lty = 1)\n# blue line at the center of bootstrap dist\nabline(v = boot.prop.mean, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 9\n\nReplace the ?? in the code cell below to calculate and store bootstrap estimate of bias to bias.ozone.\n\n# calculate bootstrap estimate of bias\nbias.ozone &lt;- ??\nbias.ozone  # print to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-10",
    "href": "18-Bootstrap-Other-Stats.html#question-10",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 10",
    "text": "Question 10\n\nRecall the data set birthwt from the MASS package we worked with earlier. In the code cell below, we load and clean the data in birthwt.\n\n\n\n\n\n\nNote\n\n\n\nIf you receive an error message when running the code cell below, then you do not have the MASS package installed.\n\nIn the R console, run the command install.packages(\"MASS\").\nThen rerun the code cell below.\n\n\n\n\nlibrary(MASS)  # load MASS package\nbirthwt$smoke[birthwt$smoke == 0]  &lt;- \"no\"  # non-smokers assigned \"no\"\nbirthwt$smoke[birthwt$smoke == 1]  &lt;- \"smoker\"  # smokers assigned \"smoker\"\nbirthwt$smoke &lt;- factor(birthwt$smoke)  # convert smoke variable to categorical factor\n\n# subset the sample into two independent samples\nnon &lt;- subset(birthwt, smoke == \"no\")\nsmoker &lt;- subset(birthwt, smoke == \"smoker\")\n\nThe code cell below constructs a bootstrap distribution for the difference in two sample means using the independent samples stored in the data frames non and smoker. The corresponding bootstrap difference in sample means are stored in the vector boot.dom.\n\nRead the code cell below.\nAfter interpreting the code, run the code cell. No edits are needed.\nIn the code cell provided in the solution space, calculate the bootstrap estimate of bias.\n\n\n##############################\n# no edits needed\n# read and run the code\n#############################\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dom &lt;- numeric(N)  # create vector to store bootstrap proportions\n\nm.non &lt;- length(non$bwt)\nn.smoker &lt;- length(smoker$bwt)\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x.non &lt;- sample(non$bwt, size = m.non, replace = TRUE)  # pick a bootstrap resample\n  x.smoker &lt;- sample(smoker$bwt, size = n.smoker, replace = TRUE)  # pick a bootstrap resample\n  boot.dom[i] &lt;- mean(x.non) - mean(x.smoker)  # compute difference in sample means\n}\n\n# plot bootstrap distribution\nhist(boot.dom,  \n     breaks=20, \n     xlab = \"x.bar.non - x.bar.smoker (in grams)\",\n     main = \"Bootstrap Distribution for Difference in Means\")\n\n# bootstrap mean and standard error\nboot.dom.mean &lt;- mean(boot.dom)  # mean of bootstrap dist\nboot.dom.se &lt;- sd(boot.dom)  # SE of bootstrap dist\n\n# red line at the observed  difference in sample means\nabline(v = mean(non$bwt) - mean(smoker$bwt), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = boot.dom.mean, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 10\n\nReplace the ?? in the code cell below to calculate and store bootstrap estimate of bias to bias.smoke.\n\n# calculate bootstrap estimate of bias\nbias.smoke &lt;- ??\nbias.smoke  # print to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-11",
    "href": "18-Bootstrap-Other-Stats.html#question-11",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 11",
    "text": "Question 11\n\nOut of the three bootstrap estimates of bias in Question 8, Question 9, and Question 10, which bias is the most extreme?\n\nSolution to Question 11"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#rule-of-thumb-for-acceptable-bias",
    "href": "18-Bootstrap-Other-Stats.html#rule-of-thumb-for-acceptable-bias",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Rule of Thumb for Acceptable Bias",
    "text": "Rule of Thumb for Acceptable Bias\n\nWhat is a significant amount of bias? The answer to this question does not depend on the bootstrap estimate of bias alone. We need to consider how much bias is present relative to the overall variability in sample statistics. For example:\n\nIf the bootstrap estimate of bias is \\(5\\) and \\(\\mbox{SE}_{\\rm{boot}} = 5,\\!000\\), then relatively speaking the bias is quite small.\nIf the bootstrap estimate of bias is \\(-0.1\\) and \\(\\mbox{SE}_{\\rm{boot}} = 2\\), then relatively speaking the bias is much larger.\nWe compare the value of the bootstrap estimate of bias relative to the bootstrap standard error.\n\nIn the first example, we have the ratio \\(\\frac{5}{5000} = 0.001\\).\nIn the second example, we have a ratio \\(\\frac{-0.1}{2} = -0.05\\)\n\nIn general, we use the ratio of the bootstrap bias to the bootstrap standard error to measure how significant is the bias.\n\n\\[{\\color{dodgerblue}{\\boxed{ \\mbox{Relative Bootstrap Bias} = \\dfrac{\\mbox{Bootstrap Bias}}{\\mbox{Bootstrap SE}}}}}\\]\n\nRule of Thumb for Bootstrap Bias\n\nIf either \n\\[{\\color{dodgerblue}{\\frac{\\mbox{Bootstrap Bias}}{\\mbox{Bootstrap SE}}  &gt; 0.02 \\qquad \\mbox{or} \\qquad \\frac{\\mbox{Bootstrap Bias}}{\\mbox{Bootstrap SE}}  &lt; -0.02,}}\\]\nthen the bias is large enough to have a substantial effect on the accuracy of the estimate."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-12",
    "href": "18-Bootstrap-Other-Stats.html#question-12",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 12",
    "text": "Question 12\n\nDetermine whether the jackal mandible bootstrap distribution in Question 8, the ozone concentration bootstrap distribution for sample proportion in Question 9, or the bootstrap distribution for the difference in mean birth weights Question 10 exceed the 0.02 rule of thumb? Relatively speaking, which of the three biases is most extreme?\n\nSolution to Question 12\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "19-Parametric-CI-Means.html",
    "href": "19-Parametric-CI-Means.html",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "",
    "text": "Parametric and Non-Parametric Methods\nWe have been investigating resampling methods as a tool to estimate the variability in sample statistics. Statistics such as means and proportions are linear combinations of random variables, and using theory from probability, we are able the derive the Central Limit Theorem (CLT) formulas to model sampling distributions for means and proportions. In this section we will construct confidence intervals by applying a parametric approach that uses formulas from the CLT to measure the uncertainty (standard error) in our estimate.\nSuppose a climatologist has contacted us for help estimating the average wind speed of all storms in the North Atlantic. They have collected a random sample of \\(n=36\\) wind speeds from North Atlantic storms over the past 5 years.\nBefore introducing parametric methods for constructing a confidence interval estimate, we first review resampling methods to construct a bootstrap distribution from the sample my.sample that we can use to construct a bootstrap confidence interval. The code below uses the original sample my.sample to generate a bootstrap distribution for the sample mean wind speed (with \\(n=36\\)) that is stored in boot.wind for the next step.\nN &lt;- 10^5  # Number of bootstrap samples\nboot.wind &lt;- numeric(N)  # create vector to store bootstrap means\nn &lt;- 36  # sample size\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(my.sample, size = n, replace = TRUE)  # pick a bootstrap resample\n  boot.wind[i] &lt;- mean(x)  # compute bootstrap sample mean\n}\n\nboot.wind.est &lt;- mean(boot.wind)  # mean of bootstrap dist\nboot.wind.se &lt;- sd(boot.wind)  # bootstrap standard error\nmy.mean &lt;- mean(my.sample)  # original sample mean\nIn the code cell below, we use the bootstrap sample means (stored in boot.wind above) to construct a 95% bootstrap percentile confidence interval estimate for the mean wind speed of all North Atlantic storms.\nlower.boot.wind &lt;- quantile(boot.wind, 0.025)  # lower 95% cutoff\nupper.boot.wind &lt;- quantile(boot.wind, 0.975)  # upper 95% cutoff\n\n# print to screen\nlower.boot.wind   \n\n 2.5% \n51.25 \n\nupper.boot.wind\n\n   97.5% \n72.36111\nBased on the output from the previous code cell, we have a 95% bootstrap percentile confidence interval for the mean wind speed of all North Atlantic storms, namely from 51.25 knots to 72.36 knots. Note the interval estimates may vary, so you likely have a slightly different interval estimate if you are working with a different sample.\nIn the code cell below, we plot the bootstrap distribution with the original sample mean of my.sample marked with a red vertical line and the cutoffs for the confidence interval marked with blue vertical lines.\n#################################\n# code is ready to run!\n# no need to edit the code cell\n#################################\n\n# plot bootstrap distribution\nhist(boot.wind,  \n     breaks=20, \n     xlab = \"x.bar, sample mean wind speed (in knots)\",\n     main = \"Bootstrap Distribution for Sample Mean\")\n\n# red line at the observed sample proportion\nabline(v = my.mean, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.wind, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.wind, col = \"blue\", lwd = 2, lty = 2)\nRecall the 68%-95%-99.7% empirical rule for normal distributions that states approximately:\nFor a sample of size \\(n\\) randomly picked from a normal distribution with unknown \\(\\mu\\) and known \\(\\mbox{Var}(X)=\\sigma^2\\), a 95% confidence interval for the mean is\n\\[{\\color{dodgerblue}{\\boxed{ \\large \\overline{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt;  \\overline{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}}}}.\\]\nIf we draw 1000’s of random samples each size \\(n\\) from a normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) and compute a 95% confidence interval from each sample, then about 95% of the intervals would successfully contain \\(\\mu\\). The 95% is the confidence level of the interval and gives the success rate of the interval estimate.\nIt can be easy to confuse the terms uncertain and unknown. In a less formal setting, people may use these terms interchangeably. In statistical inference, there is a clear distinction in their meaning that is important to clarify:\nFor a sample of size \\(n\\) randomly picked from a normal distribution with unknown \\(\\mu\\) and known \\(\\mbox{Var}(X)=\\sigma^2\\), then a confidence interval for the mean is given by\n\\[\\boxed{ \\large \\overline{X} - {\\color{tomato}{z_{\\alpha/2}}} \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + {\\color{tomato}{z_{\\alpha/2}}} \\cdot \\frac{\\sigma}{\\sqrt{n}}},\\]\nwhere the area under \\(N(0,1)\\) between \\(\\pm z_{\\alpha/2}\\) is equal to the confidence level of the estimate.\n\\[\\boxed{ \\large  \\mu \\approx  \\overline{X} \\pm {\\color{dodgerblue}{\\mbox{MoE}}}}\\]\nLet CL denote a selected confidence level that is the proportion of area in the middle.\nFigure 27.1: Identifying Z Values for Confidence Interval\nWhen estimating a population mean, in addition to \\(\\mu\\) being unknown, we often do not know the population variance, \\(\\sigma^2\\), either. We pick a random sample size \\(n\\), and we can plug-in the sample mean \\(\\bar{x}\\) as our point estimate for \\(\\mu\\). How can we find the margin of error if \\(\\sigma\\) is unknown?\nFrom the CLT for means, as long as the sample is normally distributed or if \\(n \\geq 30\\), then we know the distribution of sample means \\(\\overline{X}\\) is normally distributed with mean \\({\\color{mediumseagreen}{\\mu_{\\overline{X}} = \\mu_X}}\\) and standard error \\({\\color{dodgerblue}{\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}}}\\). Thus, the standardized distribution of \\(z\\)-scores corresponding the sampling distribution for the sample mean \\(\\overline{X}\\) is normal:\n\\[Z = \\frac{\\overline{X} - \\color{mediumseagreen}{\\mu_{\\overline{X}}}}{{\\color{dodgerblue}{\\sigma_{\\overline{X}}}}} = \\frac{\\overline{X} - \\color{mediumseagreen}{\\mu_{X}}}{{\\color{dodgerblue}{\\sigma_{X}/\\sqrt{n}}}} \\sim N(0,1).\\]\nLet’s apply our substitution of \\(s\\) in place of \\(\\sigma_{X}\\) and consider the resulting distribution of standardized statistics denoted \\(W\\):\n\\[Z = \\frac{\\overline{X} - \\mu_{X}}{{\\color{tomato}{\\sigma_{X}}}/\\sqrt{n}} \\quad \\xrightarrow{\\text{plug-in } {\\color{tomato}{s}} \\text{ for } {\\color{tomato}{\\sigma_{X}}}}  \\quad W = \\frac{\\overline{X}-\\mu_X}{{\\color{tomato}{s}}/\\sqrt{n}} \\sim \\mbox{ ?}\\]\n(a) Distribution of Standardized Statistics.\n\n\n\n\n\n\n\n(b) QQ-plot comparing W to N(0,1).\n\n\n\n\nFigure 28.1: Standardized Distribution When Variance Unknown.\nThe distribution \\(W\\) is symmetric and bell-shaped. From the QQ-plot, we see \\(W\\) is approximately normal in the middle of the distribution. However, the tails of distribution \\(W\\) are not similar to tails of a normal distribution. Distribution \\(W\\) has fatter tails than a normal distribution.\nFigure 28.2: Plots of t-distributions with different degrees of freedom\nIn our analysis of North Atlantic storm wind speeds, we used data stored in the storms data frame in the dplyr package. In our initial analysis, we assumed the data in storms represented the population, and we did not have access to the population data in storms. The data in storms is actually a sample of 19066 storm observations taken for North Atlantic storms that are measured every six hours during the lifetime of the storm. The population of interest is all storms that occur at all times in the North Atlantic.\nlibrary(dplyr)\nstorms$month &lt;- factor(storms$month)  # convert month to categorical factor\nboxplot(wind ~ month, \n        data = storms,\n        main = \"Distribution of Wind Speeds by Month\",\n        xlab = \"Month\",\n        ylab = \"Wind speed (in knots)\")\nThe t.test() function in R can used to construct a confidence interval for a difference in means from two independent populations using a \\(t\\)-distribution with Welch’s approximation for the degree’s for freedom. Let x and y denote vectors containing data from each of the two independent samples."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#loading-storms-data",
    "href": "19-Parametric-CI-Means.html#loading-storms-data",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Loading Storms Data",
    "text": "Loading Storms Data\n\nWhen practicing exploratory data analysis earlier, we accessed the dplyr package that contains a data set from the NOAA Hurricane Best Track Data, called storms, with data on many different storm attributes. We will begin analyzing the variable wind that gives the wind speed in knots. Run the code cell below to:\n\nLoad the dplyr package (which should already be installed), and\nStore the wind speeds to a new vector called population.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you receive an error message when running the code cell below, then you may not have the dplyr package installed.\n\nIn the R console, run the command install.packages(\"dplyr\").\nThen rerun the code cell below.\n\n\n\n\nlibrary(dplyr)\npopulation &lt;- storms$wind"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#picking-a-random-sample-of-storms",
    "href": "19-Parametric-CI-Means.html#picking-a-random-sample-of-storms",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Picking a Random Sample of Storms",
    "text": "Picking a Random Sample of Storms\n\nWhen performing statistical inference, we do not have complete data from the entire population. We typically have data from one sample picked randomly from the population. Based on the sample data, we try to estimate unknown parameters for the population. To begin our exploration of confidence intervals, suppose the 19066 wind speeds in the population data represents the full population of all storms at all times in the North Atlantic. We investigate the following statistical question:\n\nWhat is the average wind speed of all storms in the North Atlantic, \\(\\mu_{\\rm{wind}}\\)?\n\n\nAssume we do not have access to all the population data and cannot directly calculate \\(\\mu_{\\rm{wind}}\\).\nWe will pick one random sample of 36 wind speeds from the population.\nWe will use our sample to estimate the mean wind speed of all storms in the North Atlantic."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-1",
    "href": "19-Parametric-CI-Means.html#question-1",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 1",
    "text": "Question 1\n\nPick a random sample of \\(n=36\\) wind speeds from the population. In picking your original sample, sample without replacement. Store your sample to a vector named my.sample.\n\nSolution to Question 1\n\n\n# pick one random sample of 36 wind speeds without replacement\nmy.sample &lt;- sample(??, size = ??, replace = ??)"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-2",
    "href": "19-Parametric-CI-Means.html#question-2",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the random sample my.sample, give a possible point estimate for \\(\\mu_{\\rm{wind}}\\), the mean wind speed of all storms in the North Atlantic. The parameter \\(\\mu_{\\rm{wind}}\\) is unknown since we assume the full population data is not available.\n\nSolution to Question 2\n\n\n# use your sample to give a point estimate"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-3",
    "href": "19-Parametric-CI-Means.html#question-3",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 3",
    "text": "Question 3\n\nCheck the accuracy of the empirical rule for normal distributions.\n\nQuestion 3a\n\nUsing the R code cell below, check the accuracy of the approximation that 68% of the data is within \\(\\pm 1\\) standard deviation from the mean.\n\n\n\n\n\n\nTip\n\n\n\nUsing the standard normal distribution \\(Z \\sim N(0,1)\\), compute \\(P(-1 &lt; Z &lt; 1)\\).\n\n\n\nSolution to Question 3a\n\n\n# how much data is within +/- 1 standard deviation?\n\n\n\n\n\n\nQuestion 3b\n\nUsing the R code cell below, check the accuracy of the approximation that 95% of the data is within \\(\\pm 2\\) standard deviation from the mean.\n\nSolution to Question 3b\n\n\n# how much data is within +/- 2 standard deviations?\n\n\n\n\n\n\nQuestion 3c\n\nUsing the R code cell below, check the accuracy of the approximation that \\(99.7\\)% of the data is within \\(\\pm 3\\) standard deviation from the mean.\n\nSolution to Question 3c\n\n\n# how much data is within +/- 3 standard deviations?\n\n\n\n\n\n\nQuestion 3d\n\nLet’s improve the approximation that 95% of the data is withing \\(\\pm 2\\) standard deviations from the mean. Find a more accurate approximation for \\(z^*\\) such that \\(P( -z^* &lt; Z &lt; z^* ) = 0.95\\).\n\n\n\n\n\n\nTip\n\n\n\nSketch a picture to help determine how much area is in the left tail below \\(-z^*\\) if 95% of the area is between \\(-z^*\\) and \\(z^*\\). Then use the qnorm() command.\n\n\n\nSolution to Question 3d\n\n\n# find more accurate cutoffs for the middle 95%\nqnorm(??, 0, 1)"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-4",
    "href": "19-Parametric-CI-Means.html#question-4",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 4",
    "text": "Question 4\n\nLet \\(X\\) denote the wind speed of a randomly selected North Atlantic storm. We use \\(\\mu\\) and \\(\\sigma\\) to denote the mean and standard deviation, respectively, of the wind speed of the population of all storms in the North Atlantic. Answer the questions below to investigate a different approach to constructing an interval estimate for \\(\\mu\\).\n\nQuestion 4a\n\nLet \\(\\overline{X}\\) denote the distribution of sample means for samples size \\(n\\). Recall we denote the mean and standard error of the sampling distribution as \\(\\mu_{\\overline{X}}\\) and \\(\\sigma_{\\overline{X}}\\), respectively.\nJustify each of the steps below.\n\\[\\begin{aligned}\n0.95 &= P \\left( -1.96 &lt; Z &lt; 1.96 \\right) & \\mbox{from Question 3d}\\\\\n0.95   &= P \\left( -1.96 &lt; \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} &lt; 1.96 \\right) & \\mbox{Explanation 1}\\\\\n0.95   &=  P \\left( -1.96 &lt; \\frac{\\overline{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} &lt; 1.96 \\right) & \\mbox{Explanation 2}\\\\\n0.95   &= P \\left( -1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt;  1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right) & \\mbox{Explanation 3} \\\\\n0.95   &= P \\left( - \\overline{X} -1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt;  - \\overline{X} + 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right) & \\mbox{Explanation 4} \\\\\n0.95   &= P \\left( \\overline{X} - 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X}+1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right) & \\mbox{Explanation 5}\\\\\n\\end{aligned}\\]\n\nSolution to Question 4a\n\nExplanation 1:\n\nExplanation 2:\n\nExplanation 3:\n\nExplanation 4:\n\nExplanation 5:\n\n\n\n\n\nQuestion 4b\n\nLet \\(\\mu_{\\rm{wind}}\\) denote the mean wind speed of all North Atlantic storms which we assume is unknown. How can we use the result that \\(P \\left( \\overline{X} - 1.96\\cdot \\sigma_{\\overline{X}} &lt; \\mu_{\\rm{wind}} &lt; \\overline{X}+1.96 \\cdot \\sigma_{\\overline{X}} \\right)=0.95\\) to help construct a confidence interval for \\(\\mu_{\\rm{wind}}\\)?\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nYou will use the original random sample of \\(n=36\\) wind speeds stored in my.sample in Question 1 to construct an interval estimate using the result from Question 4b. If you have not already loaded the population data and created a sample size \\(n=36\\) stored in my.sample, be sure to answer Question 1 before continuing. Create a histogram to display the wind speeds in my.sample.\n\nSolution to Question 4c\n\n\n# Enter code to create a histogram to display your sample\n\n\n\n\n\n\nQuestion 4d\n\nAre the assumptions for the CLT for means satisfied by the data in my.sample? Explain why or why not.\n\nSolution to Question 4d\n\n\n\n\n\n\n\nQuestion 4e\n\nWe assume the population data is unknown and we do not know the actual value of the parameter \\(\\mu_{\\rm{wind}}\\). However, suppose we do know the population variance \\(\\mbox{Var}(X) = \\sigma^2_{\\rm{wind}} = 650\\) square knots. Using the CLT for means, what is the value of \\(\\sigma_{\\overline{X}}\\), the standard error of the sampling distribution for \\(\\overline{X}\\)? Show your work below either doing calculations by hand or in R. Tip: In R, the square root function is sqrt().\n\nSolution to Question 4e\n\n\n# compute standard error using CLT\n\n\n\n\n\n\n\nQuestion 4f\n\nGive an interval of values \\(?? &lt; \\mu_{\\rm{wind}} &lt; ??\\) that has a 95% chance of containing the actual value of \\(\\mu_{\\rm{wind}}\\). Include supporting work/code to explain how you determined your answer.\n\nSolution to Question 4f\n\n\n# compute upper and lower cutoffs for interval estimate"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-5",
    "href": "19-Parametric-CI-Means.html#question-5",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 5",
    "text": "Question 5\n\nA another climatologist collects their own random sample of \\(n=36\\) wind speeds of North Atlantic storms.\n\nQuestion 5a\n\nPick their random sample (without replacement) of \\(n=36\\) wind speeds from the population. Store the other sample to a vector named their.sample.\n\nSolution to Question 5a\n\n\n# pick another random sample of 36 wind speeds without replacement\ntheir.sample(??, size = ??, replace = ??)\n\n\n\n\n\n\nQuestion 5b\n\nUsing the random sample their.sample, give a possible point estimate for \\(\\mu_{\\rm{wind}}\\), the mean wind speed of all storms in population.\n\nSolution to Question 5b\n\n\n# use their sample to give another point estimate\n\n\n\n\n\n\nQuestion 5c\n\nAre the two point estimates in Question 2 and Question 5b equal?\n\nSolution to Question 5c\n\n\n\n\n\n\n\nQuestion 5d\n\nAre the true values of the population parameter different or the same for both climatologists?\n\nSolution to Question 5d\n\n\n\n\n\n\n\nQuestion 5e\n\nWithout checking, which point estimate (Question 2 or Question 5b) do you believe is “better”?\n\nSolution to Question 5e"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-6",
    "href": "19-Parametric-CI-Means.html#question-6",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 6",
    "text": "Question 6\n\nA researcher collects a random sample size \\(n=36\\) of wind speeds for storms in the North Atlantic and calculates a 95% confidence interval for the mean wind speed that is from \\(48.75\\) knots to \\(65.41\\) knots. For each statement, determine whether the interpretation is correct or not. If not, explain why not.\n\nQuestion 6a\n\nThere is a 95% chance that a randomly selected storm in the North Atlantic has a wind speed between \\(48.75\\) and \\(65.41\\) knots.\n\nSolution to Question 6a\n\n\n\n\n\n\n\nQuestion 6b\n\nThere is a 95% chance that the mean wind speed of all storms in the North Atlantic is between \\(48.75\\) and \\(65.41\\) knots.\n\nSolution to Question 6b\n\n\n\n\n\n\n\nQuestion 6c\n\nThere is a 95% chance the interval from \\(48.75\\) to \\(65.41\\) knots contains the mean wind speed of all storms in the North Atlantic.\n\nSolution to Question 6c\n\n\n\n\n\n\n\nQuestion 6d\n\n95% of all random samples of size \\(n=36\\) North Atlantic storms have a sample mean wind speed between \\(48.75\\) and \\(65.41\\) knots.\n\nSolution to Question 6d\n\n\n\n\n\n\n\nQuestion 6e\n\nThere is a 95% chance the interval from \\(48.75\\) to \\(65.41\\) knots contains the mean wind speed of a random sample of \\(n=36\\) storms.\n\nSolution to Question 6e"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#attaching-uncertainty-to-the-intervals",
    "href": "19-Parametric-CI-Means.html#attaching-uncertainty-to-the-intervals",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Attaching Uncertainty to the Intervals",
    "text": "Attaching Uncertainty to the Intervals\n\nThe plot shows 100 different confidence intervals based on 100 different random samples each size \\(n=36\\) storms selected from the population. For each sample mean \\(\\overline{X}\\), we follow the same process of constructing a 95% confidence interval using:\n\\[\\overline{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu_{\\rm{wind}} &lt;  \\overline{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}.\\]\n\nEach sample results is a different interval estimate (all with the same width).\nThe true population mean is marked with a blue vertical line at \\(\\mu_{\\rm{wind}} = 50.02\\) knots.\nThe goal of each interval is to contain the same, fixed value of the population parameter.\n\nThe confidence intervals that successfully contain \\(\\mu_{\\rm{wind}} = 50.02\\) are marked in green.\nThe unsuccessful confidence intervals that do not contain \\(\\mu_{\\rm{wind}} = 50.02\\) are marked in red.\n\n2 out of the 100 interval estimates are underestimates.\n2 out of the 100 interval estimates are overestimates.\n\nWe have a success rate of 96 out of 100 in this simulation.\n\nIf we ran this simulation many more times, the success rate would converge to 95%.\n\n\n\n\n\n\nFigure 26.1: Results of 100 randomly generated 95% confidence intervals."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-7",
    "href": "19-Parametric-CI-Means.html#question-7",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 7",
    "text": "Question 7\n\nWhat are some warnings to keep in mind when interpreting confidence intervals?\n\nSolution to Question 7\n\nWarning 1:\n\nWarning 2:\n\nWarning 3:"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-8",
    "href": "19-Parametric-CI-Means.html#question-8",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 8",
    "text": "Question 8\n\nYou will use the original random sample of \\(n=36\\) wind speeds stored in my.sample in Question 1 to construct a new confidence interval estimate for the mean wind speed of all North Atlantic storms. If you have not already loaded the population data and created a sample size \\(n=36\\) stored in my.sample, be sure to answer Question 1 before continuing.\n\nQuestion 8a\n\nBased on your sample data in my.sample, give a 90% confidence interval for the mean wind speed of all North Atlantic storms using the parametric method. As with earlier, suppose we know population variance is \\(\\sigma^2_X = 650\\). Interpret the practical meaning of your 90% confidence interval in the context of this example.\n\nSolution to Question 8a\n\n\nnew.z &lt;- ??  # Enter a command to calculate z value for 90% Conf Level\nwind.lowe90 &lt;- ??   # Enter a command to calculate the lower limit\nwind.upper90 &lt;- ??   # Enter a command to calculate the upper limit\n\n# Print your answers\nwind.lower90\nwind.upper90\n\n\n\nPractical interpretation of your 90% confidence interval:\n\n\n\n\n\n\nQuestion 8b\n\nAs we decrease the confidence level of the interval from 95% in Question 4f to 90% in Question 8a, what happened to the width of the interval estimate?\n\nSolution to Question 8b"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#the-t-distribution",
    "href": "19-Parametric-CI-Means.html#the-t-distribution",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "The \\(t\\)-Distribution",
    "text": "The \\(t\\)-Distribution\n\nWhen the population is known to be normally distributed, the distribution \\(W\\) is a \\(t\\)-distribution with \\(n-1\\) degrees of freedom, where \\(n\\) denotes the sample size. If the underlying population is known to be symmetric (but not necessarily normal), a \\(t\\)-distribution with \\(n-1\\) degrees of freedom is still a good estimate.\n\nThe larger our sample size \\(n\\), the better (less biased) the estimate \\(s\\) is for \\(\\sigma\\).\nThe larger the sample size \\(n\\), the closer the \\(t\\)-distribution is to \\(Z \\sim N(0,1)\\).\n\nIf a random sample of data is skewed, then the population is likely skewed. The more skewed the population is, the less accurate a \\(t\\)-distribution is to estimate the distribution of the standardized statistic \\(W\\).\n\n\n\n\n\n\nWarning\n\n\n\nIn cases where the sample is small (\\(n &lt; 30\\)) and skewed, a bootstrap confidence interval is typically preferred to using a \\(t\\)-distribution to approximate the margin of error."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#using-a-t-distribution-when-variance-is-unknown",
    "href": "19-Parametric-CI-Means.html#using-a-t-distribution-when-variance-is-unknown",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Using a \\(t\\)-Distribution When Variance is Unknown",
    "text": "Using a \\(t\\)-Distribution When Variance is Unknown\n\nFor a sample of size \\(n\\) randomly picked from a normal distribution with unknown \\(\\mu\\) and unknown \\(\\mbox{Var}(X)=\\sigma^2\\), we construct a confidence interval for \\(\\mu\\) using:\n\nThe sample standard deviation \\(s\\) in place of \\(\\sigma\\), and\n\nA \\(t\\)-distribution to find \\(t_{\\alpha/2}\\) instead of using \\(N(0,1)\\) to find \\(z_{\\alpha/2}\\)\n\nA corresponding \\(t\\)-distribution confidence interval is given by\n\\[{\\large \\boxed{ \\overline{X} - {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}}}},\\]\nwhere the area under the \\(t\\)-distribution with \\(n-1\\) degrees of freedom between \\({\\color{dodgerblue}{\\pm t_{\\alpha/2}}}\\) is equal to the confidence level."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-9",
    "href": "19-Parametric-CI-Means.html#question-9",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 9",
    "text": "Question 9\n\n\n\n\nCredit: Charles J. Sharp, CC BY-SA 4.0, via Wikimedia Commons\n\n\nResearchers want to estimate the average length of all adult female Komodo dragons. The pick a random sample of \\(n=8\\) adult female Komodo dragons with the following weights (in pounds). They believe the distribution in weights will by normally distributed, but otherwise, the population variance is unknown.\n\ndragon.wt &lt;- c(145, 178, 142, 139, 160, 190, 168, 122)  # load sample of weights  \ndragon.wt  # print to screen\n\n[1] 145 178 142 139 160 190 168 122\n\n\n\nQuestion 9a\n\nUse the R code cell below to construct a 95% confidence interval for mean weight of all female Komodo dragons using the sample weights in dragon.wt.\n\n\n\n\n\n\nTip\n\n\n\nUsing the data in dragon.wt, calculate the statistics \\(\\bar{x}\\) and \\(s\\). To find \\(t_{\\alpha/2}\\), use the qt() function similar to how you use the qnorm() to calculate \\(z_{\\alpha/2}\\). For more help using the qt() function, run ?qt().\n\n\n\nSolution to Question 9a\n\n\n# construct a 95% confidence interval\n\n\n\n\n\n\nQuestion 9b\n\nInterpret the practical meaning of the confidence interval in Question 9a in this context.\n\nSolution to Question 9b\n\n\n \n\n\n\nQuestion 9c\n\nUse the R code cell below to construct a 99% confidence interval for mean weight of all female Komodo dragons using the sample weights in dragon.wt.\n\nSolution to Question 9c\n\n\n# construct a 99% confidence interval"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#different-methods-for-t-distribution-confidence-intervals",
    "href": "19-Parametric-CI-Means.html#different-methods-for-t-distribution-confidence-intervals",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Different Methods for \\(t\\)-Distribution Confidence Intervals",
    "text": "Different Methods for \\(t\\)-Distribution Confidence Intervals\n\n\nUse the formula \\(\\displaystyle \\overline{X} - {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}}\\).\n\nUse the qt() function in R to find the value of \\(t_{\\alpha/2}\\), or\nUse a \\(t\\)-distribution table to estimate values for \\(t_{\\alpha/2}\\).\n\nUse the R function t.test(x, conf.level)$conf.int.\n\nx is the vector of sample data.\nSet the confidence level with the conf.level option.\nOpen the help manual with ?t.test for further information."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-10",
    "href": "19-Parametric-CI-Means.html#question-10",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 10",
    "text": "Question 10\n\nUsing the Komodo dragon sample data in dragon.wt and the t.test() function, find a 95% confidence interval. Compare the results with the 95% confidence interval from Question 9a.\n\nSolution to Question 10\n\n\nt.test(??, conf.level = ??)$conf.int"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#sec-clt-diff-means",
    "href": "19-Parametric-CI-Means.html#sec-clt-diff-means",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 11",
    "text": "Question 11\n\nLet \\(X\\) and \\(Y\\) be independent random variables with \\(X \\sim N(\\mu_1, \\sigma_1)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2)\\). Using properties of expected value and variance, show for sample sizes \\(n_1\\) and \\(n_2\\), respectively, that\n\\[E(\\overline{X}-\\overline{Y}) = \\mu_1-\\mu_2 \\qquad \\mbox{and} \\qquad \\mbox{Var}(\\overline{X}-\\overline{Y}) = \\frac{\\sigma_1^2}{n_1} +  \\frac{\\sigma_2^2}{n_2}.\\]\n\nSolution to Question 11"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#sec-informal",
    "href": "19-Parametric-CI-Means.html#sec-informal",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Confidence Intervals for a Difference in Means",
    "text": "Confidence Intervals for a Difference in Means\n\nLet \\(X_1, X_2, \\ldots , X_{n_1}\\) be independent and identically distributed (i.i.d.) random variables picked from \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and let \\(Y_1, Y_2, \\ldots , Y_{n_2}\\) be i.i.d. random variables picked from \\(Y \\sim N(\\mu_2, \\sigma_1^2)\\). An approximate confidence interval for the difference in means \\(\\mu_1 - \\mu_2\\) is given by:\n\\[{\\large \\boxed{ {\\color{mediumseagreen}{(\\overline{X} - \\overline{Y})}} -  {\\color{tomato}{t_{\\alpha/2}}} \\cdot  {\\color{dodgerblue}{\\sqrt{\\frac{s_1^2}{n_1} +  \\frac{s_2^2}{n_2}}}} &lt; \\mu_1 - \\mu_2 &lt; {\\color{mediumseagreen}{(\\overline{X} - \\overline{Y})}} +  {\\color{tomato}{t_{\\alpha/2}}} \\cdot  {\\color{dodgerblue}{\\sqrt{\\frac{s_1^2}{n_1} +  \\frac{s_2^2}{n_2}}}}}}. \\]\n\nUse \\(\\bar{x} - \\bar{y}\\) as the point estimate for \\(\\mu_1 - \\mu_2\\).\nApproximate the standard error using \\(s_1\\) and \\(s_2\\) in place of unknown standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\).\nThe area under the \\(t\\)-distribution with \\(df\\) degrees of freedom between \\(\\pm t_{\\alpha/2}\\) is equal to the confidence level.\n\nInformally, we can use the smaller of \\(n_1-1\\) and \\(n_2-1\\) as the degrees of freedom.\nMany R functions use the more accurate Welch’s approximation for the degrees of freedom,\n\n\\[ df = \\dfrac{\\left( \\dfrac{s_1^2}{n_1}+ \\dfrac{s_2^2}{n_2} \\right)^2}{ \\dfrac{(s_1^2/n_1)^2}{n_1-1} + \\dfrac{(s_2^2/n_2)^2}{n_2-1}}.\\]\n\nWe do not need to memorize Welch’s approximation since t.test() uses this method."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-12",
    "href": "19-Parametric-CI-Means.html#question-12",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 12",
    "text": "Question 12\n\nBased on the plots in the figure above, in which month do you suspect storms have the greatest wind speed? Explain how you determined your answer.\n\nSolution to Question 12"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#refining-our-question",
    "href": "19-Parametric-CI-Means.html#refining-our-question",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Refining Our Question",
    "text": "Refining Our Question\n\nThere are different statistics for assessing “greatest wind speed”. From the box plots, we have a good visual comparison of medians. With storms, outliers are typically the most important observations we do want to emphasize in our analysis. Thus, we use the mean as our measurement of center instead of a median or trimmed mean.\nSide-by-side box plots give a nice graphical summary of the distribution of winds speeds by month. As we might suspect, from this first plot we rule out some months from this analysis. Some months appear to have very similar distributions upon inspection of box plots. We dig a little into the sample data to determine which months have the greatest sample mean wind speeds.\n\nWe use the tapply() function below to calculate and compare sample mean wind speeds by month.\nHere is a nice introduction to the tapply() function.\n\n\ntapply(storms$wind, storms$month, mean)\n\n       1        4        5        6        7        8        9       10 \n49.85714 38.86364 35.17413 35.32092 41.39426 48.14414 54.56013 51.58109 \n      11       12 \n49.42741 45.54245 \n\n\nNotice September (9) and October (10) are the two months with the highest mean wind speeds of \\(54.56\\) knots and \\(51.58\\) knots, respectively. The two means are close, and we only have a subset of population data. Is the observed difference in sample means greater than the margin of error we can expect due to the uncertainty of sampling? We can use a confidence interval to answer this question!\n\nGive a 95% confidence interval to estimate the difference in the mean wind speed of storms in September compared to October?"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#subsetting-into-two-independent-samples",
    "href": "19-Parametric-CI-Means.html#subsetting-into-two-independent-samples",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Subsetting into Two Independent Samples",
    "text": "Subsetting into Two Independent Samples\n\nRun the code cell below to create two data frames, sep.wind and oct.wind, containing separate samples of wind speeds for North Atlantic storms in September and October, respectively. Both sep.wind and oct.wind are data frames with just one variable, wind.\n\n# create a vector of sep wind\nsep.wind &lt;- subset(storms, \n              month == \"9\", \n              select = wind)\n\n# create a vector of oct wind\noct.wind &lt;- subset(storms, \n              month == \"10\", \n              select = wind)"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-13",
    "href": "19-Parametric-CI-Means.html#question-13",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 13",
    "text": "Question 13\n\nAnswer the questions below to construct a 95% confidence interval for the difference in mean wind speeds of North Atlantic storms in September compared to October.\n\nQuestion 13a\n\nCalculate \\(n_s\\) and \\(n_o\\), the number of observations in September and October samples, respectively. Store the results in n.s and n.o.\n\nSolution to Question 13a\n\n\n# calculate n_s\nn.s &lt;- ??\n# calculate n_o\nn.o &lt;- ??\n\n# print to screen\nn.s\nn.o\n\n\n\n\n\n\nQuestion 13b\n\nUsing the sample data in sep.wind and oct.wind, give a point estimate for the difference in population means, \\(\\mu_s - \\mu_o\\). Store the result to point.est\n\nSolution to Question 13b\n\n\n# calculate point estimate\npoint.est &lt;- ??\npoint.est  # print to screen\n\n\n\n\n\n\nQuestion 13c\n\nUsing a \\(t\\)-distribution, calculate the margin of error of a 95% confidence interval for the difference in means.\n\n\n\n\n\n\nTip\n\n\n\nUse the sd() function to calculate each sample standard deviation. Use the qt() function to identify \\(t_{\\alpha/2}\\) using the informal count for the degrees of freedom.\n\n\n\nSolution to Question 13c\n\n\n# use code cell to construct a 95% confidence interval\n\n\nBased on the output above, a 95% confidence interval is from ?? to ??.\n\n\n\n\n\nQuestion 13d\n\nInterpret the meaning of your interval in Question 13c. In particular, can we conclude that wind speeds are on average greater in one month or the other?\n\nSolution to Question 13d"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-14",
    "href": "19-Parametric-CI-Means.html#question-14",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 14",
    "text": "Question 14\n\nUse the t.test() function with the samples sep.wind and oct.wind to construct a 95% confidence interval for the difference in mean wind speeds of North Atlantic storms in September compared to October. How does your answer compare to your confidence interval in Question 13c? If the two answers are different, which confidence interval do you believe is more accurate?\n\nSolution to Question 14\n\nComplete the command below and then compare result to answer in Question 13c.\n\nt.test(??, ??, conf.level = ??)$conf.int"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#subsetting-with-t.test",
    "href": "19-Parametric-CI-Means.html#subsetting-with-t.test",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Subsetting with t.test",
    "text": "Subsetting with t.test\n\nFrequently, we would like to compare the distributions of a quantitative variable, denoted quant, for two different groups based on a categorical variable in the data set, denoted categorical. If our sample data is stored in a data frame called data.name with this structure, we can use t.test() without having to first split the sample into independent samples according to category group. t.test() can subset the data into independent samples for us and construct a confidence interval for the difference in two means. Using the t.test() function\n\nt.test(quant ~ categorical, data = data.name, conf.level = 0.95)$conf.int\n\n will give a 95% confidence level for the difference in means of a specified quantitative variable between the two different groups of the categorical variable."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-15",
    "href": "19-Parametric-CI-Means.html#question-15",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 15",
    "text": "Question 15\n\nThe code cell below subsets the sample data for all months in storms to a new data frame named pooled that:\n\nIncludes only storm observations from September or October.\nSelects only two variables, wind and month, to keep from storms.\nThe first six rows of the data frame pooled are printed to the screen with head(pooled).\nRun the code cell below and inspect the first six rows of pooled.\n\n\npooled &lt;- subset(storms,\n               month == \"9\" | month == \"10\",   # month is 9 OR month is 10\n               select = c(wind, month))  # select wind and month variables\n\nhead(pooled)  # print first 6 rows of pooled to screen\n\n# A tibble: 6 × 2\n   wind month\n  &lt;int&gt; &lt;fct&gt;\n1    30 9    \n2    20 9    \n3    20 9    \n4    75 9    \n5    75 9    \n6    75 9    \n\n\nUse the t.test() function with the pooled sample pooled to construct a 95% confidence interval for the difference in mean wind speeds of North Atlantic storms in September compared to October. How does your answer compare to your confidence intervals in Question 13c and Question 14? If the intervals are different, which confidence interval do you believe is most accurate?\n\nSolution to Question 15\n\nComplete the command below and then compare the result to your answers in Question 13c and Question 14.\n\nt.test(?? ~ ??, data = ??, conf.level = ??)$conf.int"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#caution-using-t.test-for-a-difference-in-two-means",
    "href": "19-Parametric-CI-Means.html#caution-using-t.test-for-a-difference-in-two-means",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Caution Using t.test() for a Difference in Two Means",
    "text": "Caution Using t.test() for a Difference in Two Means\n\nThe variable month in the original sample storms has observations from 10 different months. If you try running the command t.test(wind ~ month, data = storms, conf.level = 0.95)$conf.int you will receive an error since the categorical variable month has more than two classes. The categorical variable used to split the data must have exactly 2 different classes. In solving Question 15 we avoided this error by:\n\nFirst creating the pooled sample that included only two months, September and October.\nThen using t.test() with the pooled sample instead of the full storms data set.\n\nOtherwise, we can split the original sample into two independent samples x and y and use the t.test() function as we did in Question 14 with independent wind speed samples sep.wind and oct.wind.\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html",
    "href": "20-Parametric-CI-Proportions.html",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "",
    "text": "Public Opinion Polls\nConfidence intervals are frequently used when polling public opinion. Rather than give a point estimate alone, poll results are typically given along with a margin of error corresponding to a specified confidence level, which is typically 95%. For example, summarized in the bar plot and table below are the results of a PBS NewsHour/NPR/Marist poll1 that surveyed \\({\\color{tomato}{n=1,\\!227}}\\) randomly selected adults in the US and gauged their opinions on how the US is handling the COVID pandemic approximately one year after the initial outbreak in the United States.\nA confidence interval is an interval estimate for a population parameter with a rate of success given by the confidence level of the interval. We can construct confidence intervals for all sorts of statistics, and we can use confidence intervals as a tool for analyzing possible associations between two different variables. In general, a confidence interval has three components:\nIf we want to construct a confidence interval estimate for parameter \\(\\theta\\), then we have\n\\[({\\color{dodgerblue}{\\mbox{point estimate}}}) - {\\color{tomato}{\\mbox{MoE}}} &lt; \\theta &lt; ({\\color{dodgerblue}{\\mbox{point estimate}}}) + {\\color{tomato}{\\mbox{MoE}}}.\\]\nAll confidence intervals have this same general structure that we can construct using similar steps:\nThe Wald confidence interval for a proportion is given by\n\\[{\\large \\boxed{ \\mbox{Wald:} \\qquad {\\color{tomato}{\\hat{p}}} - z_{\\alpha/2} \\cdot \\sqrt{ \\frac{{\\color{tomato}{\\hat{p}}}(1-{\\color{tomato}{\\hat{p}}})}{n}}  &lt; p &lt;  {\\color{tomato}{\\hat{p}}} + z_{\\alpha/2} \\cdot \\sqrt{ \\frac{{\\color{tomato}{\\hat{p}}}(1-{\\color{tomato}{\\hat{p}}})}{n}}}}\\]\nWe use the plug-in principle and use \\(\\hat{p}\\) for the unknown value of \\(p\\) when calculating the standard error.\nWhen constructing a Wald confidence interval for a proportion, we use the sample proportion \\(\\hat{p}\\) as an estimator for \\(p\\). The sample proportion is a very reasonable and intuitive point estimate. In addition, \\(\\hat{p}\\) is an unbiased estimator for \\(p\\). However, there are many other estimators that could make sense to use in place of the parameter \\(p\\), and there are other properties of estimators we should take into account. In particular, using a less precise estimator such as \\(\\hat{p}\\) results in a larger margin of error in the confidence interval compared to some other estimators.\nWhen studying properties of estimators, we considered another estimator for the parameter \\(p\\), namely \\({\\color{tomato}{\\tilde{p} = \\frac{X+2}{n+4}}}\\), where \\(X\\) denotes the number of “successes”. Comparing estimators \\(\\hat{p}\\) and \\(\\tilde{p}\\), we discovered that:\nThe Agresti-Coull confidence interval for a proportion is\n\\[{\\large \\boxed{ \\mbox{Agresti-Coull:} \\qquad {\\color{tomato}{\\tilde{p}}} - z_{\\alpha/2} \\left( \\sqrt{ \\frac{{\\color{tomato}{\\tilde{p}}}(1-{\\color{tomato}{\\tilde{p}}})}{{\\color{dodgerblue}{n+4}}}} \\right) &lt; p &lt;  {\\color{tomato}{\\tilde{p}}} + z_{\\alpha/2} \\left( \\sqrt{ \\frac{{\\color{tomato}{\\tilde{p}}}(1-{\\color{tomato}{\\tilde{p}}})}{{\\color{dodgerblue}{n+4}}}} \\right)}}.\\]\nFrom the CLT for proportions, we have \\(\\widehat{P} \\sim N \\left( \\mu_{\\widehat{P}} , \\sigma_{\\widehat{P}} \\right) = N\\left( p, \\sqrt{\\frac{ p (1-p)}{n}} \\right)\\). A standardized sample proportion has \\(z\\)-score \\(z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{ p (1-p)}{n}}}\\). Thus, for confidence level \\(CL\\), we have\n\\[P \\left( -z_{\\alpha/2} &lt; \\frac{\\hat{p} -\\color{tomato}{p}}{\\sqrt{\\frac{\\color{tomato}{p}(1-\\color{tomato}{p})}{n}}} &lt; z_{\\alpha/2} \\right) =CL.\\]\nIn the equation above, the unknown population parameter \\(p\\) is in red. All the other values (\\(\\hat{p}\\), \\(n\\), and \\(z_{\\alpha/2}\\)) in the formula are known values. Given a confidence level, we can algebraically solve for the cutoff values for \\({\\color{tomato}{p}}\\) by solving the equations:\n\\[\\dfrac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} = z_{\\alpha/2} \\qquad \\mbox{and} \\qquad \\dfrac{\\hat{p} - {\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p})}}{n}}} = -z_{\\alpha/2}\\]"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-1",
    "href": "20-Parametric-CI-Proportions.html#question-1",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 1",
    "text": "Question 1\n\nBased on the poll summaries above, approximately what proportion of ALL adults in the US do NOT plan to get vaccinated?\n\nSolution to Question 1"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-2",
    "href": "20-Parametric-CI-Proportions.html#question-2",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 2",
    "text": "Question 2\n\nWe would like to estimate the parameter \\(p\\), the proportion of all adults in the US that do not plan on getting vaccinated. From the vaccination poll in Question 1, we have one random sample of 1,227 adults. From our sample, we observe that 30% said they do not intend to get vaccinated. Let’s apply the same general process summarized above to construct a 95% confidence interval to estimate the proportion of all adults in the US that do not plan on getting vaccinated.\n\nQuestion 2a\n\nBased on the sample of polled adults, what is a reasonable point estimate for \\(p\\), the proportion of all adults in the US that do not plan on getting vaccinated?\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nUsing the Central Limit Theorem for proportions, estimate the standard error for the sampling distribution of sample proportions.\n\n\n\n\n\n\nTip\n\n\n\nTo calculate the standard error, we need to know the population proportion \\(p\\). Plug an appropriate sample statistic in place of \\(p\\) to estimate the standard error.\n\n\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nNext, we identify the value (\\(z_{\\alpha/2}\\) or \\(t_{\\alpha/2}\\)) we multiply the standard error by to get the margin of error. For proportions, as long as the the sample is large enough, a normal distribution is an accurate model for the underlying sampling distribution.\nFrom the poll in Question 1, we have \\(n=1227\\). Since we do not know \\(p\\), we substitute \\(\\hat{p} = 0.3\\) instead. Since both \\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\), we can use a normal distribution to calculate the margin of error for the confidence interval.\nWhat is the value of \\(z_{\\alpha/2}\\) for a 95% confidence interval for a proportion?\n\nSolution to Question 2c\n\n\n\n\n\n\n\nQuestion 2d\n\nBased on your previous answers in Question 2, give a 95% confidence interval to estimate \\(p\\), the proportion of all adults in the US that do not plan on getting vaccinated.\n\nSolution to Question 2d\n\n\n\n\n\n\n\nQuestion 2e\n\nInterpret the practical meaning of your confidence interval in Question 2d in the context of COVID vaccinations in the US.\n\nSolution to Question 2e"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-3",
    "href": "20-Parametric-CI-Proportions.html#question-3",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 3",
    "text": "Question 3\n\nUsing the same poll from Question 1, find a 95% confidence interval for the proportion of all adults in the US that do not plan to get vaccinated using the Agresti-Coull confidence interval for a proportion.\n\nSolution to Question 3\n\n\n# use code to help with the calculations"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#score-confidence-interval-formulas",
    "href": "20-Parametric-CI-Proportions.html#score-confidence-interval-formulas",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Score Confidence Interval Formulas",
    "text": "Score Confidence Interval Formulas\n\nThe confidence interval estimate resulting from the algebraic solution is called the score confidence interval for a proportion. The algebraic work involved in solving the equations above is provided in the Appendix. The corresponding lower (\\(L\\)) and upper (\\(U\\)) cutoffs are:\n\\[\\begin{aligned}\n&L= \\dfrac{\\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}} \\\\\n\\\\\n&U= \\dfrac{\\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} + z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}}\n\\end{aligned}\\]\n\nPro: There is no additional uncertainty beyond the initial variability in sampling.\nCon: The formulas are quite complicated. Calculating by hand is not really practical.\nTypically we use technology to calculate score confidence intervals."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#score-confidence-intervals-in-r",
    "href": "20-Parametric-CI-Proportions.html#score-confidence-intervals-in-r",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Score Confidence Intervals in R",
    "text": "Score Confidence Intervals in R\n\nR has a built in function prop.test()$conf.int that calculates a score confidence interval for a proportion.\n\nIn R, use the command prop.test(X, n, conf.level = CL, correct = FALSE)$conf.int\n\n\\(X\\) denotes the number of “successes” observed in the sample.\n\\(n\\) denotes the total number of observations in the sample.\nCL is a chosen confidence level (as a proportion).\nThe option correct = FALSE means no continuity correction is applied."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-4",
    "href": "20-Parametric-CI-Proportions.html#question-4",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 4",
    "text": "Question 4\n\nUsing the poll data in Question 1, find a 95% score confidence interval for the proportion of all adults in the US that do not plan to get vaccinated by completing the prop.test() command in the code cell below.\n\nSolution to Question 4\n\n\nprop.test(??, ??, conf.level = ??, correct = FALSE)$conf.int\n\n\n\n\nChecking Your Solution to Question 4\n\nBased on the polling sample data, enter the values for X, n, and z.star in the first code cell below. Then run the code cell.\n\n##################################################\n# Replace the ?? in the three lines of code below\n# with appropriate values or commands\n##################################################\nX &lt;- ??  # number of successes in sample (do not plan to get vax)\nn &lt;- ??  # sample size\nz.star &lt;- ??  # find z_alpha/2 for 95% confidence level\n\nNext, run the code cell below to calculate the upper and lower cutoffs for a 95% score confidence interval for a proportion.\n\n#########################################\n# first run the code cell above\n# nothing to edit in this code cell\n# run as is\n#########################################\nphat &lt;- X/n  # Compute sample proportion\n\n# Computes Cutoffs for Score Confidence Interval\nlower.score95 &lt;- (phat+z.star^2/(2*n) - \n                z.star*sqrt( (phat*(1-phat))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\nupper.score95 &lt;- (phat+z.star^2/(2*n) + \n                z.star*sqrt( (phat*(1-phat))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\n\n# Print cutoffs to screen\nlower.score95\nupper.score95"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#applying-the-continuity-correction",
    "href": "20-Parametric-CI-Proportions.html#applying-the-continuity-correction",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Applying the Continuity Correction",
    "text": "Applying the Continuity Correction\n\nIn our construction of a score confidence interval, we have used a normal distribution to estimate a discrete (binomial) distribution. Recall when using a continuous, normal distribution to approximate a discrete, binomial distribution (as with the Central Limit Theorem for proportions), we miss some area under the curve resulting in an underestimate. We can improve estimates resulting from using a normal distribution instead of a binomial distribution by applying a continuity correction.\nSimilarly, we can obtain more a more accurate score confidence interval for a proportion by applying a continuity correction. The Appendix explains how the continuity correction is applied and provides the corresponding formulas. In practice, we can simply change the correct = FALSE option in prop.test()$conf.int to correct = TRUE.\n\nprop.test(X, n, conf.level = CL, correct = TRUE)$conf.int\nThe default for prop.test if no correct option is specified is correct = TRUE.\nApplying the continuity correction results in a more precise confidence interval.\n\n\nApplying the Continuity Correction in Code\n\nBelow we perform the direct calculations using the continuity correction formulas derived in the Appendix.\n\n##############################################\n# Be sure you have run previous code cells\n# And have already defined X, n, and z.star\n# Run this code cell without any edits needed\n##############################################\n\n# Continuity corrections applied to sample proportion\ncc.phat.L &lt;- (X - 0.5)/n\ncc.phat.U &lt;- (X + 0.5)/n\n\n# Plugged into formulas for Score Conf Interval\ncc.lower &lt;- (cc.phat.L + z.star^2/(2*n) - \n            z.star*sqrt( (cc.phat.L*(1-cc.phat.L))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\ncc.upper &lt;- (cc.phat.U + z.star^2/(2*n) + \n            z.star*sqrt( (cc.phat.U*(1-cc.phat.U))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\n\n# Print results to screen to check\ncc.lower\ncc.upper\n\nIn the code cell below, we apply the continuity correction using the correct = TRUE option in prop.test() to compare with the previous result.\n\nprop.test(368, 1227, conf.level = 0.95, correct = TRUE)$conf.int"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#central-limit-theorem-for-widehatp_1---widehatp_2",
    "href": "20-Parametric-CI-Proportions.html#central-limit-theorem-for-widehatp_1---widehatp_2",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Central Limit Theorem for \\(\\widehat{P}_1 - \\widehat{P}_2\\)",
    "text": "Central Limit Theorem for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nFor a difference in two proportions, we can derive a Central Limit Theorem to model the sampling distribution for the difference in two sample proportions, \\(\\widehat{P}_1 - \\widehat{P}_2\\). See the Appendix for a proof of the CLT for a difference in two proportions which is stated below:\n\\[\\widehat{P}_1 - \\widehat{P}_2  \\sim N \\left( \\mu_{\\widehat{P}_1 - \\widehat{P}_2} , \\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) \\right) = N \\left( p_1 - p_2  , \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\right).\\]"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#confidence-interval-for-widehatp_1---widehatp_2",
    "href": "20-Parametric-CI-Proportions.html#confidence-interval-for-widehatp_1---widehatp_2",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Confidence Interval for \\(\\widehat{P}_1 - \\widehat{P}_2\\)",
    "text": "Confidence Interval for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nWe can modify the Wald confidence interval to give an approximation for a confidence interval for a difference in two proportions\n\nThe point estimate is the difference in the two sample proportions, \\(\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}\\).\nThe standard error we estimate by plugging \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\) in place of \\(p_1\\) and \\(p_2\\) in the formula for the standard error from the Central Limit Theorem:\n\n\\[\\mbox{SE} \\left( \\widehat{P}_1 - \\widehat{P}_2 \\right) = \\sqrt{ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\approx \\sqrt{ \\frac{{\\color{mediumseagreen}{\\hat{p}_1}}(1-{\\color{mediumseagreen}{\\hat{p}_1}})}{n_1} + \\frac{{\\color{mediumseagreen}{\\hat{p}_2}}(1-{\\color{mediumseagreen}{\\hat{p}_2}})}{n_2}}\\]\n\nWe use the standard normal distribution to identify \\(z_{\\alpha/2}\\) to find the margin of error.\n\n\\[({\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}}) - {\\color{tomato}{z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\dfrac{\\hat{p}_2 (1-\\hat{p}_2) }{n_2}}}}  &lt; p_1-p_2 &lt; ({\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}}) + {\\color{tomato}{z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\dfrac{\\hat{p}_2 (1-\\hat{p}_2) }{n_2}}}}.\\]\nWe use Wald confidence intervals for a difference in two proportions, but be aware there are other variations of confidence intervals for a difference in two proportions, similar to the Agresti-Coull and score confidence intervals. In R, the command\n\nprop.test(c(x1, x2), c(n1, n2), conf.level = CL, correct = TRUE)$conf.int\n\n computes a Wald confidence interval for a difference in two proportions with a continuity correction applied.\n\n\n\n\n\n\nNote\n\n\n\nIn R, the prop.test() function uses different methods depending on whether the confidence interval is for a single proportion or a difference in two proportions.\n\nFor a single proportion, prop.test() gives a score confidence interval.\nFor a difference in two proportions, prop.test() gives a Wald confidence interval."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-5",
    "href": "20-Parametric-CI-Proportions.html#question-5",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 5",
    "text": "Question 5\n\nUsing the data below collected from the poll in Question 1, construct a 90% Wald confidence interval for the difference in the proportion of all Democrats and the proportion of all Republicans that do not plan to be vaccinated.\n\n\n\nParty\nYes, will\nYes, already\nNo\nUnsure\nTotal\n\n\n\n\nDemocrat\n213\n108\n40\n7\n368\n\n\nRepublican\n93\n70\n120\n9\n292\n\n\nTotal\n306\n178\n160\n16\n660\n\n\n\n\nSolution to Question 5\n\n\n# use code cell to help"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-samp-size",
    "href": "20-Parametric-CI-Proportions.html#sec-samp-size",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "A Note About Sample Sizes",
    "text": "A Note About Sample Sizes\n\n\nFor a single mean, we can use the CTL to construct a parametric confidence interval as long as:\n\nEither the population is symmetric or \\(n \\geq 30\\).\nIf the sample is symmetric, we can assume the population is symmetric.\n\nFor a difference in two means , we can use the CTL to construct a parametric confidence interval as long as:\n\nPopulation 1 is either symmetric or \\(n_1 \\geq 30\\), and\nPopulation 2 is either symmetric or \\(n_2 \\geq 30\\).\n\nFor a single proportion, we can use the CTL to construct a parametric confidence interval as long as:\n\nBoth \\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\).\n\nFor a difference in two proportions, we can use the CTL to construct a parametric confidence interval as long as:\n\nAll of \\(n_1\\hat{p}_1 \\geq 10\\), \\(n_1(1-\\hat{p}_1) \\geq 10\\), \\(n_2\\hat{p}_2 \\geq 10\\), and \\(n_2(1-\\hat{p}_2) \\geq 10\\) are satisfied."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#useful-r-functions",
    "href": "20-Parametric-CI-Proportions.html#useful-r-functions",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Useful R Functions",
    "text": "Useful R Functions\n\nIn R, we have the functions:\n\nt.test()$conf.int constructs a \\(t\\)-confidence interval for a single or difference in two means.\nprop.test()$conf.int constructs a score confidence interval for a single proportion.\nprop.test()$conf.int constructs a Wald confidence interval for a difference in two proportions."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-score",
    "href": "20-Parametric-CI-Proportions.html#sec-score",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Deriving the Score Confidence Interval Formulas",
    "text": "Deriving the Score Confidence Interval Formulas\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) and consider the distribution of sample proportions, \\(\\widehat{P} = \\frac{X}{n}\\). From the CLT for proportions we know \\(\\widehat{P} \\sim N \\left( p, \\sqrt{\\frac{p(1-p)}{n}} \\right)\\). Thus, for confidence level CL, we have\n\\[P(-z_{\\alpha/2}&lt; Z &lt; z_{\\alpha/2}) = P \\left( -z_{\\alpha/2} &lt; \\frac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} &lt; z_{\\alpha/2} \\right) =CL.\\]\nThe upper cutoff, \\(U\\) is a value for \\({\\color{tomato}{p}}\\) such that\n\\[\\dfrac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} = z_{\\alpha/2}.\\]\nTo solve for \\({\\color{tomato}{p}}\\), we multiply both sides of the equation above by \\(\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}\\) and then square both sides giving\n\\[\\big( \\hat{p} - {\\color{tomato}{p}} \\big)^2 = (z_{\\alpha/2})^2 \\left( \\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n} \\right).\\]\nNext we distribute terms on both sides of the equation and have\n\\[\\hat{p}^2 - 2 {\\color{tomato}{p}}\\hat{p} + {\\color{tomato}{p}}^2 = {\\color{tomato}{p}} \\left( \\frac{z_{\\alpha/2}^2}{n} \\right) - {\\color{tomato}{p}}^2 \\left( \\frac{z_{\\alpha/2}^2}{n} \\right).\\]\nWe have a quadratic equation for the unknown \\({\\color{tomato}{p}}\\). We group all like terms together on one side of the equation,\n\\[{\\color{dodgerblue}{\\left( 1+ \\frac{z_{\\alpha/2}^2}{n} \\right)}} p^2 + {\\color{tomato}{\\left(-2\\hat{p}-\\frac{z_{\\alpha/2}^2}{n} \\right)}} p + {\\color{mediumseagreen}{\\hat{p}^2}} = {\\color{dodgerblue}{a}}p^2 + {\\color{tomato}{b}} p + {\\color{mediumseagreen}{c}} = 0.\\] We use the quadratic formula to solve for \\(p\\). The quadratic equation has two real solutions, the larger of the two solution is the upper limit for a 95% score confidence interval\n\\[{\\large \\boxed{ U = \\frac{ \\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} + z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+\\dfrac{z_{\\alpha/2}^2}{n}}}}\\]\nThe smaller of the two solutions is the lower cutoff, \\(L\\)\n\\[{\\large \\boxed{ L = \\frac{ \\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+\\dfrac{z_{\\alpha/2}^2}{n}}}}\\]\nWe also consider the equation\n\\[\\dfrac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} = -z_{\\alpha/2}.\\]\nIf we multiply both sides of the equation above by \\(\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}\\) and then square both sides, we get\n\\[\\big( \\hat{p} - {\\color{tomato}{p}} \\big)^2 = (-z_{\\alpha/2})^2 \\left( \\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n} \\right).\\]\nThe resulting equation is the same as with the first case we solved. Thus, solving the equation above gives the same expressions for \\(U\\) and \\(L\\)."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-score-corr",
    "href": "20-Parametric-CI-Proportions.html#sec-score-corr",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Applying a Continuity Correction",
    "text": "Applying a Continuity Correction\n\nRecall, when using a normal distribution to approximate a discrete, binomial distribution \\(X \\sim \\mbox{Binom}(n,p)\\), we can improve the estimate by using a continuity correction.\nIn the case of a score confidence interval for a proportion, the continuity correction is applied as follows:\n\nIn the formula for the corrected lower cutoff \\(L^*\\), we use the corrected sample proportion \\({\\color{dodgerblue}{\\hat{p}_L^*= \\dfrac{X-0.5}{n}}}\\).\nIn the formula for the corrected upper cutoff \\(U^*\\), we use the corrected sample proportion \\({\\color{tomato}{\\hat{p}_U^* = \\dfrac{X+0.5}{n}}}\\).\n\n\\[\\begin{aligned}\n&L^* = \\dfrac{{\\color{dodgerblue}{\\hat{p}_L^*}} + \\dfrac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{{\\color{dodgerblue}{\\hat{p}_L^*}}(1-{\\color{dodgerblue}{\\hat{p}_L^*}})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}} \\\\\n\\\\\n&U^* = \\dfrac{{\\color{tomato}{\\hat{p}_U^*}} + \\dfrac{z_{\\alpha/2}^2}{2n} + z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{{\\color{tomato}{\\hat{p}_U^*}}(1-{\\color{tomato}{\\hat{p}_U^*}})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-clt-diffprop",
    "href": "20-Parametric-CI-Proportions.html#sec-clt-diffprop",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "CLT: Difference in Two Proportions",
    "text": "CLT: Difference in Two Proportions\n\nLet \\(X_1 \\sim \\mbox{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mbox{Binom}(n_2,p_2)\\) be two independent binomial random variables with distribution of sample proportions \\(\\widehat{P}_1 = \\frac{X_1}{n_1}\\) and \\(\\widehat{P}_2 = \\frac{X_2}{n_2}\\), respectively. As long as both samples are large enough, the sampling distribution for the difference in sample proportions \\(\\widehat{P_1}-\\widehat{P_2}\\) will:\n\nBe approximately normally distributed.\nHave mean \\({\\color{dodgerblue}{E(\\widehat{P_1}-\\widehat{P_2}) = \\mu_{\\widehat{P}_1 - \\widehat{P}_2} =p_1 - p_2}}\\).\nHave standard error \\[{\\color{dodgerblue}{\\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) =  \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}}.\\]\n\nWe summarize the results of the Central Limit Theorem (CLT) for a Difference in Two Proportions more concisely below:\n\\[{\\color{dodgerblue}{\\boxed{ \\widehat{P}_1 - \\widehat{P}_2  \\sim N \\left( \\mu_{\\widehat{P}_1 - \\widehat{P}_2} , \\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) \\right) = N \\left( p_1 - p_2  , \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\right)}}}.\\]\n\nProof of CLT for Difference in Two Proportions\n\nBelow we prove both \\(E(\\widehat{P_1}-\\widehat{P_2}) = \\mu_{\\widehat{P}_1 - \\widehat{P}_2} =p_1 - p_2\\) and \\(\\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\).\n\nMean of Sampling Distribution for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nWe have \\(E (\\widehat{P_1}-\\widehat{P_2}) = E \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right)\\). Using properties of expected value, we have\n\\[E (\\widehat{P_1}-\\widehat{P_2}) = E \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right) = \\frac{1}{n_1} E(X_1) - \\frac{1}{n_2} E(X_2).\\]\nSince \\(X_1 \\sim \\mbox{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mbox{Binom}(n_2,p_2)\\), we know \\({\\color{dodgerblue}{E(X_1) = n_1p_1}}\\) and \\({\\color{tomato}{E(X_2) = n_2p_2}}\\). Thus we have\n\\[E (\\widehat{P_1}-\\widehat{P_2}) = \\frac{1}{n_1} {\\color{dodgerblue}{n_1p_1}} - \\frac{1}{n_2} {\\color{tomato}{n_2p_2}}=p_1-p_2.\\]\nThis concludes the proof that \\(E (\\widehat{P_1}-\\widehat{P_2}) =p_1-p_2\\).\n\n\nStandard Error of Sampling Distribution for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nWe have \\(\\mbox{Var} (\\widehat{P_1}-\\widehat{P_2}) = \\mbox{Var} \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right)\\). \\(X_1\\) and \\(X_2\\) are independent random variables, so we can use properties of variance:\n\\[\\mbox{Var} \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right) = \\left(\\frac{1}{n_1}\\right)^2 \\mbox{Var}(X_1) + \\left(- \\frac{1}{n_2} \\right)^2 \\mbox{Var}(X_2) = \\frac{1}{n_1^2}\\mbox{Var}(X_1) + \\frac{1}{n_2^2}\\mbox{Var}(X_2).\\]\nSince \\(X_1 \\sim \\mbox{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mbox{Binom}(n_2,p_2)\\), we know \\({\\color{dodgerblue}{\\mbox{Var}(X_1) = n_1p_1(1-p_1)}}\\) and \\({\\color{tomato}{\\mbox{Var}(X_2) = n_2p_2(1-p_2)}}\\). Thus we have\n\\[\\mbox{Var} (\\widehat{P_1}-\\widehat{P_2}) = \\frac{1}{n_1^2} {\\color{dodgerblue}{n_1p_1(1-p_1)}} - \\frac{1}{n_2^2} {\\color{tomato}{n_2p_2(1-p_2)}}= \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\\]\nThe standard error of the sampling distribution is the square root of the variance, thus we have\n\\[\\mbox{SE} (\\widehat{P_1}-\\widehat{P_2}) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}.\\]\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#footnotes",
    "href": "20-Parametric-CI-Proportions.html#footnotes",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "",
    "text": "“Politics still drives how Americans fell about COVID response, one year in”, PBS/News Hour, March 11, 2021↩︎"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html",
    "href": "21-Intro-Hypothesis-Tests.html",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "",
    "text": "An Experiment with Telepathy\nWe refer to the two competing hypotheses as the null hypothesis, denoted by \\(\\color{dodgerblue}{H_0}\\), and the alternative hypothesis, denoted by \\(\\color{tomato}{H_a}\\).\nThe general process form performing a hypothesis test is informally:\nWe will dive deeper into each step, but for now we take a tour through the process in the context of two different studies. Before considering our first example, we focus on setting up the hypotheses in Step 1.\nEarlier we informally discussed the concept of significance. Recall the logic of hypothesis tests, we assume \\(H_0\\) is true and consider whether the sample data supports or refutes the null claim. \\(p\\)-values are more formal statistics used to measure significance in hypothesis testing.\nThe \\(\\color{dodgerblue}{\\mathbf{p}}\\)-value is the probability that we would get a random sample with a test statistic as or more extreme as the observed test statistic if the null hypothesis were true.\n\\[{\\large {\\color{dodgerblue}{p\\mbox{-value} = P( \\mbox{test statistic as or more extreme than observed} \\ | \\ H_0 \\mbox{ is true} )}}}.\\]\nThe null distribution is the distribution of the test statistic if the null hypothesis is true. We use the null distribution to calculate the p-value."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-q1",
    "href": "21-Intro-Hypothesis-Tests.html#sec-q1",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 1",
    "text": "Question 1\n\nTelepathy is the ability of an individual to communicate thoughts and ideas by means other than the known senses. I claim that I do have telepathy. There are two possibilities: either I do or I do not. Which claim is the null hypothesis and which is the alternative?\n\nSolution to Question 1"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-q2",
    "href": "21-Intro-Hypothesis-Tests.html#sec-q2",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 2",
    "text": "Question 2\n\nThere are many different studies we could design to test these competing claims. I will collect data as as follows:\n\nI will think of a letter A, B, C, or D.\nI will telepathically communicate this letter to everyone.\nEach person will tell me what letter they believed I was thinking of.\n\nIf I could collect responses from everyone in the population, I could calculate the parameter \\(p\\), the proportion of all responses from everyone in the world that are the correct letter.\n\nIf \\(H_0\\) is true, what would you expect the value of \\(p\\) to be?\nIf \\(H_a\\) is true, what would you expect the value of \\(p\\) to be?\n\n\nSolution to Question 2"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-q3",
    "href": "21-Intro-Hypothesis-Tests.html#sec-q3",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 3",
    "text": "Question 3\n\nI cannot conduct my study on everyone in the world. Instead, I pick a sample of people from which to collect data. Suppose I am in a room with 15 other people who will participate in the study as designed in Question 2. Let \\(\\hat{p}\\) denote the proportion of the people in the room that correctly say the letter I was thinking of.\n\nWhat would be enough evidence to convince you that I do have telepathy?\n\nState your answer in terms of the value of the sample proportion \\(\\hat{p}\\).\n\nSolution to Question 3"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#stating-the-hypotheses",
    "href": "21-Intro-Hypothesis-Tests.html#stating-the-hypotheses",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Stating the Hypotheses",
    "text": "Stating the Hypotheses\n\nDeciding on a clear and pertinent set of hypotheses is perhaps the most important step in hypothesis testing. Carefully setting up hypotheses helps keep our analysis on track so we achieve our goal.\n\nThe Null and Alternative Hypotheses\n\n\nThe null hypothesis is a “boring” claim. If the null hypothesis is true, this would not really affect or change what is currently believed.\nThe alternative hypothesis is the new or exciting claim a researcher is hoping to establish that is in direct competition with the boring claim in the null hypothesis.\n\nFor example, we generally believe coins are “fair” in the sense that when you flip a coin it has a 50% chance of landing on heads and a 50% chance of landing on tails. It would be more interesting if somebody claimed that a coin was not fair.\n\nTwo-Tailed Tests\n\nLet \\(p\\) denote the proportion of all flips of a coin that land on heads. If our goal is to show a coin is not fair, without indicating a direction of the bias, we could set up hypotheses\n\n\\(H_0\\): \\(p=0.5\\). The coin is fair (boring).\n\\(H_a\\): \\({\\color{tomato}{p \\ne 0.5}}\\). The coin is not fair (what we want to prove).\n\nIf we claim a coin is not fair without specifying a direction of the bias, we use a \\({\\color{tomato}{\\ne}}\\) sign in \\(H_a\\), and we say the test is a two-tailed test. There are other ways we could claim a coin is biased.\n\n\nOne-Tailed Tests\n\nA coin might be more biased to land on heads (\\(p &gt; 0.5\\)). A coin might be more biased to land on tails (\\(p &lt; 0.5\\)).\n\nIf we want to test the claim that a coin is biased to land on heads:\n\n\\(H_0\\): \\(p=0.5\\). The coin is fair (boring).\n\\(H_a\\): \\({\\color{tomato}{p &gt; 0.5}}\\). The coin is biased to heads (what we want to prove).\n\nIf we want to test the claim that a coin is biased to land on tails:\n\n\\(H_0\\): \\(p=0.5\\). The coin is fair (boring).\n\\(H_a\\): \\({\\color{tomato}{p &lt; 0.5}}\\). The coin is biased to tails (what we want to prove).\n\n\nWhen we specify a direction of the inequality we use either \\(&gt;\\) or \\(&lt;\\) in \\(H_a\\), and we say the test is a one-tailed test.\n\n\n\nGuidelines for Stating Hypotheses\n\nFrom the examples above, we note some helpful guidelines for setting up hypotheses:\n\nWe state hypotheses both in words and using mathematical notation.\n\nClearly stating the hypothesis in words will help us interpret results.\nUsing appropriate mathematical notation will help guide our statistical analysis.\n\nThe hypotheses must be competing claims.\nBe sure to clearly indicate the population of interest.\nWhen using notation to state the hypotheses:\n\nUse population parameters such as \\(\\mu\\) and \\(p\\).\nDo not state hypotheses using statistics such as \\(\\bar{x}\\) or \\(\\hat{p}\\).\nWe always use an equality sign, \\(=\\), in \\(H_0\\).\nWe always use an inequality sign (\\(\\ne\\), \\(&gt;\\), or \\(&lt;\\)) in \\(H_a\\)."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#computing-a-test-statistic",
    "href": "21-Intro-Hypothesis-Tests.html#computing-a-test-statistic",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Computing a Test Statistic",
    "text": "Computing a Test Statistic\n\nStating our hypotheses in mathematical notation is extremely helpful in helping us decide which sample statistics will be most helpful in assessing our hypotheses. Essentially, based on the population parameters we use in \\(H_0\\) and \\(H_a\\), we use the corresponding statistic(s) from the sample(s). For example:\n\nIf we want to test claims about \\(p\\), calculating \\(\\hat{p}\\) will be useful.\nIf we want to test claims about \\(\\mu_1 - \\mu_2\\), calculating \\(\\bar{x}_1-\\bar{x}_2\\) will be useful.\n\nOften we standardize statistics, for example using \\(z\\)-scores. We will explore this idea more deeply later."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-informal-p",
    "href": "21-Intro-Hypothesis-Tests.html#sec-informal-p",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Assess Which Claim is More Likely",
    "text": "Assess Which Claim is More Likely\n\nWhen we set up hypotheses, we use a boring claim for the null hypothesis the that is generally assumed to be true. The alternative hypothesis is a new claim, if true, would cause us to doubt and possibly reject the null claim.\n\nOur goal is not to prove anything about the null claim \\(H_0\\).\nOur goal is to show \\(H_a\\) is very likely true by showing \\(H_0\\) is very unlikely to be true.\n\nThe logic in hypothesis testing is similar to a proof technique called proof by contradiction.\n\nWe assume the boring claim \\(H_0\\) is true.\nWe hope to prove our assumption is unlikely, and thus the opposite (\\(H_a)\\) is true.\n\nBased on our test statistic(s) and whether the alternative hypothesis is two-tailed or one-tailed, we assess statistical significance as follows:\n\nIf the test statistic is likely if \\(H_0\\) is true, then we do not have evidence to reject \\(H_0\\).\n\nThe test is not statistically significant.\nThe test is inconclusive.\nWe neither reject nor accept \\(H_0\\).\n\nIf the test statistic is unlikely if \\(H_0\\) is true in the direction of the claim in \\(H_a\\), then we have evidence that refutes \\(H_0\\) and supports \\(H_a\\).\n\nThe test is statistically significant.\nWe reject \\(H_0\\) and accept \\(H_a\\).\n\n\nWhat do we mean be “likely” and “unlikely”? We consider the concept informally at first, and we will explore this concept more deeply later using p-values."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#making-a-conclusion",
    "href": "21-Intro-Hypothesis-Tests.html#making-a-conclusion",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Making a Conclusion",
    "text": "Making a Conclusion\n\nIn the end, there are two possible results from a hypothesis test. Either:\n\nThe test is statistically significant.\n\nWe reject \\(H_0\\).\nWe accept the competing claim, \\(H_a\\).\n\nThe test is not statistically significant.\n\nWe fail to reject \\(H_0\\). We do NOT accept \\(H_0\\).\nThe test is inconclusive as far as \\(H_a\\) is considered,\n\n\nWe will more formally discuss just how unlikely or likely a sample must be in order to for the result to be statistically significant. Informally, we assume the null hypothesis is true and require evidence beyond a reasonable doubt that \\(\\mathbf{H_0}\\) is not true in order to be convinced we should reject \\(H_0\\).\nWe do not want different researchers subjectively choosing what “beyond a reasonable doubt” means to them. We hope to control for confounding variables in our studies, and making this decision as objective as possible helps account for potential bias by the researchers. In statistical hypothesis testing, a significance level is used to make this decision."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-diner",
    "href": "21-Intro-Hypothesis-Tests.html#sec-diner",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 4: Unscrupulous Diner’s Dilemma",
    "text": "Question 4: Unscrupulous Diner’s Dilemma\n\n\n\n\nCredit: Benreis, CC BY 3.0, via Wikimedia Commons\n\n\nAn article1 from The Economic Journal studied the so called unscrupulous diner’s dilemma.\n\nThe unscrupulous diner’s dilemma is a problem faced frequently in social settings. When a group of diners jointly enjoys a meal at a restaurant, often an unspoken agreement exists to divide the check equally. A selfish diner could thereby enjoy exceptional dinners at bargain prices. This dilemma typifies a class of serious social problems2 from environmental protection and resource conservation to eliciting charity donations and slowing arms races.\n\nResearchers wanted to test whether people order more food and beverages when they know the bill is going to split evenly, or do they order the same amount regardless of whether they are splitting the bill or paying individually.\n\nQuestion 4a\n\nState the null and alternative hypotheses both in words and using mathematical notation.\n\nSolution to Question 4a\n\n\n\n\n\n\n\nQuestion 4b\n\nTo test the claims, participants were randomly assigned into two tables, each with four people. One table (even-split group) was randomly picked and told they were going to evenly-split the bill. The other table (control) was told each person was going to pay for what they ordered. The mean amount ordered by the control group was \\(\\$8.67\\). Which of following samples for the even-split group is the most statistically significant? Support your answer with an explanation.\n\n\n\nTest Stat 1\nTest Stat 2\nTest Stat 3\nTest Stat 4\n\n\n\n\n\\(\\$4.67\\)\n\\(\\$8.50\\)\n\\(\\$8.80\\)\n\\(\\$11.23\\)\n\n\n\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nIf the table below gives the amounts ordered by each of the four people in each group, what is the value of the test statistic? Complete the R code cell below to calculate a test statistic.\n\nEven-Split Group\n\n\n\nPerson 1\nPerson 2\nPerson 3\nPerson 4\n\n\n\n\n\\(\\$15.00\\)\n\\(\\$8.00\\)\n\\(\\$8.75\\)\n\\(\\$13.17\\)\n\n\n\n\n\nControl Group\n\n\n\nPerson 1\nPerson 2\nPerson 3\nPerson 4\n\n\n\n\n\\(\\$8.50\\)\n\\(\\$7.90\\)\n\\(\\$10.85\\)\n\\(\\$7.43\\)\n\n\n\n\n\nSolution to Question 4c\n\n\n# enter each sample as a vector below\neven &lt;- c(15, 8, 8.75, 13.17)   # even-split data\ncontrol &lt;- c(??)  # control data\n\n# calculate an appropriate test statistic\n\n\n\n\n\n\nQuestion 4d\n\nBased on the test statistic, what do you think we can conclude about the two competing claims?\n\nSolution to Question 4d"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#question-5-social-pressure-and-voter-turnout",
    "href": "21-Intro-Hypothesis-Tests.html#question-5-social-pressure-and-voter-turnout",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 5: Social Pressure and Voter Turnout",
    "text": "Question 5: Social Pressure and Voter Turnout\n\nA 2008 experiment3 at Yale aimed to determine whether positive or negative pressure is more effective at improving voter turnout.\n\nVoter turnout theories based on rational self-interested behavior generally fail to predict significant turnout unless they account for the utility that citizens receive from performing their civic duty. We distinguish between two aspects of this type of utility,\n\nOne group received a mailing emphasizing the intrinsic (internal) satisfaction for voting:\n\n\n\nIntrinsic Mailing\n\n\nAnother group received a mailing placing extrinsic (outside) pressure on people to vote:\n\n\n\nExtrinsic Mailing\n\n\n\nQuestion 5a\n\nState the null and alternative hypotheses the researches can use to test whether positive or negative pressure is more effective at improving voter turnout. State the hypotheses both in words and using mathematical notation.\n\nSolution to Question 5a\n\n\n\n\n\n\n\nQuestion 5b\n\nWhat is a possible test statistic the researchers could use to assess the competing claims in your previous answer? There is not specific value to give. Rather, explain how you could use sample data to compute a test statistic.\n\nSolution to Question 5b"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#question-6",
    "href": "21-Intro-Hypothesis-Tests.html#question-6",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 6",
    "text": "Question 6\n\nRecall the telepathy example in Questions 1, 2, and 3. Let \\(T\\) be the number of people (out of the possible 15 people in the room) that said the correct the letter I was thinking. Suppose we observed that 12 out of 15 people correctly said the letter I was thinking.\n\nQuestion 6a\n\nCalculate the \\(p\\)-value of the observed test statistic. Use the R code cell below to help with the calculation.\n\n\n\n\n\n\nTip\n\n\n\nIn 15 identical and independent trials under the assumption \\(H_0\\) is true, what is \\(P(T \\geq 12)\\)?\n\n\n\nSolution to Question 6a\n\n\n# p-value of the observed 12 out 15 successes\n\n\n\n\n\n\n\nQuestion 6b\n\nBased on the value of the \\(p\\)-value, what can we conclude about my telepathy ability?\n\nSolution to Question 6b"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#question-7",
    "href": "21-Intro-Hypothesis-Tests.html#question-7",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 7",
    "text": "Question 7\n\nWhat is the null distribution for the previous telepathy example?\n\nSolution to Question 7\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#footnotes",
    "href": "21-Intro-Hypothesis-Tests.html#footnotes",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "",
    "text": "Gneezy, U., E. Haruvy, and H. Yafe (2004), “The Inefficiency of Splitting the Bill”, The Economic Journal 114↩︎\nGlance, N., and B. Huberman, “The Dynamics of Social Dilemmas”, Scientific American↩︎\nGerber, A., D. Green, and C. Larimer, “Social Pressure and Voter Turnout: Evidence from a Largescale Field Experiment”, American Political Science Review, Feb 2008↩︎"
  },
  {
    "objectID": "22-Permutation-Tests.html",
    "href": "22-Permutation-Tests.html",
    "title": "6.2: Permutation Tests",
    "section": "",
    "text": "A Summary of Hypothesis Testing\nIn the section Introduction to Hypothesis Tests, we walked through the process of performing a statistical hypothesis test:\nSee Introduction to Hypothesis Tests for a refresher on Steps 1 and 2. We also informally explored the concept of statistical significance. Today, we focus on Step 3 and discuss a resampling method, called a permutation test, that we can use to calculate p-values to assess significance.\nWe previously considered the unscrupulous diner’s dilemma1.\nResearchers wanted to test whether people order more food and beverages when they know the bill is going to be split evenly compared to when each person only pays for what they ordered.\nTo perform a two-sample permutation test on data collected from two samples size \\(m\\) and \\(n\\):\nThe p-value is the proportion of times the randomized statistics are as or more extreme than the observed difference.\nSkin is the largest organ in the human body. In the United States, skin cancer is the most common cancer. Current estimates are that one in five Americans will develop skin cancer in their lifetime3. There are different types of skin cancers, and melanoma is one form of skin cancer that is particularly dangerous if not detected early. However, if a skin lesion is detected early, it can be surgically removed before it spreads, and a patient generally has good long-term outcomes.\nThe data set melanoma in the boot package contains measurements from a random sample of \\(205\\) patients with malignant melanoma at the University Hospital of Odense in Denmark. Each patient had a skin lesion (or tumor) surgically removed and various attributes of the patient and tumors are recorded. Run the code cell below to load the boot package and summarize the variables in the melanoma data set.\nlibrary(boot)\nsummary(melanoma)\n\n      time          status          sex              age             year     \n Min.   :  10   Min.   :1.00   Min.   :0.0000   Min.   : 4.00   Min.   :1962  \n 1st Qu.:1525   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1968  \n Median :2005   Median :2.00   Median :0.0000   Median :54.00   Median :1970  \n Mean   :2153   Mean   :1.79   Mean   :0.3854   Mean   :52.46   Mean   :1970  \n 3rd Qu.:3042   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:65.00   3rd Qu.:1972  \n Max.   :5565   Max.   :3.00   Max.   :1.0000   Max.   :95.00   Max.   :1977  \n   thickness         ulcer      \n Min.   : 0.10   Min.   :0.000  \n 1st Qu.: 0.97   1st Qu.:0.000  \n Median : 1.94   Median :0.000  \n Mean   : 2.92   Mean   :0.439  \n 3rd Qu.: 3.56   3rd Qu.:1.000  \n Max.   :17.42   Max.   :1.000\nWe use the permutation distribution as an estimate for the null distribution. The p-value is the proportion of all resampled test statistics that are as or more extreme than the observed test statistic. We can use a logical test to help compute this proportion."
  },
  {
    "objectID": "22-Permutation-Tests.html#the-null-distribution-and-p-values",
    "href": "22-Permutation-Tests.html#the-null-distribution-and-p-values",
    "title": "6.2: Permutation Tests",
    "section": "The Null Distribution and p-values",
    "text": "The Null Distribution and p-values\n\nRecall the logic of hypothesis test. We want to assess which of two competing claims, the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_a\\), are more likely to be true. We assume \\(H_0\\) is true and consider whether the sample data supports or refutes the null claim.\n\nThe null distribution is the distribution of the test statistic if the null hypothesis is true. We use the null distribution to calculate the p-value.\nThe p-value is the probability that we would get a random sample with a test statistic as or more extreme as the observed test statistic if the null hypothesis were true.\n\n\\[{\\large {\\color{dodgerblue}{p\\mbox{-value} = P( \\mbox{test statistic as or more extreme than observed} \\ | \\ H_0 \\mbox{ is true} )}}}.\\]\n\nThe smaller the \\(p\\)-value, the less likely the sample is assuming \\(H_0\\) is true.\nThere is evidence that contradicts \\(H_0\\) and supports \\(H_a\\).\nThe smaller the \\(p\\)-value, the more statistically significant the result."
  },
  {
    "objectID": "22-Permutation-Tests.html#step-1-state-the-hypotheses",
    "href": "22-Permutation-Tests.html#step-1-state-the-hypotheses",
    "title": "6.2: Permutation Tests",
    "section": "Step 1: State the Hypotheses",
    "text": "Step 1: State the Hypotheses\n\n\n\\(H_0\\): There is no difference in how much people order regardless of how the bill is split. \\({\\color{dodgerblue}{\\mu_{\\rm even} - \\mu_{\\rm control}=0}}.\\)\n\\(H_a\\): People order more when the bill is split evenly as opposed to when each person pays for what they order. \\({\\color{dodgerblue}{\\mu_{\\rm even} - \\mu_{\\rm control}&gt;0}}.\\)"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-2-collect-sample-data-and-define-a-test-statistic",
    "href": "22-Permutation-Tests.html#step-2-collect-sample-data-and-define-a-test-statistic",
    "title": "6.2: Permutation Tests",
    "section": "Step 2: Collect Sample Data and Define a Test Statistic",
    "text": "Step 2: Collect Sample Data and Define a Test Statistic\n\n8 people volunteered to take part in the study.\n\n4 people were randomly assigned to sit at a table where they were told the bill would be evenly split between the 4 people.\n4 people were randomly assigned to sit at a table where they were told each person would pay only for what they order themselves.\n\nThe results of the study are given in the tables below:\n\n\n\n\n\n\n\nEven Split Group\nPay for what you order (control) Group\n\n\n\n\n\\(\\$15.00\\), \\(\\$8.00\\), \\(\\$8.75\\), \\(\\$13.17\\)\n\\(\\$8.50\\) , \\(\\$7.90\\) , \\(\\$10.85\\), \\(\\$7.43\\)\n\n\n\\(\\bar{x}_{\\rm even} = \\$11.23\\)\n\\(\\bar{x}_{\\rm control} = \\$8.67\\)\n\n\n\n\nWe can use the difference in the two sample means as a test statistic:\n\n\\[{\\color{dodgerblue}{T= \\bar{x}_{\\rm even} - \\bar{x}_{\\rm control} = 2.56}}\\]"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-3-how-extreme-is-the-observed-difference",
    "href": "22-Permutation-Tests.html#step-3-how-extreme-is-the-observed-difference",
    "title": "6.2: Permutation Tests",
    "section": "Step 3: How Extreme Is the Observed Difference?",
    "text": "Step 3: How Extreme Is the Observed Difference?\n\nThe p-value is the probability of getting a difference in sample means between the even-split and self-pay groups as or more extreme than the observed test statistic, \\(\\bar{x}_{\\rm even} - \\bar{x}_{\\rm control} = 2.56\\). In this case (one-tailed test), more extreme implies a difference that is even bigger than \\(\\$2.56\\).\n\\[{\\color{dodgerblue}{\\mbox{p-value} = P(\\bar{x}_{\\rm even} - \\bar{x}_{\\rm control} \\geq 2.56 \\ | \\ H_0 \\mbox{ is true} )}}\\]\nIf \\(H_0\\) is true, then \\(\\mu_{\\rm even} - \\mu_{\\rm control} =0\\). Under this assumption, the center of the sampling distribution for the difference in sample means would be \\(0\\). However, we do not know how much variability there is due to the sampling. Is a difference in sample means equal to \\(\\$2.56\\) a “big” difference, or is the difference within the margins we would expect due to the uncertainty of sampling?\n\nHow can we calculate the p-value if we do not know the underlying probability distribution for the test statistic \\(T = \\mu_{\\rm{even}} - \\mu_{\\rm{control}}\\) if \\(H_0\\) is true?"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-1",
    "href": "22-Permutation-Tests.html#question-1",
    "title": "6.2: Permutation Tests",
    "section": "Question 1",
    "text": "Question 1\n\nIf people order the same amount of food no matter how the bill is split (assuming \\(H_0\\) is true), we assume each person would order the same amount of food regardless of the table they were seated at. Splitting by groups based on how the bill is paid is no different than if the eight values were just randomly divided into two groups of four people. If each person would have ordered the same regardless of which table they were seated at, then another possible sample could have been:\n\n\n\n\n\n\n\nEven Split Group\nPay for what you order (control) Group\n\n\n\n\n\\(\\mathbf{\\color{dodgerblue}{\\$7.43}}\\) , \\(\\$8.00\\), \\(\\$8.75\\), \\(\\$13.17\\)\n\\(\\$8.50\\) , \\(\\$7.90\\) , \\(\\$10.85\\), \\(\\mathbf{\\color{dodgerblue}{\\$15.00}}\\)\n\n\n\n\nQuestion 1a\n\nWhat would be the test statistic for the two samples above?\n\nSolution to Question 1a\n\n\n# find the test statistic for another possible sample\n\n\n\n\n\n\nQuestion 1b\n\nHow many different ways can we divide the eight participants into two groups of four?\n\nSolution to Question 1b\n\n\n# how many possible ways can we create two groups of 4"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-2",
    "href": "22-Permutation-Tests.html#question-2",
    "title": "6.2: Permutation Tests",
    "section": "Question 2",
    "text": "Question 2\n\nBased on the summary(melanoma) output, give a possible statistical question that could be analyzed using a hypothesis test. Which variable(s) in the melanoma data set would be involved in your analysis?\n\n\n\n\n\n\nTip\n\n\n\nRun ?melanoma to access the help documentation and learn more about the data.\n\n\n\nSolution to Question 2"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-3",
    "href": "22-Permutation-Tests.html#question-3",
    "title": "6.2: Permutation Tests",
    "section": "Question 3",
    "text": "Question 3\n\nWhich variables in melanoma are categorical? Are those variables being stored as categorical variables? If so, explain how you can tell. If not, in the code cell below, convert the categorical variables to a factor.\n\nSolution to Question 3\n\n\n# if needed, convert each categorical variable to a factor"
  },
  {
    "objectID": "22-Permutation-Tests.html#creating-a-pooled-sample",
    "href": "22-Permutation-Tests.html#creating-a-pooled-sample",
    "title": "6.2: Permutation Tests",
    "section": "Creating a Pooled Sample",
    "text": "Creating a Pooled Sample\n\nOne possible question we could ask: “Is the mean tumor thickness greater for all people with fatal melanoma compared to the mean tumor thickness for all people that survive melanoma?” The two variables of interest are status and thickness.\n\nstatus: The patients status at the end of the study. 1 indicates that they had died from melanoma, 2 indicates that they were still alive and 3 indicates that they had died from causes unrelated to their melanoma.\nthickness: Tumor thickness in millimeters (mm).\n\nFor our study, we are comparing two independent populations:\n\nPeople that have a melanoma tumor removed and survive. This is status group 2.\nPeople that have a melanoma tumor removed and died from melanoma. This is status group 1.\nThe people in status group 3 we exclude from this analysis."
  },
  {
    "objectID": "22-Permutation-Tests.html#question-3-1",
    "href": "22-Permutation-Tests.html#question-3-1",
    "title": "6.2: Permutation Tests",
    "section": "Question 3",
    "text": "Question 3\n\nAnswer the questions below to state our hypotheses, organize our data, and calculate a test statistic to help determine whether the mean tumor thickness is greater for all people with fatal melanoma compared to the mean tumor thickness for all people that survive melanoma?\n\nQuestion 3a\n\nCreate side-by-side box plots to display the distribution of tumor thickness for each of the three status groups 1 (died from melanoma), 2 (survived), and 3 (died from other causes).\n\nSolution to Question 3a\n\nFill in the boxplot() command to answer the question.\n\nboxplot(??)\n\n\n\n\n\n\nQuestion 3b\n\nThe function tapply(data, index, function) has three inputs:\n\nThe data is the data you want to summarize.\nThe index is a categorical feature that will split the data into two or more different classes or factors.\nThe function is some function that you want to apply to the data.\n\nInterpret the output from the code cell below.\n\n# run code and interpret output below\ntapply(melanoma$thickness, melanoma$status, length)  # nothing to edit\n\n  1   2   3 \n 57 134  14 \n\n\n\nSolution to Question 3b\n\n\n\n\n\n\n\nQuestion 3c\n\nWe would like to use the data in melanoma to test the if the mean tumor thickness is greater for all people with fatal melanoma compared to the mean tumor thickness for all people that survive melanoma? State the corresponding null and alternative hypotheses for this test. Be sure to state each hypothesis both in words and using mathematical notation.\n\nSolution to Question 3c\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 3d\n\nUsing the code cell below, subset the melanoma data to create three different vectors:\n\nThe vector died is the thickness values for status group 1.\nThe vector survived is the thickness values for status group 2.\nThe vector pooled is the thickness values for both status groups 1 and 2 (excluding group 3).\n\n\n\n\n\n\n\nNote\n\n\n\nThe logical operator == means “is equal to” while the logical operator != means “is not equal to”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe option drop = TRUE means the output will be a vector of numerical values as opposed to a data frame that has a variable with the name thickness. For example, if we want to calculate the sample mean for the pooled data:\n\nSince the data is stored as a vector, we use mean(pooled). There are no headers in vectors.\nIf the data is stored as a data frame, we do need to use headers and have mean(pooled$thickness).\n\nThe output of the code cell below are vectors, so we do not use the $var_name convention when referring to the sample data which helps simplify the code a little.\n\n\n\nSolution to Question 3d\n\nReplace each of the 9 ?? with an appropriate value or variable name.\n\ndied &lt;- subset(melanoma, select = ??, ?? == \"??\", drop = TRUE)\nsurvived &lt;- subset(melanoma, select = ??, ?? == \"??\", drop = TRUE)\npooled &lt;- subset(melanoma, select = ??, ?? != \"??\", drop = TRUE)\n\n\n\n\n\n\nQuestion 3e\n\nUsing the vectors died, survived, and pooled from Question 3d, calculate the sample size and the sample mean for each of the samples died, survived, and pooled.\n\nSolution to Question 3e\n\n\nn.died &lt;- ??  # size of sample of melanoma deaths\nn.survived &lt;- ??  # size of sample of survivors\nn.pooled &lt;- ??  # size of both samples pooled together\nxbar.died &lt;- ??  # mean thickness of sample that died\nxbar.survived &lt;- ??  # mean thickness of sample that survived\nxbar.pooled &lt;- ??  # mean thickness of pooled sample\n\n# print each result to screen\nn.died \nn.survived \nn.pooled \nxbar.died \nxbar.survived \nxbar.pooled   \n\n\n\n\n\n\nQuestion 3f\n\nBased on the output in Question 3e, calculate, store, and print the test statistic for this test to the screen.\n\nSolution to Question 3f\n\n\ntest.stat &lt;- ??  # compute test statistic\ntest.stat  # print test statistic to screen"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-1-create-a-vector-of-pooled-data",
    "href": "22-Permutation-Tests.html#step-1-create-a-vector-of-pooled-data",
    "title": "6.2: Permutation Tests",
    "section": "Step 1: Create a Vector of Pooled Data",
    "text": "Step 1: Create a Vector of Pooled Data\n\nSee the code used to create pooled in Question 3d."
  },
  {
    "objectID": "22-Permutation-Tests.html#step-2-create-resamples-for-each-treatment-group",
    "href": "22-Permutation-Tests.html#step-2-create-resamples-for-each-treatment-group",
    "title": "6.2: Permutation Tests",
    "section": "Step 2: Create Resamples for Each Treatment Group",
    "text": "Step 2: Create Resamples for Each Treatment Group\n\nFor this step, it is important to note the size of each original sample. From Question 3e we know\n\nThe sample died consists of 57 observations.\nThe sample survived consists of 134 observations.\nThe pooled sample consists of \\(57 + 134= 191\\) observations.\n\n\nCreate an Index Vector\n\nWe first create a vector called index that selects (without replacement) 57 random integers out of the integers \\(1, 2, \\ldots , 191\\).\n\nset.seed(3021)  # fix the randomization seeding\nindex &lt;- sample(191, size = 57, replace = FALSE)  # these are the 57 observations chosen for resample 1\n\nThe command head(index) shows the first 6 values in the vector index.\n\nhead(index)\n\n[1]  55 164  88 189 171  68\n\n\nThe first index value 55 means observation 55 from the pooled vector is the first thickness value randomly assigned to the died sample. From the code cell below, we see the 55th observation in pooled is a thickness of \\(0.81\\) mm.\n\npooled[55]\n\n[1] 0.81\n\n\nThe next index value 164 means observation 164 from the pooled vector is the second thickness value randomly assigned to the died sample. From the code cell below, we see the 164th observation in pooled is a thickness of \\(0.65\\) mm.\n\npooled[164]\n\n[1] 0.65\n\n\n\n\nUse index to Select Resample of Group 1\n\nThe index vector is a vector of integers that tells us which values in pooled are randomly assigned to the died resample. However, index does not contain the corresponding thicknesses of the selected observations. The vector pooled[index] will contain the tumor thicknesses for all 57 randomly selected observations picked in index.\n\ndied.resample &lt;- pooled[index]\n\n\n\nUse -index to Select the Remaining Values for Resample Group 2\n\n\nThe vector index consists of 57 randomly selected integers out of the integers \\(1, 2, \\ldots , 191\\).\nThe vector -index contains the remaining 134 integers that were not selected for index.\nThe vector pooled[-index] will contain thicknesses for the remaining 134 observations in resample group 2.\n\n\nsurvived.resample &lt;- pooled[-index]"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-3-calculate-the-test-statistic-for-the-resamples",
    "href": "22-Permutation-Tests.html#step-3-calculate-the-test-statistic-for-the-resamples",
    "title": "6.2: Permutation Tests",
    "section": "Step 3: Calculate the Test Statistic for the Resamples",
    "text": "Step 3: Calculate the Test Statistic for the Resamples\n\nCalculate the difference in the sample means between the two resamples.\n\nperm.stat &lt;- mean(died.resample) - mean(survived.resample)\nperm.stat\n\n[1] -0.1487287"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-4-repeat-this-many-times-to-construct-a-permutation-distribution",
    "href": "22-Permutation-Tests.html#step-4-repeat-this-many-times-to-construct-a-permutation-distribution",
    "title": "6.2: Permutation Tests",
    "section": "Step 4: Repeat this Many Times to Construct a Permutation Distribution",
    "text": "Step 4: Repeat this Many Times to Construct a Permutation Distribution\n\n\nIn practice, it takes a lot of time and energy (and money) to generate all possible resamples (without any duplicate resamples).\n\nInstead we’ll generate a lot of resamples rather than all possible resamples.\n\nWe’ll use \\(N=10^5-1=99,\\!999\\) as the default number of resamples. Why use 99,999 resamples?\n\nWe may not generate the original sample as one of the resamples.\nWe want to be sure that we do include the original sample when we calculate the p-value.\nWe will add the original sample back in with the \\(99,\\!999\\) resamples giving \\(100,\\!000\\) samples.\n\nThe resulting distribution of test statistics of the permutation resamples is called a permutation distribution.\nWe use the permutation distribution as an estimate for the null distribution to compute the p-value."
  },
  {
    "objectID": "22-Permutation-Tests.html#question-4",
    "href": "22-Permutation-Tests.html#question-4",
    "title": "6.2: Permutation Tests",
    "section": "Question 4",
    "text": "Question 4\n\nComplete the first code cell below to create a permutation distribution for the difference in sample mean tumor thickness. Be sure you have already created the vector pooled in Question 3d and stored test.stat in Question 3f.\nAfter generating a permutation distribution, run the second code cell below to create a histogram to display the distribution of resample statistics along with a red vertical line through the observed test statistic. There is nothing to edit in the second code cell.\n\nSolution to Question 4\n\n\n##########################################\n# save all 99,999 permutation resample \n# statistics to the vector perm.stat\n##########################################\nN &lt;- 10^5 - 1\nperm.stat &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  index &lt;- sample(??, size = ??, replace = ??)  # create index vector\n  x.died &lt;- ??  # use index to select died resample\n  x.survived &lt;- ??  # the rest of the values go to survived resample\n  perm.stat[i] &lt;- ??  # calc difference in sample means\n}\n\n\n##################################################\n# plot permutation distribution as a histogram\n# mark observed test stat with red vertical line\n##################################################\nhist(perm.stat, xlab = \"xbar.died - xbar.survived\",\n     main = \"Permutation Distribution\")\nabline(v = test.stat, col = \"red\")"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-5",
    "href": "22-Permutation-Tests.html#question-5",
    "title": "6.2: Permutation Tests",
    "section": "Question 5",
    "text": "Question 5\n\nHow likely is it to get resamples with a difference in means as or more extreme than the observed test statistic? Explain what the code below is doing in practical terms.\n\np.value &lt;- sum(perm.stat &gt;= test.stat)/N\np.value\n\n\nSolution to Question 5\n\nInterpret the code cell above."
  },
  {
    "objectID": "22-Permutation-Tests.html#question-6",
    "href": "22-Permutation-Tests.html#question-6",
    "title": "6.2: Permutation Tests",
    "section": "Question 6",
    "text": "Question 6\n\nThe calculation from Question 5 used the \\(N=10^5-1=99,\\!999\\) permutation resamples, but recall we want to be sure to include the original, observed sample when computing the p-value. Explain how the code cell below accomplishes this goal.\n\np.value &lt;- (sum(perm.stat &gt;= test.stat) + 1) / (N + 1)\np.value\n\n\nSolution to Question 6"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-7",
    "href": "22-Permutation-Tests.html#question-7",
    "title": "6.2: Permutation Tests",
    "section": "Question 7",
    "text": "Question 7\n\nInterpret the practical meaning of the p-value from Question 6 to a person who is not very familiar with statistics.\n\nSolution to Question 7"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-8",
    "href": "22-Permutation-Tests.html#question-8",
    "title": "6.2: Permutation Tests",
    "section": "Question 8",
    "text": "Question 8\n\nUlceration is a breakdown of the skin over the melanoma tumor. Using the data set melanoma from the boot package, perform a permutation test to see if the variance of the tumor thickness for ulcerated tumors is different from the variance of the tumor thickness for non-ulcerated tumors.\n\n\n\n\n\n\nTip\n\n\n\nThe variable ulcer indicates whether the removed tumor was ulcerated (ulcer group 1) or not ulcerated (ulcer group 0).\n\n\n\nQuestion 8a\n\nWrite out the null and alternative hypotheses using appropriate notation.\n\nSolution to Question 8a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\n\nQuestion 8b\n\nWhat can we use as the test statistic? What is the value of the observed test statistic?\n\nSolution to Question 8b\n\n\ntest.stat2 &lt;- ??  # compute observed test statistic\ntest.stat2  # print output to screen\n\n\n\n\n\n\nQuestion 8c\n\nCreate a permutation distribution for the difference in sample variances.\n\nSolution to Question 8c\n\n\n# nothing to edit in this cell\npooled2 &lt;- melanoma$thickness  # create pooled vector of all tumor thicknesses\n\n\n# Save resamples to vector called result\nN &lt;- 10^5 - 1\nresult &lt;- numeric(N)\n\n# Create permutation distribution\nfor (i in 1:N)\n{\n  index &lt;- sample(??, size = ??, replace = ??)\n  result[i] &lt;- ??\n}\n\n# Display permutation distribution and observed sample diff\nhist(result, xlab = \"diff in sample variances\",\n     main = \"Permutation Distribution\")\nabline(v = c(-test.stat2, test.stat2), col = c(\"blue\", \"red\"))\n\n\n\n\n\n\nQuestion 8d\n\nCalculate the p-value of the observed test statistic and interpret its meaning in practical terms.\n\nSolution to Question 8d\n\n\n# compute the p-value"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-9",
    "href": "22-Permutation-Tests.html#question-9",
    "title": "6.2: Permutation Tests",
    "section": "Question 9",
    "text": "Question 9\n\nIs the proportion of females with ulcerated tumors less than the proportion of males with ulcerated tumors?\n\nQuestion 9a\n\nWrite out the null and alternative hypotheses using appropriate notation.\n\nSolution to Question 9a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\n\nQuestion 9b\n\nWhat can we use as the test statistic? What is the value of the observed test statistic?\n\nSolution to Question 9b\n\n\n# original ulceration data for female sample\nfemale &lt;- subset(melanoma, select = \"ulcer\", sex == \"0\", drop = TRUE)\n\n# original ulceration data for male sample\nmale &lt;- subset(melanoma, select = \"ulcer\", sex == \"1\", drop = TRUE)\n\n# original ulceration data for both samples pooled together\npooled.sex &lt;- melanoma$ulcer \n\n\n# enter a formula to compute the test statistic\ntest.diff.prop &lt;- ??\n\n\n\n\n\n\nQuestion 9c\n\nCreate a permutation distribution for the difference in sample proportions.\n\nSolution to Question 9c\n\nComplete the code cell below.\n\nN &lt;- 10^5 - 1\nresult.prop &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  index &lt;- sample(??, size = ??, replace = ??)\n  result.prop[i] &lt;- ??\n}\n\nhist(result.prop, xlab = \"phat1-phat2\",\n     main = \"Permutation Distribution\")\nabline(v = test.diff.prop, col = \"red\")\n\n\n\n\n\n\nQuestion 9d\n\nCalculate the p-value of the observed test statistic and interpret its meaning in practical terms.\n\nSolution to Question 9d\n\n\n# compute the p-value"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-10",
    "href": "22-Permutation-Tests.html#question-10",
    "title": "6.2: Permutation Tests",
    "section": "Question 10",
    "text": "Question 10\n\nIn this example, we use data collected from a matched pair designed study to determine whether smoking during pregnancy is associated with lower birth weight. In our study, we solicit volunteers that have already given birth to two babies. During one of the pregnancies, the parent smoked. During the other pregnancy, they did not smoke. Below is hypothetical data from such a study. A sample of \\(n=10\\) people volunteer to share their data with the researchers from which we have 10 different pairs of birth weights (in grams) summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225\n\n\n\n\nQuestion 10a\n\nResearchers are testing to see if the birth weight of babies born to a parent that smoked while pregnant is less, on average, compared to a babies whose parent did not smoke while they were pregnant. Write out the null and alternative hypotheses using appropriate notation.\n\nSolution to Question 10a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\n\nQuestion 10b\n\nWhat can we use as the test statistic? What is the value of the observed test statistic?\n\nSolution to Question 10b\n\n\n# data from study\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking births weights\n\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight\n\ndiff &lt;- no - smoker  # differences between matched pairs\n\n# Calculate the observed test statistic\n\n\n\n\n\n\nQuestion 10c\n\nWhen we construct a permutation distribution for the sample mean difference between matched pairs, we want to be sure the resampling we use preserves each pairing.\n\nWe do not randomize how the pairs are formed.\nEach pair of values should remain paired after resampling.\nInstead, we randomly assign values in each pair to the smoker and non-smoker positions.\n\nFor example, our original sample of matched pair differences is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225\n\n\nDifference\n960\n539\n-80\n85\n157\n1125\n14\n331\n199\n610\n\n\n\nOne possible resample is given below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking Resample\n1790\n2920\n3860\n3317\n2282\n2665\n3586\n3487\n2920\n2835\n\n\nSmoked Resample\n2750\n2381\n3940\n3402\n2125\n3790\n3572\n3156\n2721\n2225\n\n\nDifference Resample\n-960\n539\n-80\n-85\n157\n-1125\n14\n331\n199\n610\n\n\n\nTo create a permutation distribution for the sample mean difference between matched pairs, we randomly choose a sign (positive or negative) for each observed matched-pair difference. Complete the code cell below to generate a permutation distribution for the sample mean difference between matched pairs.\n\nSolution to Question 10c\n\nComplete and run the code cell below to create a permutation distribution.\n\nN &lt;- 10^5-1\nperm.match &lt;-numeric(N)\n\n# for each pair, randomly assign the difference to be positive or negative.\n# then calculate the new mean of the paired differences\nfor (i in 1:N)\n{\n  sign &lt;-sample(c(-1,1), size = ??, replace = ??) # random choose a sign -1 or 1\n  diff.resample &lt;- sign * diff\n  perm.match[i] &lt;- ??\n}\n\nThere is nothing to edit in the code cell below. Run the code cell below to plot the permutation distribution and test statistic.\n\n# create a histogram of the permutation distribution\n# and add a vertical line at the observed test statistic\nhist(perm.match,  xlab = \"xbar-diff\",\n     main = \"Permutation Distribution\")\nabline(v = test.match, col =\"red\")\n\n\n\n\n\n\nQuestion 10d\n\nCalculate the p-value of the observed test statistic and interpret its meaning in practical terms.\n\nSolution to Question 10d\n\n\n# compute the p-value"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-11",
    "href": "22-Permutation-Tests.html#question-11",
    "title": "6.2: Permutation Tests",
    "section": "Question 11",
    "text": "Question 11\n\nIs there a difference in the price of groceries sold by Target and Walmart? The data set Groceries in the resampledata package contains a sample of \\(n=24\\) different grocery items and a pair prices (price at Target and price at Walmart) advertised on their respective websites on a specific day.\n\nFirst we load the resampledata package.\n\n\nlibrary(resampledata)  # load resampledata package\n\n\nThen we print the first six rows of the Groceries data.\nNotice this is matched pairs data!\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you received an error when running the code cell below, it is possible you do not have the resampledata package installed. From the R console, run the command install.packages(\"resampledata\") to first install the resampledata packaged. Run the library(resampledata) command in the code cell above again. Then try running the code cell below again.\n\n\n\nhead(Groceries)\n\n                             Product    Size Target Walmart Units UnitType\n1            Kellogg NutriGrain Bars  8 bars   2.50    2.78     8     bars\n2 Quaker Oats Life Cereal  Original     18oz   3.19    6.01    18       oz\n3         General Mills Lucky Charms 11.50oz   3.19    2.98    11       oz\n4          Quaker Oats Old Fashioned    18oz   2.82    2.68    18       oz\n5               Nabisco Oreo Cookies  14.3oz   2.99    2.98    14       oz\n6                 Nabisco Chips Ahoy    13oz   2.64    1.98    13       oz\n\n\n\nNext, we save corresponding Target and Walmart prices to separate vectors target and walmart.\nThe first six values in each vector are printed to the screen.\nNotice the ordering of the values in each vector is very important to preserve.\n\n\ntarget &lt;- Groceries$Target\nwalmart &lt;- Groceries$Walmart\nhead(target)\n\n[1] 2.50 3.19 3.19 2.82 2.99 2.64\n\nhead(walmart)\n\n[1] 2.78 6.01 2.98 2.68 2.98 1.98\n\n\nUsing the sample data stored in target and walmart, answer the questions below to perform a permutation test.\n\nSet up hypotheses to test whether there a difference in the price of groceries sold by Target and Walmart?\nWhat is the observed test statistic?\nCreate a permutation distribution for the sample mean difference between matched pairs.\nCalculate the p-value.\nInterpret the meaning of the p-value.\n\n\nSolution to Question 11\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "22-Permutation-Tests.html#footnotes",
    "href": "22-Permutation-Tests.html#footnotes",
    "title": "6.2: Permutation Tests",
    "section": "",
    "text": "Gneezy, U., E. Haruvy, and H. Yafe (2004), “The Inefficiency of Splitting the Bill”, The Economic Journal 114.↩︎\nGlance, N., and B. Huberman, “The Dynamics of Social Dilemmas”, Scientific American↩︎\nAmerican Academy of Dermatology, https://www.aad.org/media/stats-skin-cancer.↩︎"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html",
    "href": "23-Hypothesis-Single-Population.html",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "",
    "text": "What is Significant Enough?\nThe general process form performing a hypothesis test is informally:\nWe have discussed Steps 1 and 2 and used resampling methods as one method to calculate p-values in Step 3. Refer to Appendix A for a summary of the steps outlined above. Before investigating parametric methods for computing p-values, let’s discuss steps and 4 and 5:\nAs with confidence intervals, when estimating an unknown population mean, we often do not know the population variance. Nevertheless, we can still conduct a hypothesis test on a single mean, but there will be some additional uncertainty due to our need to estimate \\(\\sigma^2\\). Below we work through an example using the storms data frame in the dplyr package to devise a method for computing p-values under these circumstances.\nIf we are performing a two-tailed hypothesis test using a significance level \\(\\alpha=0.05\\), then we can reject the null hypothesis if either:\nWe can adjust the statements above for a hypothesis test performed at other significance levels. For example, if we are conducting a two-tailed test at a 1% significance level, we can use 99% confidence interval instead of calculating a p-value.\nThe plots below require the package ggplot2 that is loaded in the code cell below. Be sure to first run the code cell below to load ggplot2 before running any of the code cells that follow.\nlibrary(ggplot2)"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#the-significance-level",
    "href": "23-Hypothesis-Single-Population.html#the-significance-level",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "The Significance Level",
    "text": "The Significance Level\n\nThe significance level of a test, denoted \\({\\color{dodgerblue}{\\alpha}}\\), is the value we choose that is used to determine whether the p-value is small enough to claim the result is statistically significant and reject \\(H_0\\).\n\nIf p-value \\(\\leq \\alpha\\), we reject \\(H_0\\).\nIf p-value \\(&gt; \\alpha\\), we do not reject \\(H_0\\).\n\nGenerally speaking, \\(H_0\\) is a claim we currently accept as true. \\(H_a\\) is some new and interesting result that if true would contradict the currently accepted belief in \\(H_0\\). We typically require compelling evidence, beyond a “reasonable doubt”, to reject the currently accepted claim in \\(H_0\\) in favor of a new and competing claim in \\(H_a\\).\n\nThe default significance level is typically 5% (or \\(\\alpha = 0.05\\)).\nSome other (less) common significance levels are \\(\\alpha = 0.1\\), \\(0.01\\) or \\(0.001\\).\nThe smaller we set \\(\\alpha\\), the more certainty we require to reject \\(H_0\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe significance level is not something we compute. We choose the significance level for the test, and the significance level should be determined prior to our analysis. Do not first calculate the p-value, and then retroactively choose the significance level to ensure the result is significant."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#summarizing-the-results",
    "href": "23-Hypothesis-Single-Population.html#summarizing-the-results",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Summarizing the Results",
    "text": "Summarizing the Results\n\nThere are two possible results with hypothesis tests:\n\nIf p-value \\(\\leq \\alpha\\), the test is statistically significant.\n\nThere is strong enough evidence to reject \\(H_0\\).\nAnd thus, we accept the competing claim in \\(H_a\\).\n\nIf p-value \\(&gt; \\alpha\\), the test is not statistically significant and there is not sufficient evidence to reject \\(H_0\\):\n\nWe fail to reject \\(H_0\\) (which is different from accepting \\(H_0\\)).\nThe test is inconclusive regarding the claims in \\(H_0\\) and \\(H_a\\).\n\n\nIn the end, we want to be sure we communicate the results clearly, in proper context, to a more general audience that may not have an advanced background in statistics and mathematics."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-1",
    "href": "23-Hypothesis-Single-Population.html#question-1",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 1",
    "text": "Question 1\n\n\n\n\nCredit: Kelvinsong, CC BY 3.0, via Wikimedia Commons\n\n\nPressure is a common measurement used to characterize the strength of a storm. The lower the storm pressure, the higher the wind speeds, and the more dangerous the storm. Let \\(\\mu\\) denote the mean pressure (in millibars) of all storms in the North Atlantic. Suppose we set up the following hypothesis to test claims about the value of \\(\\mu\\).\n\n\\(H_0\\): \\(\\mu = 950\\). The mean pressure of all storms in the North Atlantic is 950 millibars.\n\\(H_a\\): \\(\\mu \\ne 950\\). The mean pressure of all storms in the North Atlantic is not 950 millibars.\n\nWe collect a random sample of \\(n\\) storm pressure observation. We find the sample mean pressure \\(\\bar{x} = 992\\) millibars has a p-value \\(= 0.012\\).\n\nQuestion 1a\n\nSummarize the results if we perform the hypothesis test using a 5% significance level. Be sure to explain in the context of the example using terminology a more general audience would understand.\n\nSolution to Question 1a\n\n\n \n\n\n\nQuestion 1b\n\nSummarize the results if we perform the hypothesis test using a 10% significance level. Be sure to explain in the context of the example using terminology a more general audience would understand.\n\nSolution to Question 1b\n\n\n \n\n\n\nQuestion 1c\n\nSummarize the results if we perform the hypothesis test using a 1% significance level. Be sure to explain in the context of the example using terminology a more general audience would understand.\n\nSolution to Question 1c\n\n\n \n\n\n\nQuestion 1d\n\nSuppose we instead we want to show the mean storm pressure is greater than 950 millibars.\n\n\\(H_0\\): \\(\\mu = 950\\). The mean pressure of all storms in the North Atlantic is 950 millibars.\n\\(H_a\\): \\(\\mu &gt; 950\\). The mean pressure of all storms in the North Atlantic is greater than 950 millibars.\n\nWe still have the sample of the same size \\(n\\) and sample mean \\(\\bar{x} = 992\\) millibars as in Question 1. For the two-tailed test, this sample has a p-value \\(= 0.012\\).\n\nWhat would be the p-value for this same sample be if we use this one-tailed test instead?\nSummarize the result of the one-tailed test in practical terms if we use a significance level of 5%.\nSummarize the result of the one-tailed test in practical terms if we use a significance level of 1%.\n\n\nSolution to Question 1d"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-2",
    "href": "23-Hypothesis-Single-Population.html#question-2",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n\nCredit: Mbz1, CC BY-SA 3.0, via Wikimedia Commons\n\n\nIn the 2010 World Cup in Germany, Paul the octopus (aka the Oracle Octopus) correctly predicted the correct outcome in all 8 of the matches he predicted. Paul beat out his rival Mani, Singapore’s psychic parakeet, who predicted six matches in a row before missing a prediction. Below is an excerpt from a recent article1 about animal clairvoyance and the World Cup.\n\nNo FIFA World Cup would be complete without “psychic” animals predicting the winners, and Qatar 2022 has been no exception. From “clairvoyant” camels to “mystic” elephants and “cryptic” rats, a range of animals – big and small – have tried their paws, hooves and tentacles at predicting the score line.   It all started with Paul, the “psychic” octopus. The eight-tentacled icon put TV pundits to shame with an incredible string of correct World Cup winner predictions from his glass tank at the Aquarium Sea Life Centre in Oberhausen, Germany. The tentacled tipster had an incredible success rate: he correctly predicted eight world cup matches at South Africa’s tournament in 2010, including Spain beating the Netherlands in the World Cup final.\n\nIn 2010, Paul the Octopus “predicted” 8 matches and made 8 correct predictions. Is this evidence that Paul actually has psychic powers?\n\nQuestion 2a\n\nSet up the null and alternative hypotheses in terms of the proportion of all World Cup matches ever played that Paul correctly predicts. State hypotheses both in words and using appropriate notation.\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nHow unusual would this be if Paul was just randomly guessing? Compute the p-value corresponding to this sample of \\(n=8\\) predictions all of which are correct.\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nSummarize the result of the test to a general audience if a 5% significance level is chosen.\n\nSolution to Question 2c"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#p-value-for-a-single-proportion",
    "href": "23-Hypothesis-Single-Population.html#p-value-for-a-single-proportion",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "p-value for a Single Proportion",
    "text": "p-value for a Single Proportion\n\nLet \\(X\\) be a binomial random variable and let \\(p_0\\) denote the value of \\(p\\) claimed in \\(H_0\\). If we observe \\(X=x\\) successes out of a sample of \\(n\\) independent and identical trials, then we can find the p-value using the binomial null distribution \\({\\color{dodgerblue}{X \\sim \\mbox{Binom}(n, {\\color{tomato}{p_0}})}}\\)."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-3",
    "href": "23-Hypothesis-Single-Population.html#question-3",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 3",
    "text": "Question 3\n\nThe mean height of all adult males in the United Kingdom is claimed2 to be the \\(68.5\\) inches (\\(5\\) foot \\(8.5\\) inches or \\(173.9\\) cm) with a standard deviation of \\(2.5\\) inches (or \\(6.35\\) cm). A physician suspects males in her town seem to be taller than average when compared to the population of all adult males in the UK. She collects data from a random sample of \\(n=25\\) adult males from the town and calculates the mean height of the sample is \\(69.25\\) inches.\n\nQuestion 3a\n\nSet up the null and alternative hypotheses (both in words and using appropriate notation) to test the physician’s claim that adult males in the town are taller than the national average height for all adult males in the UK.\n\nSolution to Question 3a\n\n\n\\(H_0\\): ??\n\\(H_a\\): ??\n\n\n\n\n\n\nQuestion 3b\n\nCompute the test statistic.\n\nSolution to Question 3b\n\n\n\n\n\n\n\nQuestion 3c\n\nWhat is a reasonable null distribution to use to perform this test? Standardize the test statistic (give the \\(z\\)-score) from Question 3b. Interpret the meaning of the standardized test statistic.\n\nSolution to Question 3c\n\n\n\n\n\n\n\nQuestion 3d\n\nCompute the p-value and interpret the meaning in practical terms.\n\nSolution to Question 3d\n\n\n\n\n\n\n\nQuestion 3e\n\nShade area(s) under the graph of a null distribution corresponding to the p-value. Either make an informal sketch on paper or see Appendix B to plot in R.\n\nSolution to Question 3e\n\n\n\n\n\n\n\nQuestion 3f\n\n\nIf a 5% significance level is chosen, summarize the result in practical terms.\nIf a 10% significance level is chosen, summarize the result in practical terms.\n\n\nSolution to Question 3f"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-known-sigma2",
    "href": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-known-sigma2",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "p-value for a Single Mean: Known \\(\\sigma^2\\)",
    "text": "p-value for a Single Mean: Known \\(\\sigma^2\\)\n\nSuppose a random sample size \\(n\\) is picked from a population with known population variance \\(\\sigma^2\\) but unknown mean \\(\\mu\\). If we are doing a hypothesis test on a single mean with null claim \\({\\color{tomato}{H_0: \\mu = \\mu_0}}\\), then as long as the population is symmetric or the sample size is large enough \\((n \\geq 30)\\), we can use the Central Limit Theorem for means to:\n\nModel the null distribution with the sampling distribution \\({\\color{dodgerblue}{\\overline{X} \\sim N \\left( {\\color{tomato}{\\mu_0}}, \\frac{\\sigma}{\\sqrt{n}} \\right)}}\\).\nCalculate the standardized test statistic which is the \\(z\\)-score of the sample mean:\n\n\\[{\\color{dodgerblue}{z = \\frac{\\mbox{sample stat}- {\\color{tomato}{\\mbox{null claim}}}}{\\mbox{SE}(\\overline{X})} = \\dfrac{\\bar{x} - {\\color{tomato}{\\mu_0}}}{\\frac{\\sigma}{\\sqrt{n}}}}}.\\]"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#picking-a-random-sample-of-storm-pressures",
    "href": "23-Hypothesis-Single-Population.html#picking-a-random-sample-of-storm-pressures",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Picking a Random Sample of Storm Pressures",
    "text": "Picking a Random Sample of Storm Pressures\n\nThe storms data set is from the NOAA Hurricane Best Track Data. We will perform a hypothesis test to test claims about the mean storm pressure, so we will need to analyze the variable pressure.\n\nRun the code cell below to load the dplyr package (which should already be installed).\n\n\nlibrary(dplyr)  # load dplyr package\n\n\nRun the code cell below to pick a random sample of \\(n=32\\) storm pressures from storms.\n\n\nmy.sample &lt;- sample(storms$pressure, size=32, replace=FALSE)"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-4",
    "href": "23-Hypothesis-Single-Population.html#question-4",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 4",
    "text": "Question 4\n\nIt is claimed3 that the average pressure of all North Atlantic storms is 950 millibars. You believe this claim is inaccurate and would like to show the average pressure of all storms is not 950 millibars.\n\nQuestion 4a\n\nSet up the null and alternative hypotheses both in words and using appropriate notation.\n\nSolution to Question 4a\n\n\n\\(H_0\\): ??\n\\(H_a\\): ??\n\n\n\n\n\n\nQuestion 4b\n\nCompute the test statistic.\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nWhat is a reasonable standardized null distribution to use to perform this test? Standardize the test statistic from Question 4b and interpret its meaning.\n\nSolution to Question 4c\n\n\n\n\n\n\n\nQuestion 4d\n\nCompute the p-value and interpret the meaning in practical terms.\n\nSolution to Question 4d\n\n\n\n\n\n\n\nQuestion 4e\n\nShade area(s) under the graph of a null distribution corresponding to the p-value in Question 4d. Either make an informal sketch on paper or see Appendix B to plot in R.\n\nSolution to Question 4e\n\n\n\n\n\n\n\nQuestion 4f\n\nIf a 5% significance level is chosen, summarize the result in practical terms.\n\nSolution to Question 4f"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-unknown-sigma2",
    "href": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-unknown-sigma2",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "p-value for a Single Mean: Unknown \\(\\sigma^2\\)",
    "text": "p-value for a Single Mean: Unknown \\(\\sigma^2\\)\n\nSuppose a random sample size \\(n\\) is picked from a population with unknown population mean and variance. If we are doing a hypothesis test on a single mean with null claim \\(H_0: \\mu = \\mu_0\\), then as long as the population is symmetric or the sample size is large enough \\((n \\geq 30)\\):\n\nThe standardized test statistic is called the t-test statistic:\n\n\\[{\\large \\boxed{{\\color{dodgerblue}{{\\color{tomato}{t}} = \\frac{\\mbox{sample stat}-\\mbox{null claim}}{\\mbox{SE}(\\overline{X})} = \\dfrac{\\bar{x} - \\mu_0}{\\frac{{\\color{tomato}{s}}}{\\sqrt{n}}}}}}}.\\]\n\nThe null distribution is the distribution of t-test statistics that we model using a \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\n\nIn R, we can use the command t.test(x, mu = [null], alt = [direction]).\n\nSample data is stored in the vector x.\nSet the option mu equal to the value, \\(\\mu_0\\), claimed in \\(H_0\\).\nSet the option alt based on the inequality used in \\(H_a\\).\n\nUse \"greater\" for right-tail test.\nUse \"less\" for left-tail test.\nUse \"two.sided\" for a two-tail test.\nIf you do not indicate any alt option, the default is a two-tailed test."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-5",
    "href": "23-Hypothesis-Single-Population.html#question-5",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 5",
    "text": "Question 5\n\nCheck your results for the hypothesis test in Question 4 using the t.test() function.\n\nSolution to Question 5\n\nFill in the options for the t.test() function in the code cell below.\n\nt.test(??)"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-6",
    "href": "23-Hypothesis-Single-Population.html#question-6",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 6",
    "text": "Question 6\n\nThe output of t.test() gives both a p-value and a 95% confidence interval (by default). Let’s interpret the confidence interval and see if we obtain a result that is consistent with our summary in Question 4f.\n\nQuestion 6a\n\nBased on the output of your code in Question 5, what is a 95% confidence interval for the mean pressure of all North Atlantic storms?\n\nSolution to Question 6a\n\nA 95% confidence interval for the mean pressure of all storms is from ?? millibars to ?? millibars.\n \n\n\n\nQuestion 6b\n\nBased on the 95% confidence interval Question 6a, is 950 millibars (the null claim for \\(\\mu\\) in \\(H_0\\)) a plausible estimate for \\(\\mu\\)? Is this consistent with your answer in Question 4f? Explain why or why not.\n\nSolution to Question 6b"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#sec-appendb-known",
    "href": "23-Hypothesis-Single-Population.html#sec-appendb-known",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Illustrating p-values: Known Population Variance",
    "text": "Illustrating p-values: Known Population Variance\n\nIf we are performing a hypothesis test on a single mean for a population whose variance is known, then we can either use:\n\nThe null distribution is \\({\\color{dodgerblue}{\\overline{X} \\sim N \\left( \\mu_0, \\frac{\\sigma}{\\sqrt{n}} \\right)}}\\) with test statistic is \\(\\bar{x}\\), or\nThe standardized normal distribution \\(Z \\sim N(0,1)\\) with standardized test statistic\n\n\\[{\\color{dodgerblue}{z =  \\dfrac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}}}.\\]\n\nIn the code cell below, enter values for the mean and standard error of the null distribution as well as the test statistic.\n\n\nnull.mean &lt;- ??  # mean of the null distribution\nnull.se &lt;- ??  # standard error of the null distribution\ntest.stat &lt;- ??  # test statistic\n\n\nTwo-Tailed Test: Known Variance\n\nTo illustrate the p-value for a two-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined null.mean, null.se and test.stat.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a two-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(null.mean + 4*null.se, x.test)\nxmin &lt;- -1*xmax\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, x.test)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = c(-x.test, x.test), linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(-x.test,  x.test)) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nLeft-Tailed Test: Known Variance\n\nTo illustrate the p-value for a left-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined null.mean, null.se and test.stat.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a left-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(null.mean + 4*null.se, x.test)\nxmin &lt;- -1*xmax\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, xmax)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_vline(xintercept = -x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=-x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nRight-Tailed Test: Known Variance\n\nTo illustrate the p-value for a right-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined null.mean, null.se and test.stat.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a right-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(null.mean + 4*null.se, x.test)\nxmin &lt;- -1*xmax\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = NA, \n            xlim = c(xmin, x.test)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#sec-appendb-unknown",
    "href": "23-Hypothesis-Single-Population.html#sec-appendb-unknown",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Illustrating p-values: Unknown Population Variance",
    "text": "Illustrating p-values: Unknown Population Variance\n\nIf we are performing a hypothesis test on a single mean for a population whose variance is unknown, then we use \\(\\mathbf{t_{n-1}}\\), a \\(t\\)-distribution with \\(n-1\\) degrees of freedoms, for the null distribution and have t-test statistic\n\\[{\\color{dodgerblue}{{\\color{tomato}{t}} = \\dfrac{\\bar{x} - \\mu_0}{\\frac{{\\color{tomato}{s}}}{\\sqrt{n}}}}}.\\]\n\nIn the code cell below, enter the value of the t-test statistic and the degrees of freedom.\n\n\ntest.stat &lt;- ??  # t-test statistic\ndeg.free &lt;- ??  # degrees of freedom  \n\n\nTwo-Tailed Test: Unknown Variance\n\nTo illustrate the p-value for a two-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined test.stat and deg.free.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a two-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(4, x.test)\nxmin &lt;- -1*xmax\nv &lt;- deg.free\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, x.test)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = c(-x.test, x.test), linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(-x.test,  x.test)) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nLeft-Tailed Test: Unknown Variance\n\nTo illustrate the p-value for a left-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined test.stat and deg.free.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a left-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(4, x.test)\nxmin &lt;- -1*xmax\nv &lt;- deg.free\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, xmax)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_vline(xintercept = -x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=-x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nRight-Tailed Test: Unknown Variance\n\nTo illustrate the p-value for a right-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined test.stat and deg.free.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a right-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(4, x.test)\nxmin &lt;- -1*xmax\nv &lt;- deg.free\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = NA, \n            xlim = c(xmin, x.test)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#footnotes",
    "href": "23-Hypothesis-Single-Population.html#footnotes",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "",
    "text": "“The ‘psychic’ animals predicting who will win the World Cup”, ABC News, accessed July 13, 2023.↩︎\n“Height, Weight, and Body Mass of the British Population Since 1820” by Roderick Floud, National Bureau of of Economic Research, October 1998.↩︎\nThe University of Arizona Hydrology and Atmospheric Sciences, accessed July 13, 2023.↩︎"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html",
    "href": "24-Hypothesis-Comparing-Two.html",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "",
    "text": "Questions that Compare Two Populations\nIn many cases, the explanatory variable of interest in our question has more than just two levels. We can apply hypothesis tests on two populations to compare two levels of the explanatory variable. Often the researchers have a goal in mind, and they choose which two levels they would like to compare and/or how the levels are defined. Another group of researchers might approach the same statistical question from a different perspective and choose a different set up.\nWhen doing a hypothesis test for a difference between population means or two population proportions, we apply the same steps as with previous tests. See Appendix A for a summary of the steps below.\nIn our previous work with hypothesis tests on two populations, we used two-sample permutations tests to find standardized test statistics and p-values in steps 2 and 3. We now devise parametric methods for hypothesis tests that compare two populations.\nIf we are doing a test on a difference of two proportions, then the null claim is \\(H_0\\): \\(p_1 - p_2=0\\). We use the difference in sample proportions, \\({\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}}\\), as the test statistic:\n\\[{\\color{dodgerblue}{\\mbox{stand test stat} = \\frac{ (\\mbox{diff in sample stats}) - ({\\color{tomato}{\\mbox{diff in null claim}}})}{\\mbox{SE of Null Dist}} = \\frac{ (\\hat{p}_1 - \\hat{p}_2) - {\\color{tomato}{0}}}{\\mbox{SE of Null Dist}}}}.\\]\nFrom the CLT for a difference in proportions we have\n\\[\\widehat{P}_1 - \\widehat{P}_2  \\sim N \\left( {\\color{tomato}{p_1 - p_2}} , {\\color{dodgerblue}{\\mbox{SE}}} \\right) = N \\left( {\\color{tomato}{0}} , {\\color{dodgerblue}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}} } \\right).\\]"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-1",
    "href": "24-Hypothesis-Comparing-Two.html#question-1",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 1",
    "text": "Question 1\n\nSuppose the explanatory variable for statistical questions Q1, Q2, and Q3 are:\n\nQ1: Medication Type\nQ2: Gender\nQ3: Latitude\n\nIn the questions below, set up null and alternative hypothesis (in words and symbolically) to perform a hypothesis test to answer the statistical question.\n\nQuestion 1a\n\nQ1: Is a new medication more effective at preventing disease than current treatments?\nIn general, we might be interested in comparing several treatments. For example, the variable Medication Type could have four different levels: Treatment A, Treatment B, Treatment C, and No Treatment. Within our hypothesis test framework, we can compare two different treatments types. Set up hypotheses for the following question:\n\nDoes xylitol (a sweetener) reduce ear infections in children compared to no treatment (control)?\n\n\nSolution to Question 1a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 1b\n\nQ2: Is there a gender pay gap in the US?\nThe explanatory variable Gender may be gathered using different methods: free response question, multiple choice question (with various ways of defining the choices), or pulled directly from available records. Suppose we would like to focus on identifying if there is a pay gap between men and women that have just completed a bachelor’s degree in the US. Set up hypothesis for the following question:\n\nDo women that have recently completed a bachelor’s degree in the US get paid less, on average, compared to men that have recently completed a bachelor’s degree?\n\n\n\n\n\n\n\nNote\n\n\n\nThe median is often used as the measure of center to summarize income since the distribution of income is typically heavily skewed. Outliers will have a bigger influence in the analysis if we compare means. With the median, outliers have as much influence as any other observation. However, there is no parametric method for testing claims about medians.\n\n\n\nSolution to Question 1b\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 1c\n\nQ3: Is there a region in the North Atlantic where storm strength is greatest?\nThe explanatory variable Latitude is typically a quantitative variable given in degrees north or south of the equator. There are many different ways a researcher can define different regions, and how they define those regions depends on the data available and their research goals. Set up hypothesis for the following question:\n\nIs the storm pressure different, on average, for storms in the North Atlantic above the Tropic of Cancer compared to those below the Tropic of Cancer?\n\n\nSolution to Question 1c\n\n\n\\(H_0\\):\n\\(H_a\\):"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#a-general-test-statistic",
    "href": "24-Hypothesis-Comparing-Two.html#a-general-test-statistic",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "A General Test Statistic",
    "text": "A General Test Statistic\n\nWe can apply the Central Limit Theorem, CLT, to standardize the test statistic and calculate p-values using the same strategy as the parametric approach for testing claims about a single population. We begin by assuming the claim in the null hypothesis is true:\n\n\\(H_0\\): There is no difference. For example, \\(\\mu_1 - \\mu_2 =0\\) or \\(p_1 -p_2 = 0\\).\n\nThe standardized test statistic is the \\(z\\)-score or t-test statistic corresponding to the observed difference in sample statistics:\n\\[{\\color{dodgerblue}{\\mbox{stand test stat} = \\frac{ (\\mbox{diff in sample stats}) - ({\\color{tomato}{\\mbox{diff in null claim}}})}{\\mbox{SE of Null Dist}} = \\frac{ (\\mbox{diff in sample stats}) - {\\color{tomato}{0}}}{\\mbox{SE of Null Dist}}}}.\\]\nTo calculate the standardized test statistic, we consider the following questions:\n\nDo the samples satisfies conditions for CLT?\nWhat is the correct probability distribution to use, a t-distribution or a normal distribution?\nWhat is the difference in observed sample statistics?\nWhat is an estimate for the standard error of the null distribution?"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-2",
    "href": "24-Hypothesis-Comparing-Two.html#question-2",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 2",
    "text": "Question 2\n\nIf we are doing a test on a difference of means from two independent populations, then the null claim is \\(H_0\\): \\(\\mu_1 - \\mu_2=0\\) and recall the CLT for a difference in means is\n\\[\\overline{X}_1 - \\overline{X}_2  \\sim N \\left( {\\color{tomato}{\\mu_1 - \\mu_2}} , {\\color{dodgerblue}{\\mbox{SE}}} \\right) = N \\left( {\\color{tomato}{0}} , {\\color{dodgerblue}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} } \\right).\\] Suppose we are performing a test on the difference of two independent means, \\(\\mu_1 - \\mu_2\\), and the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown.\n\nWhat are reasonable estimates to use in place of \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)?\nWhat is a formula for estimating the standard error of the null distribution?\nShould we use a t-distribution or normal distribution to approximate the null distribution?\n\nBased on your answers to the previous questions, fill in the blanks label (i), (ii), (iii), and (iv) to give a formula for standardized test statistic. In blank (i), enter either \\(z\\) or \\(t\\) depending on whether a normal or t-distribution is appropriate.\n\\[{\\color{dodgerblue}{\\mbox{(i)} = \\frac{ (\\mbox{diff in sample stats}) - ({\\color{tomato}{\\mbox{diff in null claim}}})}{\\mbox{SE of Null Dist}} = \\frac{ \\mbox{(ii)} - {\\color{tomato}{\\mbox{(iii)}}}}{\\mbox{(iv)}}}}.\\]\n\nSolution to Question 2\n\nComplete the formula for the standardized test statistic:\n\nChoose either \\(z\\) or \\(t\\) for blank (i).\nBlank (ii) is ??\nBlank (iii) is ??\nBlank (iv) is ??"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-3",
    "href": "24-Hypothesis-Comparing-Two.html#question-3",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 3",
    "text": "Question 3\n\nWe return to our statistical question and hypothesis from Question 1b regarding a possible gender pay gap in the US. We collect data from a random sample of people that completed their bachelor’s degree in 2021 and compare the starting salaries of men and women in the sample. A sample of 154 women have a mean income of \\(\\$52,\\!266\\) with standard deviation \\(\\$15,\\!510\\) A sample of 122 men have a mean income of \\(\\$64,\\!022\\) with a standard deviation of \\(\\$16,\\!890\\).\nIs there enough evidence to support the claim that women who have recently completed a bachelor’s degree in the US get paid less, on average, compared to men who have recently completed a bachelor’s degree? Test using a significance level of \\(\\alpha=0.05\\).\n\n\n\n\n\n\nNote\n\n\n\nThe statistical question in this example as well as the data are motivated by an analysis of students that earned a bachelor’s degree in the US in 2021 by the National Association of Colleges and Employers (NACE).\n\n\n\nSolution to Question 3\n\nUsing the hypotheses you set up in Question 1b, answer the following:\n\nBased on the sample data, compute the standardized test statistic.\n\n\n# calculate standardized test statistic\n\n\n\n\nBased on your standardized test statistic, compute the p-value.\n\n\n# compute p-value\n\n\n\n\nMake a decision and summarize the results in the context of this example."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#using-t.test-to-compare-two-populations",
    "href": "24-Hypothesis-Comparing-Two.html#using-t.test-to-compare-two-populations",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Using t.test() to Compare Two Populations",
    "text": "Using t.test() to Compare Two Populations\n\nTo calculate a p-value for a test on the difference of means from two independent populations with unknown variances, we can calculate the t-test statistic\n\\[{\\boxed{\\color{dodgerblue}{{\\color{tomato}{t}} = \\frac{ \\mbox{diff in sample stats} - 0}{\\mbox{SE of Null Dist}} = \\dfrac{(\\bar{x}_1 - \\bar{x}_2) - 0 }{\\sqrt{\\frac{{\\color{tomato}{s_1}}^2}{n_1} + \\frac{{\\color{tomato}{s_2}}^2}{n_2}}}}}}.\\]\nIn Question 3, you were conveniently provided numerical summaries of each sample that we substituted into the formula for the t-test statistic. Then we can err on the side caution and choose the degrees of freedom, \\(df\\), to be the smaller of either \\(\\mathbf{n_1-1}\\) or \\(\\mathbf{n_2-1}\\).\nIf we have access to all the sample data (not just summaries), then we can load the sample data into R and use the t.test() function to calculates both the t-test statistic and p-value using a more accurate approximation for \\(df\\) obtained using Welch’s approximation.\n\nSamples Stored as Separate Vectors\n\nIn R, we can use the command t.test(x1, x2, alt = [direction]).\n\nSamples are stored in independent vectors x1 and x2.\nSet the option alt based on the inequality used in \\(H_a\\).\n\nUse \"greater\" for right-tail test.\nUse \"less\" for left-tail test.\nUse \"two.sided\" for a two-tail test.\nIf you do not indicate any alt option, the default is a two-tailed test.\n\nThe default is mu=0 which is exactly what we want, so we do not specify this option.\n\n\n\nSubsetting Samples Inside t.test()\n\nFrequently, we would like to compare the means of a quantitative response variable, denoted quant, for two different levels of a categorical explanatory variable, denoted categorical, in the data frame named data.name. The t.test() function can simultaneously subset the quant data into two independent samples (one for each level of category) and then perform a hypothesis test for the difference in two means:\n\nt.test(quant ~ categorical, data = data.name, alt = [direction])\n\n\n\n\n\n\n\nCaution\n\n\n\nIf the categorical variable has exactly two levels, then the command t.test(quant ~ categorical, data = data.name, alt = [direction]) works as desired. However, if categorical has more than two levels, this results in an error. If there are more than two levels, you can first subset or filter the data and then use t.test()."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-4",
    "href": "24-Hypothesis-Comparing-Two.html#question-4",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 4",
    "text": "Question 4\n\nWe use the storms data set from the NOAA Hurricane Best Track Data in the dplyr package to test if the storm pressure is different, on average, for storms in the North Atlantic above the Tropic of Cancer compared to those below the Tropic of Cancer.\n\nRun the code cell below to load the dplyr package (which should already be installed).\n\n\nlibrary(dplyr)  # load dplyr package\n\n\nQuestion 4a\n\nRun the code cell below to calculate the p-value of the sample data in storms. Interpret the meaning of the output.\n\nt.test(pressure ~ lat, data = storms, alt = \"two.tail\")\n\n\nSolution to Question 4a\n\n\n\n\n\n\n\nQuestion 4b\n\nThe Tropic of Cancer is the most northerly circle of latitude on Earth at which the Sun can be directly overhead. The position of this circle is constantly wobbling just slightly. The Tropic of Cancer is currently located at \\(23.43623^{\\circ}\\) north of the Equator.\n\nCreating Levels for the Explanatory Variable with mutate()\n\nThe code cell use the mutate() function that is in the dplyr package to create and add a new categorical variable, we name tropic, consisting of two levels (above and below) depending on whether the storm’s location is above or below the Tropic of Cancer. Then the tapply() function is used to calculate separate means for the sample of storms above and below the Tropic of Cancer.\n\n\n\n\n\n\nNote\n\n\n\nThe data frame storms and mutate() script are both in the dplyr package. Be sure you have already run library(dplyr) before running the code cell below.\n\n\n\n# create and add new variable tropic to storms\nstorms &lt;- mutate(storms, \n                 tropic = case_when(lat &lt; 23.43623 ~ 'below',  # below is assigned if lat &lt; 23.43623\n                                    lat &gt; 23.43623 ~ 'above'))  # above is assigned if lat &gt; 23.43623\n\nstorms$tropic &lt;- factor(storms$tropic)  # ensure tropic stored as categorical factor\ntapply(storms$pressure, storms$tropic, mean)  # compare mean pressures\n\nAfter running the code cell above, complete the code cell below to create side-by-side box plots of the distributions of storm pressures above and below the Tropic of Cancer.\n\n\nSolution to Question 4b\n\nReplace each ?? with an appropriate variable or data frame name.\n\nboxplot(?? ~ ??, data = ??)\n\n\n\n\n\n\nQuestion 4c\n\nFill in the t.test() function below to compute the test statistic and corresponding p-value based on the sample data in storms to test the hypothesis in Question 1c\n\nSolution to Question 4c\n\nComplete and run the code cell below.\n\nt.test(?? ~ ??, data = ??, alt = ??)\n\n\n\n\n\n\nQuestion 4d\n\nBased on the output in Question 4c, summarize the result of this test in practical terms. Use a significance level of 5%.\n\nSolution to Question 4d"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-5",
    "href": "24-Hypothesis-Comparing-Two.html#question-5",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 5",
    "text": "Question 5\n\nWhen doing a test on the difference of two proportions, the null hypothesis is the claim \\(p_1 - p_2 = 0\\) or \\(p_1 = p_2\\).\n\nWe have not claimed anything about the value of \\(p_1\\) on its own.\nWe have not claimed anything about the value of \\(p_2\\) on its own.\n\nWhat is a reasonable estimate to plug in place of \\(p_1\\) and \\(p_2\\) in the formula for the standard error?\n\nSolution ot Question 5"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#defining-a-test-statistic",
    "href": "24-Hypothesis-Comparing-Two.html#defining-a-test-statistic",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Defining a Test Statistic",
    "text": "Defining a Test Statistic\n\nLet \\(X_1\\) and \\(X_2\\) denote the number of successes in samples 1 and 2 with sizes \\(n_1\\) and \\(n_2\\), respectively.\n\nWe use a normal distribution to model the null distribution (as long as both samples are large enough).\nIn \\(H_0\\), we assume there is no difference in the two groups, so we pool the samples together and calculate the sample pooled proportion:\n\n\\[{\\color{tomato}{\\mbox{sample pooled proportion} = \\hat{p}_p = \\frac{X_1 + X_2}{n_1 + n_2}}}.\\]\n\nTo estimate the standard error, we use \\({\\color{tomato}{\\hat{p}_p}}\\) in place of both \\(p_1\\) and \\(p_2\\).\nThe standardized test statistic is the z-score of the difference in sample proportions:\n\n\\[\\boxed{{\\color{dodgerblue}{z = \\dfrac{ (\\mbox{diff in sample stats}) - (\\mbox{diff in null claim})}{\\mbox{SE of Null Dist}}= \\dfrac{(\\hat{p}_1 - \\hat{p}_2) - 0 }{\\sqrt{{\\color{tomato}{\\hat{p}_p}}(1 -{\\color{tomato}{\\hat{p}_p}})\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}}}}.\\]"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#calculating-the-p-value",
    "href": "24-Hypothesis-Comparing-Two.html#calculating-the-p-value",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Calculating the p-Value",
    "text": "Calculating the p-Value\n\nWhen doing a hypothesis test on a difference of two proportions, we use a normal distribution to estimate the null distribution and calculate p-values. If we denote the z-score of the observed difference in sample means as z, then:\n\nFor a left-tailed test, p-value = pnorm(z, 0, 1)\nFor a right-tailed test, p-value = 1 - pnorm(z, 0, 1)\nFor a two-tailed test, p-value = 2 * pnorm(-1*abs(z), 0 , 1)"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-6",
    "href": "24-Hypothesis-Comparing-Two.html#question-6",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 6",
    "text": "Question 6\n\n\n\n\nCredit: Nick Youngson CC BY-SA 3.0 via Pix4free.org\n\n\nIt is believed that a sweetener called xylitol helps prevent ear infections. In a randomized experiment1 \\(n_1 = 165\\) children took a placebo and \\(68\\) of them got ear infections. Another \\(n_2 = 159\\) children took xylitol and \\(46\\) of them got ear infections. We believe that the proportion of ear infections in the placebo group will be greater than the xylitol group. Test this claim using \\(\\alpha=0.01\\).\n\nSolution to Question 6\n\nUse hypotheses from Question 1a and answer the questions below.\n\n\nEstimate the standard error of the null distribution.\n\n\n# estimate of SE\n\n\n\n\nBased on your answer to (a), use the parametric formula to find the z-score of the observed difference in sample proportions.\n\n\n# calculate standardized test statistic\n\n\n\n\nBased on your answer in (b), compute the p-value.\n\n\n# calculate p-value\n\n\n\n\nMake a decision and summarize the results in the context of this example."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#applying-a-continuity-correction",
    "href": "24-Hypothesis-Comparing-Two.html#applying-a-continuity-correction",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Applying a Continuity Correction",
    "text": "Applying a Continuity Correction\n\n\nThe CLT approximation uses a continuous normal distribution to estimate a discrete distribution.\nWe can improve the approximation using a continuity correction that we saw earlier.\nAdd or subtract \\(0.5\\) from each success count so the difference in proportions becomes closer to 0, and thus gives a larger p-value."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-7",
    "href": "24-Hypothesis-Comparing-Two.html#question-7",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 7",
    "text": "Question 7\n\nRedo the solution to Question 6 by applying a continuity correction.\n\nSolution to Question 7\n\n\nStandard error is the same as in Question 6\n\n\n\nBased on the SE in Question 6, use the parametric formula along with a continuity correction to find the z-score of the observed difference in sample proportions.\n\n\n# calculate standardized test statistic\n\n\n\n\nBased on your answer in (b), compute the p-value.\n\n\n# calculate p-value\n\n\n\n\nDoes the p-value in (c) lead to a different result than in Question 6? Explain."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#using-the-prop.test-function",
    "href": "24-Hypothesis-Comparing-Two.html#using-the-prop.test-function",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Using the prop.test() Function",
    "text": "Using the prop.test() Function\n\nIf one sample has size \\(n_1\\) with \\(X_1\\) successes and the other sample has size \\(n_2\\) with \\(X_2\\) successes, then we can use the prop.test() function in R:\n\nprop.test(c(X1, X2), c(n1, n2), alt = [direction], correct = [option])\n\n\n\nThe alt option works the same as with t.test().\nWe can turn the continuity correction on or off with the correct option.\n\nUse correct = TRUE to apply a continuity correction.\nUse corret = FALSE if you do not want to apply a correction.\nIf you do not indicate any correct option, the default is TRUE."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-8",
    "href": "24-Hypothesis-Comparing-Two.html#question-8",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 8",
    "text": "Question 8\n\nUse the prop.test() function to verify your work in Question 6 and Question 7.\n\nSolution to Question 8\n\n\n# verify answers to question 6\nprop.test(??, ??, alt = ??, correct = ??)\n\n\n# verify answers to question 7\nprop.test(??, ??, alt = ??, correct = ??)"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-9",
    "href": "24-Hypothesis-Comparing-Two.html#question-9",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 9",
    "text": "Question 9\n\nIn this example, we use data collected from a matched pair designed study to determine whether smoking during pregnancy is associated with lower birth weight. In our study, we solicit volunteers that have already given birth to two babies. During one of the pregnancies, the parent smoked. During the other pregnancy, they did not smoke. Below is hypothetical data from such a study. A sample of \\(n=10\\) people volunteer to share their data with the researchers from which we have 10 different pairs of birth weights (in grams) summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225\n\n\n\n\n# data from study\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking births weights\n\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight\n\ndiff &lt;- no - smoker  # vector of matched pair differences\n\nResearchers will perform a hypothesis test to determine if the birth weight of babies born to a parent that smoked while pregnant is less, on average, compared to a babies whose parent did not smoke while they were pregnant. Test the claim using a 5% significance level.\n\nSolution to Question 9\n\n\nSet up the hypothesis for the test both in words and using appropriate notation.\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\nUse the data stored in no, smoker, and/or diff to calculate a reasonable test statistic.\n\n\n# compute a test statistic\n\n\n\n\n\nCalculate the p-value of the observed sample.\n\n\n# compute p-value\n\n\nWhat is the conclusion? Summarize the result in the context of this example."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-10",
    "href": "24-Hypothesis-Comparing-Two.html#question-10",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 10",
    "text": "Question 10\n\nThe data set storms in the dplyr package is summarized below.\n\nlibrary(dplyr)  # package may already be loaded\nsummary(storms)\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512            \n\n\n\nQuestion 10a\n\nWrite one possible claim that could be tested by a hypothesis test for a difference in two means. Then write the corresponding hypotheses and calculate the p-value of the observed samples using the t.test() function. What is the result of your test?\n\nSolution to Question 10a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \nt.test(??)\n\n\nSummarize Result:\n\n\n\n\n\nQuestion 10b\n\nWrite one possible claim that could be tested by a hypothesis test for a difference in two proportions. Then write the corresponding hypotheses and calculate the p-value of the observed samples using the prop.test() function. What is the result of your test?\n\nSolution to Question 10b\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \nprop.test(??)\n\n\nSummarize Result:\n\n\n\n\n\nQuestion 10c\n\nWrite one possible claim that could be tested by a hypothesis test for a single mean. Then write the corresponding hypotheses and calculate the p-value of the observed samples using the t.test() function. What is the result of your test?\n\nSolution to Question 10c\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \nt.test(??)\n\n\nSummarize Result:\n\n\n\n\n\nQuestion 10d\n\nWrite one possible claim that could be tested by a hypothesis test for a single proportion. Then write the corresponding hypotheses and calculate the p-value of the observed sample using a binomial distribution. What is the result of your test?\n\nSolution to Question 10d\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \n\n\nSummarize Result:"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#sec-appenda",
    "href": "24-Hypothesis-Comparing-Two.html#sec-appenda",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Appendix A: Summary of Hypothesis Testing",
    "text": "Appendix A: Summary of Hypothesis Testing\n\n\nState the hypotheses and identify (from the alternative claim in \\(H_a\\)) if it is a one or two-tailed test.\n\n\\(H_0\\) is the “boring” claim. Express using an equal sign \\(=\\).\n\\(H_a\\) is the claim we want to show is likely true. Use inequality sign (\\(&gt;\\), \\(&lt;\\), or \\(\\ne\\)).\nState both \\(H_0\\) and \\(H_a\\) in terms of population parameters such as \\(\\mu_1-\\mu_2\\) and \\(p_1-p_2\\).\n\nCompute the test statistic.\n\nIf the observed sample contradicts the null claim, the result is significant.\nA standardized test statistic measures how many SE’s the observed stat is from the null claim.\nA standardized test statistic with a large absolute value is supporting evidence to reject \\(H_0\\).\n\nUsing the null distribution, compute the p-value. The p-value is the probability of getting a sample with a test statistic as or more extreme than the observed sample assuming \\(H_0\\) is true.\n\nThe p-value is the area in one or both tails beyond the test statistic.\nThe p-value is a probability, so we have \\(0 &lt; \\mbox{p-value} &lt; 1\\).\nThe smaller the p-value, the stronger the evidence to reject \\(H_0\\).\n\nBased on the significance level, \\(\\alpha\\), make a decision to reject or not reject the null hypothesis\n\nIf p-value \\(\\leq \\alpha\\), we reject \\(H_0\\).\nIf p-value \\(&gt; \\alpha\\), we do not reject \\(H_0\\).\n\nSummarize the results in practical terms, in the context of the example.\n\nIf we reject \\(H_0\\), this means there is enough evidence to support the claim in \\(H_a\\).\nIf we do not reject \\(H_0\\), this means there is not evidence to reject \\(H_0\\) nor support \\(H_a\\). The test is inconclusive."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#sec-samp-size",
    "href": "24-Hypothesis-Comparing-Two.html#sec-samp-size",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Appendix B: Sample Sizes and the CLT",
    "text": "Appendix B: Sample Sizes and the CLT\n\n\nFor a single mean, we can use the CTL to perform a hypothesis test as long as:\n\nEither the population is symmetric or \\(n \\geq 30\\).\nIf the sample is symmetric, we can assume the population is symmetric.\n\nFor a difference in two means , we can use the CTL to perform a hypothesis test as long as:\n\nPopulation 1 is either symmetric or \\(n_1 \\geq 30\\), and\nPopulation 2 is either symmetric or \\(n_2 \\geq 30\\).\n\nFor a single proportion, we can use the CTL to perform a hypothesis test as long as:\n\nBoth \\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\).\n\nFor a difference in two proportions, we can use the CTL to perform a hypothesis test as long as:\n\nAll of \\(n_1\\hat{p}_1 \\geq 10\\), \\(n_1(1-\\hat{p}_1) \\geq 10\\), \\(n_2\\hat{p}_2 \\geq 10\\), and \\(n_2(1-\\hat{p}_2) \\geq 10\\) are satisfied."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#appendix-c-parametric-formulas",
    "href": "24-Hypothesis-Comparing-Two.html#appendix-c-parametric-formulas",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Appendix C: Parametric Formulas",
    "text": "Appendix C: Parametric Formulas\n\n\n\n\n\n\n\n\n\n\nParameter(s)\nSample Stat\nStandard Error\nDistribution\n\n\n\n\nA single mean  (\\(\\sigma^2\\) known)\n\\(\\bar{x}\\)\n\\(\\dfrac{\\sigma}{\\sqrt{n}}\\)\n\\(N(0,1)\\)\n\n\nA single mean  (\\(\\sigma^2\\) unknown)\n\\(\\bar{x}\\)\n\\(\\dfrac{{\\color{tomato}{s}}}{\\sqrt{n}}\\)\n\\({\\color{tomato}{t_{n-1}}}\\)\n\n\nA single proportion\n\\(\\hat{p}\\)\nNot Applicable\n\\(\\mbox{Binom}(n, p_0)\\)\n\n\nA difference in two means (unknown \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\))\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\dfrac{{\\color{tomato}{s_1}}^2}{n_1} + \\dfrac{{\\color{tomato}{s_2}}^2}{n_2}}\\)\n\\({\\color{tomato}{t_{n_{\\rm min}-1}}}\\)\n\n\nA difference in two proportions\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\\(\\sqrt{{\\color{tomato}{\\hat{p}_p}}(1 -{\\color{tomato}{\\hat{p}_p}})\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\\)\n\\(N(0,1)\\)\n\n\nMean difference between matched-pairs\n\\(\\bar{x}_{\\rm diff}\\)\n\\(\\dfrac{{\\color{tomato}{s_{\\rm diff}}}}{\\sqrt{n}}\\)\n\\({\\color{tomato}{t_{n-1}}}\\)\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#footnotes",
    "href": "24-Hypothesis-Comparing-Two.html#footnotes",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "",
    "text": "This example is based on research by University of Toronto Faculty of Dentistry. The data provided in this question has been modified from the original research.↩︎"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html",
    "href": "25-Error-Types-and-Power-of-Tests.html",
    "title": "6.5: Error Types and Power of Tests",
    "section": "",
    "text": "Potential Errors with Hypothesis Tests\nThe result of a hypothesis test has one of two possibilities:\nAs with confidence intervals, it is possible we do all of our analysis perfectly without any mistakes, but our conclusion is incorrect due to the randomness in sampling. Rarely, we are unlucky and dealt a biased sample, in which case we arrive at an incorrect conclusion.\nIn this section, we explore the following questions:\nThere are two possible errors in a hypothesis test:\nFor example, when a jury is deciding a case in court, the hypotheses would be:\nA jury can make two possible errors:\nThe significance level of a hypothesis test is the largest value of \\(\\mathbf{\\alpha}\\) we find acceptable for the probability for a type I error.\nWhen performing a hypothesis test at a significance level of \\(\\alpha\\), the rejection or critical region, denoted \\(\\mathcal{R}\\), is the set of all values of the test statistic for which we reject \\(H_0\\). The endpoint(s) of the region are called critical values.\nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-1",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-1",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 1",
    "text": "Question 1\n\nA hospital is testing to see whether a donated organ is a match for a recipient in need of an organ transplant.\n\n\\(H_0\\): The organ is not a match (boring).\n\\(H_a\\): The organ is a match (interesting).\n\nDescribe the type I and type II errors in this context. What are the practical consequences of making these errors?\n\nSolution to Question 1"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-2",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-2",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 2",
    "text": "Question 2\n\nA lab runs viral tests to see whether a person is currently infected with COVID-19.\n\n\\(H_0\\): The person is not currently infected with COVID-19 (boring).\n\\(H_a\\): The person is currently infected with COVID-19 (interesting).\n\nDescribe the type I and type II errors in this context. What are the practical consequences of making these errors?\n\nSolution to Question 2"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-3",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-3",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 3",
    "text": "Question 3\n\nThe cholesterol level of healthy men is normally distributed with a mean of 180 mg/dL and a standard deviation of 20 mg/dL, whereas men predisposed to heart disease have a mean cholesterol level of 300 mg/dL with a standard deviation of 30 mg/dL. The cholesterol level 225 mg/dL is used to demarcate healthy from predisposed men.\n\nQuestion 3a\n\nGiven that a man is healthy, what is the probability they are diagnosed as predisposed?\n\nSolution to Question 3a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 3b\n\nGiven that a man is not healthy, what is the probability they are not diagnosed as predisposed?\n\nSolution to Question 3b\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 3c\n\nWhich of the previous answers gives the probability of a type I error and which is for a type II error? Explain.\n\nSolution to Question 3c"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-4",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-4",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 4",
    "text": "Question 4\n\nSuppose we want to test whether a ten-sided die is fair (with sides numbered 0 to 9). Let \\(p\\) be the proportion of all rolls that land on an even number.\n\nQuestion 4a\n\nSet up the hypotheses to test our claim.\n\nSolution to Question 4a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 4b\n\nRoll the die 20 times, and record how many times it lands on an even number (0, 2, 4, 6, or 8). If you do not have a ten-sided die, use the code cell below to simulate rolling a fair, ten-sided die \\(n=20\\) times.\n\nSolution to Question 4b\n\n\n# run code cell if you do not have a ten-sided die\nsample(0:9, 20, replace = TRUE)\n\n\n\n\n\n\nQuestion 4c\n\nCalculate the p-value of your sample.\n\nSolution to Question 4c\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 4d\n\nWhat (if anything) can you conclude about the hypothesis at 10% significance level?\n\nSolution to Question 4d"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-5",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-5",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 5",
    "text": "Question 5\n\n\n\n\nCredit: Seobility CC BY-SA 4.0\n\n\nA company claims that only 3% of people who use their facial lotion develop an allergic reaction (a rash). You are suspicious of their claim based on hearing some of your friends had an allergic reaction, and you believe it is more than 3%. You pick a random sample of 50 people and have them try the lotion. If more than 3 out of the 50 people develop the rash, you will blow up social media with posts about the dishonesty of the company’s claim.\n\nQuestion 5a\n\nSet up hypotheses for this test.\n\nSolution to Question 5a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 5b\n\nExplain what type I and type II errors are in this case. Make sure you explain in the context of this example.\n\nSolution to Question 5b\n\n\n\n\n\n\n\nQuestion 5c\n\nWhat is the probability of making a type I error?\n\nSolution to Question 5c\n\n\n# code cell to help with calculations\n\n\n\n\n\n\n\nQuestion 5d\n\nIf you were to perform the hypothesis test at a 5% significance level, and you observe \\(X=4\\), what would be the result of the test?\n\nSolution to Question 5d\n\n\n\n\n\n\n\nQuestion 5e\n\nFor what values of \\(X\\) would you reject \\(H_0\\) at a 5% significance level?\n\nSolution to Question 5e"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-6",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-6",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 6",
    "text": "Question 6\n\nIn Question 4 we tested whether or not a ten-sided die is fair by rolling it 20 times and counting the number of rolls that land on an even number. If \\(p\\) is the proportion of all rolls that land on an even number, then we have\n\\[H_0: p = 0.5 \\qquad \\mbox{vs.} \\qquad H_a: p \\ne 0.5.\\]\n\nQuestion 6a\n\nIf you found only \\(X=7\\) rolls landed on an even number, what is the p-value?\n\nSolution to Question 6a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 6b\n\nFind the critical values and rejection region if we use a significance level of 10%.\n\nSolution to Question 6b\n\n\n# code cell to help with calculations"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-7",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-7",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 7",
    "text": "Question 7\n\nSuppose you are interested in the lengths of a certain species of snake in an ecosystem. Assume the lengths (in cm.) are normally distributed with unknown mean \\(\\mu\\), but the standard deviation of the population is known to be \\(\\sigma = 4\\) cm. It has been claimed that the mean length of this species is 25 cm. You believe the actual mean length is greater than 25 cm. You collect a random sample of 30 snakes. You will test using a significance level of \\(\\alpha = 0.05\\).\n\nQuestion 7a\n\nSet up hypotheses for the test.\n\nSolution to Question 7a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 7b\n\nFind the critical value, and give the rejection region.\n\nSolution to Question 7b\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 7c\n\nIf in fact \\(\\mu = 27\\) cm, what is the probability of making a type II error?\n\nSolution to Question 7c\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 7d\n\nWhat is the probability of correctly rejecting \\(H_0\\) when \\(H_a\\) is true?\n\nSolution to Question 7d"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#definition-of-the-power-of-a-test",
    "href": "25-Error-Types-and-Power-of-Tests.html#definition-of-the-power-of-a-test",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Definition of the Power of a Test",
    "text": "Definition of the Power of a Test\n\nThe power of a test is the probability of correctly rejecting \\(H_0\\).\n\\[{\\color{dodgerblue}{\\mbox{power} = P(\\mbox{Reject } H_0 \\  | \\  H_a \\mbox{ is true}) = 1 - {\\color{tomato}{\\beta}}}},\\]\nwhere \\(\\beta\\) denotes the probability of a type II error."
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-8",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-8",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 8",
    "text": "Question 8\n\nLet \\(X_1\\), \\(X_2\\), \\(\\ldots\\) , \\(X_{12}\\) be a random variable from a Bernoulli distribution with unknown probability \\(p\\). We test\n\\[H_0: p=0.3 \\qquad \\mbox{versus} \\qquad H_a: p &lt; 0.3.\\]\nWe will reject the null if the number of success \\(Y= X_1 + X_2 + \\ldots + X_{12} \\leq 1\\).\n\nQuestion 8a\n\nFind the probability of a type I error.\n\nSolution to Question 8a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 8b\n\nIf the alternative hypothesis is true, find an expression for the power, \\(1-\\beta\\), as a function of \\(p\\).\n\nSolution to Question 8b\n\n\n# code cell to help with calculations"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-9",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-9",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 9",
    "text": "Question 9\n\nYou draw a random sample \\(X_1, X_2, \\ldots , X_{10}\\) from an exponential distribution \\(f(x; \\lambda) = \\lambda e^{-\\lambda x}\\) (recall \\(\\mu = 1/\\lambda\\)). You will test\n\\[H_0: \\lambda = 0.25 \\qquad \\mbox{versus} \\qquad H_a: \\lambda &lt; 0.25.\\]\nYou decide you will reject the null hypothesis if at least 3 of the values of \\(X_i\\) are greater than 9.\n\nQuestion 9a\n\nCompute the probability of a type I error.\n\nSolution to Question 9a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 9b\n\nIf actually \\(\\lambda = 0.15\\), what is the power of this test?\n\nSolution to Question 9b\n\n\n# code cell to help with calculations"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html",
    "href": "Intro-to-Vectors-Dataframes.html",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "",
    "text": "Introduction\nUnderstanding the data types of the variables in our data set, and the structure of our data is crucial before we can identify what aspects might need to be cleaned and transformed so we can perform statistical analysis more efficiently.\nThis notebook is intended to be a brief overview of some fundamentals of working with data in R.\nThese topics are important. This notebook just scratches the surface on many concepts. If you do not find a complete answer here, there are free resources online that dig deeper and more completely. Below are two such recommended references.\nThe package dplyr contains a data set called storms. Let’s find some useful information about this data.\n# get a numerical summary of all variables\nsummary(storms)\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512\nTo store a data structure in the computer’s memory we must assign it a name.\nData structures can be stored using the assignment operator &lt;- or =.\nSome comments:\nIn the following code, we compute the mean of a vector. Why can’t we see the result after running it?\nw &lt;- storms$wind  # wind is now stored in w\nxbar.w &lt;- mean(w)  # compute mean wind speed and assign to xbar.w\nxbar.w  # print the mean wind speed to screen\n\n[1] 50.01741\nprint(xbar.w)  # print the mean with print() command\n\n[1] 50.01741\n# calculate, assign, and print standard deviation\n(s &lt;- sd(w))  # note ( ) around the entire command\n\n[1] 25.50103\nR has 6 basic data types:\nR operates on data structures. A data structure is simply some sort of “container” that holds certain kinds of information\nR has 5 basic data structures:\nSee R documentation for more info.\nThe read.table function imports data from file into R as a data frame.\nUsage: read.table(file, header = TRUE, sep = \",\")\nHere is an example reading a csv (comma separated file) with a header:\n# import data as data frame\nbike.store &lt;- read.table(file=\"https://raw.githubusercontent.com/CU-Denver-MathStats-OER/Statistical-Theory/main/Data/Transactions.csv\",\n                         header = TRUE,  # Keep column headers as names\n                         sep = \",\")  # comma as separator of columns\n\nglimpse(bike.store)\n\nRows: 20,000\nColumns: 13\n$ transaction_id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ product_id              &lt;int&gt; 2, 3, 37, 88, 78, 25, 22, 15, 67, 12, 5, 61, 3…\n$ customer_id             &lt;int&gt; 2950, 3120, 402, 3135, 787, 2339, 1542, 2459, …\n$ transaction_date        &lt;chr&gt; \"25-02-2017\", \"21-05-2017\", \"16-10-2017\", \"31-…\n$ online_order            &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, F…\n$ order_status            &lt;chr&gt; \"Approved\", \"Approved\", \"Approved\", \"Approved\"…\n$ brand                   &lt;chr&gt; \"Solex\", \"Trek Bicycles\", \"OHM Cycles\", \"Norco…\n$ product_line            &lt;chr&gt; \"Standard\", \"Standard\", \"Standard\", \"Standard\"…\n$ product_class           &lt;chr&gt; \"medium\", \"medium\", \"low\", \"medium\", \"medium\",…\n$ product_size            &lt;chr&gt; \"medium\", \"large\", \"medium\", \"medium\", \"large\"…\n$ list_price              &lt;dbl&gt; 71.49, 2091.47, 1793.43, 1198.46, 1765.30, 153…\n$ standard_cost           &lt;dbl&gt; 53.62, 388.92, 248.82, 381.10, 709.48, 829.65,…\n$ product_first_sold_date &lt;int&gt; 41245, 41701, 36361, 36145, 42226, 39031, 3416…\n?read.table\nSometimes we need to know if the elements of an object satisfy certain conditions. This can be determined using the logical operators &lt;, &lt;=, &gt;, &gt;=, ==, !=.\nExecute the following commands in R and see what you get.\na &lt;- seq(2, 16, by = 2) # creating the vector a\na\n\n[1]  2  4  6  8 10 12 14 16\n\na &gt; 10\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\na &lt;= 4\n\n[1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\na == 10\n\n[1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\na != 10\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-load",
    "href": "Intro-to-Vectors-Dataframes.html#sec-load",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Loading Packages with the library() Command",
    "text": "Loading Packages with the library() Command\n\nTo explore some fundamentals of working with data in R, we will use the storms data set which is located in the package dplyr.\n\nThe dplyr package is already installed in Google Colaboratory\nWe still need to use a library command to load the package.\nRun the code cell below to load the dplyr package.\n\n\n# load the library of functions and data in dplyr\nlibrary(dplyr)\n\n\n\n\n\n\n\nCaution\n\n\n\nEach time you connect or restart a session, you will need to run a library() command in order to access data and scripts in a package."
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-help",
    "href": "Intro-to-Vectors-Dataframes.html#sec-help",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Help Documentation",
    "text": "Help Documentation\n\nThe functions introduced in this document have robust help documentation with lots of options to customize. If you want to view help documentation for any of the functions used in this document, run commands such?typeof, ?is.numeric, ?read.table, and so on.\n\n# access help documentation for storms\n?storms  # side panel should open with help manual for storms\n\n\n# access help documentation for typeof\n?typeof"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-na",
    "href": "Intro-to-Vectors-Dataframes.html#sec-na",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Missing Data",
    "text": "Missing Data\n\nA missing value occurs when the value of something isn’t known. R uses the special object NA to represent missing value. If you have a missing value, you should represent that value as NA. Note: The character string \"NA\" is not the same thing as NA.\n\nThe storms data has properly coded 14,382 missing values for category since storms that are not hurricanes do not have a category.\nThe storms data has properly coded 9,512 missing values for each of tropicalstorm_force_diameter and hurricane_force_diameter since these value only began being recorded in 2004."
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-typeof",
    "href": "Intro-to-Vectors-Dataframes.html#sec-typeof",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Checking Data Type Using typeof()",
    "text": "Checking Data Type Using typeof()\n\n\nThe typeof() function returns the R internal type or storage mode of any object.\n\n\ntypeof(1.0)\n\n[1] \"double\"\n\ntypeof(2)\n\n[1] \"double\"\n\ntypeof(3L)\n\n[1] \"integer\"\n\ntypeof(\"hello\")\n\n[1] \"character\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\ntypeof(storms$status)\n\n[1] \"integer\"\n\ntypeof(storms$year)\n\n[1] \"double\"\n\ntypeof(storms$name)\n\n[1] \"character\""
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-isdatatype",
    "href": "Intro-to-Vectors-Dataframes.html#sec-isdatatype",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Investigating Data Types with is.numeric()",
    "text": "Investigating Data Types with is.numeric()\n\n\nThe is.numeric(x) function tests whether or not an object x is numeric.\nThe is.character(x) function tests whether x is a character or not.\nThe is.factor(x) function tests whether x is a factor or not.\n\n\n\n\n\n\n\nNote\n\n\n\nCategorical data is typically stored as a factor in R.\n\n\n\nis.numeric(storms$year)  # year is numeric\n\n[1] TRUE\n\nis.numeric(storms$category)  # category is also numeric\n\n[1] TRUE\n\nis.numeric(storms$name)  # name is not numeric\n\n[1] FALSE\n\nis.character(storms$name)  # name is character string\n\n[1] TRUE\n\n\n\nis.numeric(storms$status)  # status is not numeric\n\n[1] FALSE\n\nis.character(storms$status)  # status is not a character\n\n[1] FALSE\n\nis.factor(storms$status)  # status is a factor which is categorical\n\n[1] TRUE\n\n\n\nThe function str(x) provides information about the levels or classes of x.\n\n\nstr(storms$status)\n\n Factor w/ 9 levels \"disturbance\",..: 7 7 7 7 7 7 7 7 8 8 ..."
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-changetype",
    "href": "Intro-to-Vectors-Dataframes.html#sec-changetype",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Changing Data Types",
    "text": "Changing Data Types\n\n\nConverting to Categorical Data with factor()\n\n\nSometimes we think a variable is one data type, but it is actually being stored (and thus interpreted by R) as a different data type.\nOne common issue is categorical data is stored as characters. We would like observations with the same values to be group together.\nThe status variable in storms is being properly stored as a factor!\nThe category variable in storms is being stored as a numeric since it is ordinal.\nWith ordinal categories, we may choose to keep it stored as numeric, or we may prefer to treat them as factors.\n\n\nsummary(storms$category)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   1.000   1.898   3.000   5.000   14382 \n\n\n\nThe summary of category computes statistics such as mean and median.\nTypically with categorical data, we prefer to count how many observations are in each class of the variable.\nIn the code cell below, we convert category to a factor, and then observe the resulting summary.\n\n\nstorms$category &lt;- factor(storms$category)\nsummary(storms$category)\n\n    1     2     3     4     5  NA's \n 2478   973   579   539   115 14382 \n\n\n\n\nConverting Data Types with as.numeric(), as.integer(), etc.\n\nFrom the summary of the storms data set we first found above, we see that the variables year and month are being stored as double. These variables actually are integer values.\nWe can convert another variable of one format into another format using as.[new_datatype]()\n\nFor example, to convert to year to integer, we use as.integer(storms$year).\nTo convert a data type to character, we can use as.character(x).\nTo convert to a decimal (double), we can use as.numeric(x)\n\n\ntypeof(storms$year)\n\n[1] \"double\"\n\ntypeof(storms$month)\n\n[1] \"double\"\n\nstorms$year &lt;- as.integer(storms$year)\nstorms$month &lt;- as.integer(storms$month)\ntypeof(storms$year)\n\n[1] \"integer\"\n\ntypeof(storms$month)\n\n[1] \"integer\""
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-vectors",
    "href": "Intro-to-Vectors-Dataframes.html#sec-vectors",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Vectors",
    "text": "Vectors\n\nA vector is a single-dimensional set of data of the same type.\n\nCreating Vectors from Scratch\n\nThe most basic way to create a vector is the combine function c. The following commands create vectors of type numeric, character, and logical, respectively.\n\nx1 &lt;- c(1, 2, 5.3, 6, -2, 4)\nx2 &lt;- c(\"one\", \"two\", \"three\")\nx3 &lt;- c(TRUE, TRUE, FALSE, TRUE)\nx4 &lt;- c(TRUE, 3.4, \"hello\")\ntypeof(x1)\n\n[1] \"double\"\n\ntypeof(x2)\n\n[1] \"character\"\n\ntypeof(x3)\n\n[1] \"logical\"\n\ntypeof(x4)\n\n[1] \"character\"\n\n\n\nWe can check the data structure of an object using commands such as is.vector(), is.list(), is.matrix(), and so on.\n\n\nis.list(x1)\n\n[1] FALSE\n\nis.vector(x1)\n\n[1] TRUE\n\nis.list(x4)\n\n[1] FALSE\n\nis.vector(x4)\n\n[1] TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-dataframe",
    "href": "Intro-to-Vectors-Dataframes.html#sec-dataframe",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Data Frames",
    "text": "Data Frames\n\nData frames are two-dimensional data objects and are the fundamental data structure used by most of R’s libraries of functions and data sets.\n\nTabular data is tidy if each row corresponds to a different observation and column corresponds to a different variable.\n\nEach column of a data frame is a variable (stored as a vector). If the variable:\n\nIs measured or counted by a number, it is a quantitative or numerical variable.\nGroups observations into different categories or rankings, it is a qualitative or categorical variable.\n\n\nCreating Data Frames from Scratch\n\nData frames are created by passing vectors into the data.frame() function.\nThe names of the columns in the data frame are the names of the vectors you give the data.frame function.\nConsider the following simple example.\n\n# create basic data frame\nd &lt;- c(1, 2, 3, 4)\ne &lt;- c(\"red\", \"white\", \"blue\", NA)\nf &lt;- c(TRUE, TRUE, TRUE, FALSE)\ndf &lt;- data.frame(d,e,f)\ndf\n\n  d     e     f\n1 1   red  TRUE\n2 2 white  TRUE\n3 3  blue  TRUE\n4 4  &lt;NA&gt; FALSE\n\n\n\n\nNaming Column Headers\n\nThe columns of a data frame can be renamed using the names() function on the data frame.\n\n# name columns of data frame\nnames(df) &lt;- c(\"ID\", \"Color\", \"Passed\")\ndf\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE\n\n\nThe columns of a data frame can be named when you are first creating the data frame by using [new_name] = [orig_vec_name] for each vector of data.\n\n# create data frame with better column names\ndf2 &lt;- data.frame(ID = d, Color = e, Passed = f)\ndf2\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-checkstructure",
    "href": "Intro-to-Vectors-Dataframes.html#sec-checkstructure",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Checking Data Structure",
    "text": "Checking Data Structure\n\n\nThe is.matrix(x) function tests whether or not an object x is a matrix.\nThe is.vector(x) function test whether x is a vector.\nThe is.data.frame(x) function test whether x is a data frame.\n\n\nis.matrix(df)\n\n[1] FALSE\n\nis.vector(df)\n\n[1] FALSE\n\nis.data.frame(df)\n\n[1] TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-extractname",
    "href": "Intro-to-Vectors-Dataframes.html#sec-extractname",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Extracting a Column By Name",
    "text": "Extracting a Column By Name\n\nThe column vectors of a data frame may be extracted using $ and specifying the name of the desired vector.\n\ndf$Color would access the Color column of data frame df.\n\n\ndf$Color  # prints column of data frame df named Color\n\n[1] \"red\"   \"white\" \"blue\"  NA"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-indexing",
    "href": "Intro-to-Vectors-Dataframes.html#sec-indexing",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Slicing Rows and Columns By Indexing",
    "text": "Slicing Rows and Columns By Indexing\n\nPart of a data frame can also be extracted by thinking of at as a general matrix and specifying the desired rows or columns in square brackets after the object name.\n\nNote R starts with index 1 which is different from Python which indexes starting from 0.\n\nFor example, if we had a data frame named df:\n\ndf[1,] would access the first row of df.\ndf[1:2,] would access the first two rows of df.\ndf[,2] would access the second column of df.\ndf[1:2, 2:3] would access the information in rows 1 and 2 of columns 2 and 3 of df.\n\n\ndf[,2]  # second column is Color\n\n[1] \"red\"   \"white\" \"blue\"  NA     \n\n\n\ndf[2,]  # second row of df\n\n  ID Color Passed\n2  2 white   TRUE\n\n\n\ndf[1:2,2:3]  # first and second rows of columns 2 and 3\n\nIf you need to select multiple columns of a data frame by name, you can pass a character vector with column names in the column position of [].\n\ndf[, c(\"ID\", \"Passed\")] would extract the ID and Passed columns of df.\n\n\ndf[, c(\"ID\", \"Passed\")]\n\n  ID Passed\n1  1   TRUE\n2  2   TRUE\n3  3   TRUE\n4  4  FALSE\n\n\n\ndf[, c(1, 3)]  # another we to pick columns 1 and 3\n\n  ID Passed\n1  1   TRUE\n2  2   TRUE\n3  3   TRUE\n4  4  FALSE\n\n\n\n# another we to pick columns 1 and 3\ndf[, -2]  # exclude column 2\n\n  ID Passed\n1  1   TRUE\n2  2   TRUE\n3  3   TRUE\n4  4  FALSE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-andor",
    "href": "Intro-to-Vectors-Dataframes.html#sec-andor",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "And and Or Statements",
    "text": "And and Or Statements\n\nMore complicated logical statements can be made using & and |.\n\n& means “and”\n\nBoth statements must be true for state1 & state2 to return TRUE.\n\n| means “or”\n\nOnly one of the the two statements must be true for state1 | state2 to return TRUE.\nIf both statements are true in an “or” statement, the statement is also TRUE.\n\n\nBelow is a summary of “and” and “or” logic:\n\nTRUE & TRUE returns TRUE\nFALSE & TRUE returns FALSE\nFALSE & FALSE returns FALSE\nTRUE | TRUE returns TRUE\nFALSE | TRUE returns TRUE\nFALSE | FALSE returns FALSE\n\n\n# relationship between logicals & (and), | (or)\nTRUE & TRUE\n\n[1] TRUE\n\nFALSE & TRUE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\nTRUE | TRUE\n\n[1] TRUE\n\nFALSE | TRUE\n\n[1] TRUE\n\nFALSE | FALSE\n\n[1] FALSE\n\n\nExecute the following commands in R and see what you get.\n\nb &lt;- 3  # b is equal to the number 3\n\n# complex logical statements\n(b &gt; 6) & (b &lt;= 10)  # FALSE and TRUE\n\n[1] FALSE\n\n(b &lt;= 4) | (b &gt;= 12)  # TRUE or FALSE\n\n[1] TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-logicindex",
    "href": "Intro-to-Vectors-Dataframes.html#sec-logicindex",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Logical Indexing",
    "text": "Logical Indexing\n\nWe can use a logical statement as an index to extract certain entries from a vector or data frame. For example, if we want to to know the product_id (column 2), brand (column 7), product_line (column 8), and list_price (column 11) of all transactions that have a list_price greater than $2,090, we can run the code cell below.\n\nWe use a logical index for the row to extract just the rows that have a list_price value strictly greater than 2090.\nWe indicate we want to keep just columns 2, 7 through 8, and 11 with the column index c(2, 7:8, 11).\nWe store the results to a new data frame named expensive.\nFinally, we print the first 6 rows of our new data frame with the head() function to check the results.\n\n\nexpensive &lt;- bike.store[bike.store$list_price &gt; 2090, c(2, 7:8, 11)]\nhead(expensive)\n\n    product_id         brand product_line list_price\n2            3 Trek Bicycles     Standard    2091.47\n16           3 Trek Bicycles     Standard    2091.47\n69          38 Trek Bicycles     Standard    2091.47\n154          3 Trek Bicycles     Standard    2091.47\n165          3 Trek Bicycles     Standard    2091.47\n188          3 Trek Bicycles     Standard    2091.47"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-cc",
    "href": "Intro-to-Vectors-Dataframes.html#sec-cc",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "Overview-of-Plots.html",
    "href": "Overview-of-Plots.html",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "",
    "text": "Introduction\nPlots can provide a useful visual summary of the data. Sometimes, a nice plot or two is all that is need for statistical analysis. In this document, we cover a basic overview of creating some plots in R.\nHere’s a link to a more thorough coverage of plotting in R: https://r-graph-gallery.com/index.html.\nR packages are a collection functions, sample data, and/or other code scripts. R installs a set of default packages during installation.\nRun the code cell below to get a list of all default R packages that are already installed.\n# See a list of installed default packages\nallpack &lt;- installed.packages()\nrownames(allpack)\nThe package dplyr contains a data set called storms. Let’s find some useful information about this data.\n# be sure to run the code cell above first\n# so you have loaded the dplyr package\n?storms\n# See a summary of all variables\nsummary(storms)\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512\nOften a graph or plot is a more preferred format to summarize a variable than a summary statistics. The documentation below explains we could graphically summarize the quantitative variable pressure.\nQualitative (also called categorical) variables required other types of plots. For example, we cannot create a density or boxplot for a qualitative variable. Qualitative variables may be stored as characters (such as the status variable) or values (such as the category variable). This brings up a good question:\nImagine we would like to compare the wind speeds of storms by status. In this case, we would like to compare a quantitative variable (wind) for different classes of a qualitative variable (status).\nImagine we would like to compare the number of different category hurricanes that occurred in each month. In this case, we would like to compare two qualitative variables, namely category and month.\nImagine we would like to compare the wind speeds (wind) to the pressure (pressure). In this case, we would like to compare two quantitative variables.\nA scatter plot can be used to identify the relationship between two quantitative variables.\n# create a scatter plot\n# first variable wind is response (y-axis)\n# second variable pressure is predictor (x-axis)\n\nplot(wind ~ pressure,  # response ~ predictor(s)\n     data = storms,  # data frame name\n     main = \"Relation of Pressure and Wind Speed of Storms\",  # main title\n     xlab = \"Pressure (in millibars)\",  # horizontal axis label\n     ylab = \"Wind Speed (in knots)\")  # vertical axis label\npar(mfrow = c(2, 2))  # create a 2 x 2 array of plots\n\n# the next 5 plots created will be arranged in the array\nboxplot(storms$wind)  # create boxplot of wind speed\n\n# code below creates a histogram of wind speed\n# we can add many options to customize\nhist(storms$wind, xlab = \"wind speed (in knots)\",   # x-axis label\n     ylab = \"Frequency\",  # y-axis label\n     main = \"Distribution of Storm Wind Speed 1975-2020\",  # main label\n     col = \"steelblue\")  # change color of bars\n\nplot(storms$status, col = \"gold\")  # plots status, which is categorical\n\nplot(wind ~ pressure, data = storms)  # plots two numerical variables\n# create a table of status counts\n# we will pull of the row names of the table\n# as the labels in the legend\nstatus.table &lt;- table(storms$status)\n\nplot(wind ~ status,  # quantitative first ~ categorical second\n     data = storms,  # name of data frame\n     col = my.colors,  # fill colors colors\n     ylab = \"Wind speed in knots\",  # vertical axis label\n     main = \"Wind Speeds of Storms by Status\")  # main title\n\n# we can add a legend to identify which plot is which storm status\nlegend(x = \"topright\",  # place legend in top right corner\n       legend=rownames(status.table),  # each row of table is label in legend\n       fill = my.colors)  # fill colors\nThe previous plots were created using R’s base graphics system.\nA fancier alternative is to construct plots using the ggplot2 package.\nIn its simplest form, to construct a (useful) plot in ggplot2, you need to provide:"
  },
  {
    "objectID": "Overview-of-Plots.html#help-documentation",
    "href": "Overview-of-Plots.html#help-documentation",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Help Documentation",
    "text": "Help Documentation\n\nThe plotting functions introduced in this document have robust help documentation with lots of options to customize your plots. If you want to view help documentation for any of the functions used in this document, run commands such?hist, ?plot, ?table, and so on.\n\n# access help documentation for hist\n?hist  #Side panel should open with help doc"
  },
  {
    "objectID": "Overview-of-Plots.html#loading-packages-with-the-library-command",
    "href": "Overview-of-Plots.html#loading-packages-with-the-library-command",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Loading Packages with the library() Command",
    "text": "Loading Packages with the library() Command\n\nEach time we start or restart a new session and want to access the library of functions and data in the package, we need to load the library of files in the package with the library() command.\nTo demonstrate how to create common statistical plots in R, we will use the storms data set which is located in the package dplyr.\n\nThe dplyr package is already installed in Google Colaboratory\nWe still need to use a library command to load the package.\nRun the code cell below to load the dplyr package.\n\n\n# load the library of functions and data in dplyr\nlibrary(dplyr)"
  },
  {
    "objectID": "Overview-of-Plots.html#reloading-packages-when-restarting-a-session",
    "href": "Overview-of-Plots.html#reloading-packages-when-restarting-a-session",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Reloading Packages When Restarting a Session",
    "text": "Reloading Packages When Restarting a Session\n\nIf we take a break in our work, it is possible our R session will time out and close. Each time we restart an R session, we will need to rerun library() commands in order reload any packages we plan to use.\nThe same caution applies to any objects, vectors, or data frames we create or edit in an R session. If a session times out, and we want to use an object x that we previously created, we will need to run the code cell(s) where object x is created again before we can refer back to x in the current session.\nBE SURE YOU RUN THE COMMAND library(dplyr) BEFORE ATTEMPTING TO RUN ANY OF THE CODE CELLS BELOW!"
  },
  {
    "objectID": "Overview-of-Plots.html#histograms",
    "href": "Overview-of-Plots.html#histograms",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Histograms",
    "text": "Histograms\n\nThe hist function can be used create a histogram of a numerical vector.\n\nSee histogram documentation: https://r-graph-gallery.com/histogram.html\nLike making colorful plots? Here’s a guide to colors in R.\nWe use a $ symbol to indicate the name of the variable in storms we will access in the plot.\n\n\nhist(storms$pressure,  # plot pressure variable in storms data\n     xlab = \"storm pressure (in millibars)\",  # x-axis label\n     main = \"Distribution of Storm Pressure\",  # main title\n     breaks = 10,  # number of breaks or bins\n     col = \"aquamarine4\")  # color of bars"
  },
  {
    "objectID": "Overview-of-Plots.html#density-plots",
    "href": "Overview-of-Plots.html#density-plots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Density plots",
    "text": "Density plots\n\nA histogram is more sensitive to its options. For example, a histogram with 3 breaks may tell a different story than plotting the same data with 20 breaks.\nThus, we may prefer to use a density plot.\n\nFirst compute density of pressure.\n\n\nFor more information, see density help documentation.\n\n\nThe plot() function will then create a density plot.\n\n\nFor more advanced density plots see https://r-graph-gallery.com/density-plot.html.\nIf a variable is categorical, plot() will create a different plot, namely a bar chart.\nplot() can also be used to generate a plot to compare two different variables.\nThe output of plot() depends on the type and number of variables that we input in the function.\n\n\n# approximate densities and then plot\nplot(density(storms$pressure),\n     xlab = \"storm pressure (in millibars)\",  # horizontal axis label\n     main = \"Distribution of Storm Pressure\")  # main title"
  },
  {
    "objectID": "Overview-of-Plots.html#boxplots",
    "href": "Overview-of-Plots.html#boxplots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Boxplots",
    "text": "Boxplots\n\nBoxplots are another useful plot for presenting the distribution of a quantitative variable using quartiles and the five number summary.\n\nSee boxplot documentation at https://r-graph-gallery.com/boxplot.html.\nRun the command ?boxplot to see more options.\n\n\n# create boxplot of quantitative variable\nboxplot(storms$pressure,\n        ylab = \"storm pressure (in millibars)\",  # horizontal axis label\n        col = \"gold\",  # color of box\n        main = \"Distribution of Storm Pressure\")  # main title\n\n\n\n\n\nChanging the Layout of Boxplots\n\n\n# horizontally aligned boxplot\nboxplot(storms$pressure,\n        horizontal = TRUE,  # display horizontally\n        xlab = \"storm pressure (in millibars)\",  # horizontal axis label\n        main = \"Distribution of Storm Pressure\",  # main title\n        col = \"azure3\")  # color"
  },
  {
    "objectID": "Overview-of-Plots.html#checking-the-data-type",
    "href": "Overview-of-Plots.html#checking-the-data-type",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Checking the Data Type",
    "text": "Checking the Data Type\n\nThe typeof() command can help identify what is the type of a variable.\n\ntypeof(storms$status)\n\n[1] \"integer\"\n\ntypeof(storms$category)\n\n[1] \"double\"\n\n\n\nData Types\n\nFrom the output above, we see:\n\nThe variable status is initially read as an integer.\nThe individual values are strings of characters such as “tropical storm” or “hurricane”.\nThe summary statistics of status are counts that are stored as integers.\nThe variable category is initially read as double or decimal values.\nThe individual values are ordinal integers “1”, “2”, “3”, “4”, and “5” for category of hurricane.\nThere are 14,2328 NA (or missing) values corresponding to the observations that are not hurricanes.\nThe summary statistics of category (such as the mean) are stored decimals.\nHowever, we would like to treat category as a qualitative variable and plot how many storms fall into each category."
  },
  {
    "objectID": "Overview-of-Plots.html#caution-with-data-types-and-using-plot",
    "href": "Overview-of-Plots.html#caution-with-data-types-and-using-plot",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Caution with Data Types and Using plot()",
    "text": "Caution with Data Types and Using plot()\n\nIf we try to use the general plot() function, R will give its best guess at which plot makes the most sense to display the data. If the data is stored as the wrong data type, plot() will not work as we might expect.\n\nRun the two code cells below, and notice the following:\n\nThe output of the plot(storms$status) looks like a reasonable bar chart.\nThe output of plot(storms$category) does not nicely summarize the counts of how many storms are in each category.\n\n\n\nplot(storms$status)  # plot of status\n\n\n\n\n\nplot(storms$category)  # plot of category"
  },
  {
    "objectID": "Overview-of-Plots.html#creating-bar-charts-from-tables",
    "href": "Overview-of-Plots.html#creating-bar-charts-from-tables",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creating Bar Charts From Tables",
    "text": "Creating Bar Charts From Tables\n\nThe table() function will count the number of times a value (or string of characters) occurs in a vector or variable.\n\nOfficial Documentation Page for table()\nAnother nice resource with examples\n\nOne way to improve the initial plot of categories above is as follows:\n\nFirst use the table() command to count how many storms are in each category.\nThen create a bar chart using the barplot() function.\n\n\ncat.table &lt;- table(storms$category)  # create table of counts\ncat.table  # print table to screen\n\n\n   1    2    3    4    5 \n2478  973  579  539  115 \n\n\n\n# create bar chart from table counts\nbarplot(cat.table,  # input table from previous code cell\n        main = \"Distribution of Hurricane Categories\",  # main title\n        xlab = \"Hurricane Category\",  # horizontal axis label\n        ylab = \"Frequency\",  # vertical axis label\n        col = \"steelblue\")  # fill color of bars"
  },
  {
    "objectID": "Overview-of-Plots.html#relative-frequency-tables-and-bar-charts",
    "href": "Overview-of-Plots.html#relative-frequency-tables-and-bar-charts",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Relative Frequency Tables and Bar Charts",
    "text": "Relative Frequency Tables and Bar Charts\n\nIf instead of plotting the number of hurricanes in each category we wish to plot the proportion of all hurricanes in each category, we can use the prop.table() function to convert the table counts to proportions relative to the grand total.\nRun the two code cells below to create a relative frequency bar chart.\n\nWe input our previous table of counts, cat.table, into the function prop.table() to convert counts to proportions.\nThen we create a bar chart of the resulting proportions.\n\n\ncat.prop &lt;- prop.table(cat.table)  # create table of proportions\nbarplot(cat.prop,  # input table of proportions\n        main = \"Relative Frequency of Hurricane Categories\",  # main title\n        xlab = \"Hurricane Category\",  # horizontal axis label\n        ylab = \"Proportion\",  # vertical axis label\n        col = \"steelblue\")  # fill color of bars\n\n\n\n\n\n\n\n\n\n\nCaution with prop.table()\n\n\n\n\n\nThe input into prop.table() must be a table rather than a vector or data frame column.\nThe code cell below does return a relative frequency table as we would expect since we did not first create a table of counts from storms$category.\n\n\n\n\ntemp &lt;- prop.table(storms$category)  # do not input a vector\nhead(temp)  #  print first several entries of result\n\n[1] NA NA NA NA NA NA"
  },
  {
    "objectID": "Overview-of-Plots.html#pie-charts-with-pie",
    "href": "Overview-of-Plots.html#pie-charts-with-pie",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Pie Charts with pie()",
    "text": "Pie Charts with pie()\n\nPie charts can also be used to illustrate the distribution of one qualitative variable.\n\nSee https://r-graph-gallery.com/pie-plot.html.\nFor help and a list of options, you can run ?pie.\n\n\n?pie\n\n\n# create pie chart of one qualitative variable\npie(cat.table,  # input table\n    main = \"Distribution of Hurricane Categories\")  # main title"
  },
  {
    "objectID": "Overview-of-Plots.html#converting-to-a-factor-and-then-plot",
    "href": "Overview-of-Plots.html#converting-to-a-factor-and-then-plot",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Converting to a factor() and Then plot()",
    "text": "Converting to a factor() and Then plot()\n\nOne common issue with a qualitative variable is that it is often stored as the wrong datatype.\n\nQualitative data should typically be stored as a factor.\n\nAnother way we can create a bar chart of the counts in each category is to:\n\nFirst convert the qualitative variable to a factor.\nThen use plot() to create an appropriate plot.\n\nRun the code cell below to first see the summary output of the category variable after converting it to a factor.\n\n# creates a copy of storms data set\n# so we don't overwrite original storms\nstorms2 &lt;- storms  \n\nstorms2$category &lt;- factor(storms$category)  # convert category to factor\nsummary(storms2$category)  # get new summary of categories\n\n    1     2     3     4     5  NA's \n 2478   973   579   539   115 14382 \n\n\nNotice the summary is a table of counts in each hurricane category.\n\nOnce the variable status is converted to a factor, the plot() function will know to use a bar chart to give a summary display.\n\n\n# create bar chart from counts of a factor\nplot(storms2$category,  # input a factor\n     main = \"Distribution of Hurricane Category\",  # main title\n     xlab = \"Hurrican Category\",  # horizontal axis label\n     ylab = \"Frequency\",  # vertical axis label\n     col = \"steelblue\")  # color of fill of ba\n\n\n\n\n\nRecall without first changing category to a factor, plot() will create a different graph.\n\n\n# default plot of category when not first converted to factor\nplot(storms$category)"
  },
  {
    "objectID": "Overview-of-Plots.html#side-by-side-boxplots",
    "href": "Overview-of-Plots.html#side-by-side-boxplots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Side by Side Boxplots",
    "text": "Side by Side Boxplots\n\nThere are many classes of storms status in storms.\nIn the storms data:\n\nwind is a quantitative variable.\nstatus is a qualitative variable.\nWe can use the default plot() function to create a side by side boxplots.\n\n\n# create a vector of fill colors\n# one color for each status type.\nmy.colors &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \n               \"blue\", \"gold\", \"cyan\", \"pink\", \"orange\")\n\nplot(wind ~ status,  # quantitative first ~ categorical second\n     data = storms,  # name of data frame\n     col = my.colors,  # fill colors\n     main = \"Wind Speeds of Storms by Status\")  # main title\n\n\n\n\n\nAdding a Legend to Plots\n\n\nThere are a lot of different status of storms.\nIt is not easy (or possible) to tell which boxplot corresponds to which storm status.\nAdding a legend to the plot will help!\n\n\n# create a table of status counts\n# we will pull of the row names of the table\n# as the labels in the legend\nstatus.table &lt;- table(storms$status)\n\nplot(wind ~ status,  # quantitative first ~ categorical second\n     data = storms,  # name of data frame\n     col = my.colors,  # fill colors colors\n     ylab = \"Wind speed in knots\",  # vertical axis label\n     main = \"Wind Speeds of Storms by Status\")  # main title\n\n# we can add a legend to identify which plot is which storm status\nlegend(x = \"topright\",  # place legend in top right corner\n       legend=rownames(status.table),  # each row of table is label in legend\n       fill = my.colors)  # fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#subsetting-data-by-category",
    "href": "Overview-of-Plots.html#subsetting-data-by-category",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Subsetting Data by Category",
    "text": "Subsetting Data by Category\n\nThere are many classes of storms status in storms. Often, we want to only focus on a smaller subset of classes. We can restrict our attention to compare the wind speeds of three of the classes: “tropical storm”, “tropical depression”, and “hurricane”.\n\nWe can subset storms data frame into three separate data frames, one for each status of storm, using the subset() function.\nCurious to learn more about subset? Run ?subset in a code cell to access help documentation.\nThen we can create three separate boxplots of the wind speeds for each status.\n\n\n# split data by storm status\nhur &lt;- subset(storms,  # data frame name\n              status == \"hurricane\",  # logical test to select observations\n              select = wind)  # which quantitative variable(s) to select\n\ntrop.storm &lt;- subset(storms, \n                     status == \"tropical storm\",  # tropical storms\n                     select = wind)\ntrop.dep &lt;- subset(storms, \n                   status == \"tropical depression\",   # tropical depressions\n                   select = wind)\n\n# create side by side boxplot\n# for each of the three subsets\nboxplot(hur$wind, trop.storm$wind, trop.dep$wind, \n        main = \"Windspeed of Storms\", \n        names = c(\"Hurricanes\", \"Tropical Storms\", \"Tropical Depressions\"), \n        col = c(\"red\", \"blue\", \"green\"), \n        xlab = \"Wind speed in knots\", \n        horizontal = TRUE)"
  },
  {
    "objectID": "Overview-of-Plots.html#creating-contingency-or-two-way-table",
    "href": "Overview-of-Plots.html#creating-contingency-or-two-way-table",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creating Contingency or Two-Way Table",
    "text": "Creating Contingency or Two-Way Table\n\nThe command table(x) will count the number of times a value (or string of characters) occurs in a vector x.\n\nOfficial Documentation Page for table()\nNice resource with examples\n\nThe command table(x, y) will similarly create a contingency (or two-way) table to jointly compare counts of x and y.\n\n# create a contingency table for status and month\ncon.table &lt;- table(storms$category, storms$month) \ncon.table  # print output to screen\n\n   \n       1    4    5    6    7    8    9   10   11   12\n  1    5    0    0   18  140  581 1099  462  140   33\n  2    0    0    0    0   25  198  571  150   29    0\n  3    0    0    0    0   18  113  346   86   16    0\n  4    0    0    0    0   18  114  295   88   24    0\n  5    0    0    0    0    1   32   69   13    0    0"
  },
  {
    "objectID": "Overview-of-Plots.html#creating-grouped-frequency-bar-charts",
    "href": "Overview-of-Plots.html#creating-grouped-frequency-bar-charts",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creating Grouped Frequency Bar Charts",
    "text": "Creating Grouped Frequency Bar Charts\n\nAfter creating a two-way table, we can present the results visually in a grouped bar chart.\n\nSee documentation at https://r-graph-gallery.com/211-basic-grouped-or-stacked-barplot.html.\n\n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create side by side bar chart\nbarplot(con.table,  # use counts from contingency table\n        beside = TRUE,  # groups side-by-side\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # fill color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add a legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels from row name in contingency table\n       fill = my.colors2)  # use same fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#grouped-frequency-bar-charts",
    "href": "Overview-of-Plots.html#grouped-frequency-bar-charts",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Grouped Frequency Bar Charts",
    "text": "Grouped Frequency Bar Charts\n\n\nNote beside = FALSE is the default.\nIf we do not specify a beside option, a stacked bar chart is created instead.\nIn the second code cell, we also add a legend to the plot.\n\n\n########################################################\n# Note this has already been run in a previous section\n# You do not need to run again if already created\n#######################################################\n\n# create a contingency table for status and month\ncon.table &lt;- table(storms$category, storms$month) \ncon.table  # print output to screen\n\n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create stacked bar chart\nbarplot(con.table,  # use counts from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels\n       fill = my.colors2)  # use same colors"
  },
  {
    "objectID": "Overview-of-Plots.html#stacked-bar-charts-relative-to-grand-total",
    "href": "Overview-of-Plots.html#stacked-bar-charts-relative-to-grand-total",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Stacked Bar Charts Relative to Grand Total",
    "text": "Stacked Bar Charts Relative to Grand Total\n\n\nFirst we create a contingency table using table(x, y).\nThen we use prop.table([table_name]) to convert to frequencies to proportions out of the grand total.\nFinally we can create a group bar chart of relative frequencies.\n\n\n# create two-table of counts\ncon.table &lt;- table(storms$category, storms$month)\n\n# convert counts to proportions\ncon.prop &lt;- prop.table(con.table) \n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create stacked bar chart\nbarplot(con.table,  # use counts from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # color of bars\n        ylab = \"Relative Frequency (of grand total)\")  # vertical axis label\n\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels\n       fill = my.colors)  # use same fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#stacked-bar-chart-relative-to-column-totals",
    "href": "Overview-of-Plots.html#stacked-bar-chart-relative-to-column-totals",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Stacked Bar Chart Relative to Column Totals",
    "text": "Stacked Bar Chart Relative to Column Totals\n\nOften, we would like the proportions in the table to be computed out of the total in each column (instead of the grand total).\n\nWe add the option 2 inside prop.table().\nIn this example, each column is a different month.\n\n\n# create two-table of counts\ncon.table &lt;- table(storms$category, storms$month)\n\n# convert counts to proportions\n# note the option 2 added to command below\ncon.prop.column &lt;- prop.table(con.table, 2)  \n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create stacked bar chart\nbarplot(con.prop.column,  # use counts from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # color of bars\n        ylab = \"Relative Frequency (to month totals\")  # vertical axis label\n\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels\n       fill = my.colors)  # use same fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#loading-ggplot2",
    "href": "Overview-of-Plots.html#loading-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Loading ggplot2",
    "text": "Loading ggplot2\n\n\nThe ggplot2 package is already installed as a default package in Google Colaboratory.\nHowever, recall each time we start or restart a new session and want to access the library of functions and data in the package, we need to load the library of files in the package with the library command.\nRun the first code cell below to load the ggplot2 package.\nIf restarting a new session, you also need to reload the dplyr package to access storms data.\n\n\nlibrary(ggplot2)  # make sure you have installed ggplot2 package\n\n\n# may need to reload\nlibrary(dplyr)"
  },
  {
    "objectID": "Overview-of-Plots.html#plotting-one-numerical-variable-with-ggplot2",
    "href": "Overview-of-Plots.html#plotting-one-numerical-variable-with-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Plotting One Numerical Variable with ggplot2",
    "text": "Plotting One Numerical Variable with ggplot2\n\nTo create various types of plots for one quantitative variable, such as wind:\n\nThe ggplot object is the data frame storms.\nThe aesthetic is the variable wind that we will plot on the x-axis.\nGeometric objects histogram, density, and boxplot are specified in each of the three code cells below.\nThere a numerous options we can include as well.\n\n\n# create a histogram\nggplot(storms, aes(x = wind)) + \n  geom_histogram(fill = \"steelblue\", color=\"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# create a density plot\nggplot(storms, aes(x = wind)) + \n  geom_density(color=\"red\") + \n  theme_bw() # adding theme_bw()  makes white background\n\n\n\n\n\n# create a boxplot\nggplot(storms, aes(x = wind)) + \n  geom_boxplot(color=\"black\", fill=\"blueviolet\")"
  },
  {
    "objectID": "Overview-of-Plots.html#scatter-plots-with-ggplot2",
    "href": "Overview-of-Plots.html#scatter-plots-with-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Scatter Plots with ggplot2",
    "text": "Scatter Plots with ggplot2\n\nTo create a scatter plot to compare two quantitative variables such as wind speed and pressure of storms:\n\nThe ggplot object is the data frame storms.\nThe aesthetic are the variables\n\npressure is the predictor plotted on the x-axis.\nwind is the response plotted on the y-axis.t\n\nGeometric object is scatter.\n\n\n# create a scatter plot\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind))"
  },
  {
    "objectID": "Overview-of-Plots.html#scaling-ggplot2-plots",
    "href": "Overview-of-Plots.html#scaling-ggplot2-plots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Scaling ggplot2 plots",
    "text": "Scaling ggplot2 plots\n\nIn general, scaling is the process by which ggplot2 maps variables to unique values. When this is done for discrete numeric or qualitative variables, ggplot2 will often scale the variable to distinct colors, symbols, or sizes, depending on the aesthetic mapped.\nIn the example below, we map the status variable to the color aesthetic, which is then scaled to different colors for the different status levels.\n\n# scatter plot with scaling\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind, color = status))\n\n\n\n\n\nScaling by Shape\n\nAlternatively, we can map the status variable to the shape aesthetic, which creates a plot with different shapes for each observation based on the status level.\n\nBy default, 6 shapes can be used.\nThere are 9 different status of storms.\nThe last option manually sets the shapes for each status to avoid an error.\n\n\n# scaling by shape\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind, shape = status)) + \n  scale_shape_manual(values=0:8)  # manually setting shapes\n\n\n\n\n\n\nApplying Multiple Scales\n\nWe can even combine these two aesthetic mappings in a single plot to get different colors and symbols for each level of month and status, respectively.\n\nBy default, 6 shapes can be used.\nThere are 9 different status of storms.\nThe last option manually sets the shapes for each status to avoid an error.\n\n\n# scaling by month and status\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind, color = month, shape = status)) + \n  scale_shape_manual(values=0:8)  # manually setting shapes for status"
  },
  {
    "objectID": "Overview-of-Plots.html#facetting-in-ggplot2",
    "href": "Overview-of-Plots.html#facetting-in-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Facetting in ggplot2",
    "text": "Facetting in ggplot2\n\nFaceting creates separate panels (facets) of a data frame based on one or more faceting variables.\nTo create various scatter plots (one for each category) to compare two quantitative variables such as wind speed and pressure of storms, we can add a facet_grid.\n\nNote the NA plot corresponds to the storms that are not hurricanes.\n\n\n# faceting by category\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind)) + \n  facet_grid(~ category)"
  },
  {
    "objectID": "Overview-of-Plots.html#bar-charts-with-ggplot2",
    "href": "Overview-of-Plots.html#bar-charts-with-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Bar Charts with ggplot2",
    "text": "Bar Charts with ggplot2\n\nImagine we would like to compare the number of different types of storms (status) that occurred in each month.\n\nStacked Bar Charts of Counts with ggplot2\n\nTo create a stacked bar chart of counts for one or more qualitative variable:\n\nThe ggplot object is the data frame storms.\nGeometric object is geom_bar.\nThe aesthetic is specified as:\n\nFill color, (fill) is status.\nThe height of each bar is summarizing the statistic (stat) is \"count\".\nThe position=\"stack\" creates a stacked bar chart of counts.\n\n\n\n# stacks bars on top of each other \nggplot(storms, aes(x=month)) + \n    geom_bar(aes(fill=status), stat = \"count\", position=\"stack\") + \n    ggtitle(\"Occurrence of Storms by Month\")\n\n\n\n\n\n\nStacked Relative Frequency Bar Charts with ggplot2\n\nTo create a stacked bar chart of relative frequencies for two qualitative variables:\n\nThe ggplot object is the data frame storms.\nGeometric object is geom_bar.\nThe aesthetic is specified as:\n\nFill color, (fill) is status.\nThe height of each bar is summarizing the statistic (stat) is \"count\".\nThe position=\"fill\" creates a stacked bar chart of relative frequencies.\n\n\n\n# stacks bars and standardizing each stack\nggplot(storms, aes(x=month)) + \n    geom_bar(aes(fill=status), stat = \"count\", position=\"fill\") +  \n    ggtitle(\"Occurrence of Storms by Month\")\n\n\n\n\n\n\nGrouped Bar Charts of Counts with ggplot2\n\nTo create various types of bar plots for one or more qualitative variables:\n\nThe ggplot object is the data frame storms.\nGeometric object is geom_bar.\nThe aesthetic is specified as:\n\nFill color, (fill) is status.\nThe height of each bar is summarizing the statistic (stat) is \"count\".\nThe position=\"dodge\" creates a stacked bar chart.\n\n\n\n# creates grouped bar chart\nggplot(storms, aes(x=month)) + \n    geom_bar(aes(fill=status), stat = \"count\", position=\"dodge\") +  \n    ggtitle(\"Occurrence of Storms by Month\")"
  },
  {
    "objectID": "Overview-of-Plots.html#load-library",
    "href": "Overview-of-Plots.html#load-library",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Load Library",
    "text": "Load Library\n\n\nlibrary(mapview)  # load spatial mapping package"
  },
  {
    "objectID": "Overview-of-Plots.html#mapping-all-storms-by-status",
    "href": "Overview-of-Plots.html#mapping-all-storms-by-status",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Mapping All Storms by Status",
    "text": "Mapping All Storms by Status\n\n\nmapview(storms, xcol = \"long\", ycol = \"lat\", \n        zcol = \"status\", \n        crs = 4269, grid = FALSE)"
  },
  {
    "objectID": "Overview-of-Plots.html#mapping-category-5-hurricanes",
    "href": "Overview-of-Plots.html#mapping-category-5-hurricanes",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Mapping Category 5 Hurricanes",
    "text": "Mapping Category 5 Hurricanes\n\nFirst we filter out observations with category equal to 5.\n\ncat5 &lt;- subset(storms , category == \"5\")  # keep only category 5\n\n\nmapview(cat5, xcol = \"long\", ycol = \"lat\", cex = \"wind\", crs = 4269, grid = FALSE)\n\n\nmapview(cat5, xcol = \"long\", ycol = \"lat\", zcol = \"name\", cex = \"wind\", crs = 4269, grid = FALSE)"
  },
  {
    "objectID": "Overview-of-Plots.html#sec-cc",
    "href": "Overview-of-Plots.html#sec-cc",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  }
]