[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "",
    "text": "Preface\nStatistical Methods: Exploring the Uncertain is a set of interactive course materials designed to explore statistical methods. These materials are Open Education Resources (OER) designed to serve as both the textbook and in-class labs for MATH 3382: Statistical Theory at University of Colorado Denver. MATH 3382 is an undergraduate course in statistics at the 300 (junior) level that is required for all math majors at University of Colorado Denver. These materials are not intended for an introduction to statistics course for a more general audience of students from various backgrounds that are typically at the 100 or 200 levels. Students of Mathematics, Statistics, Data Science, Economics, Biostatistics, Computer Science and other STEM fields that want to advance beyond a typical introduction to statistics course are the intended audience.\nThe materials are designed for full semester (3 credit) course. Topics covered include exploratory data analysis, statistical inference, probability, sampling distributions, maximum likelihood estimators, method of moments estimators, properties of estimators, confidence intervals (both bootstrap and parametric methods), and hypothesis tests (both permutation tests and parametric methods)\nThe prerequisite for the course at CU Denver is multivariable calculus; however, students with a background in single variable calculus are able to work through the materials as well. The only instance where multivariable calculus might arise is with joint probability distributions and maximum likelihood estimators for distributions with two or more parameters, both of these applications can be skipped.\nStudents are not required to have any previous course work in statistics, probability, or coding in R (or any other language). The “essentials” of probability are covered in Chapter 2 with a focus on how we apply theory from probability to do statistics. These materials do not include a comprehensive treatment of probability."
  },
  {
    "objectID": "index.html#a-virtual-lab-for-exploring-statistics",
    "href": "index.html#a-virtual-lab-for-exploring-statistics",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "A Virtual Lab for Exploring Statistics",
    "text": "A Virtual Lab for Exploring Statistics\n\nThese materials are intended as set of activities to experiment and explore statistical theory and methods. Each interactive Jupyter notebooks is a “virtual laboratory” where we perform our experiments and summarize the results. The objectives of experimental mathematics/statistics are generally to make the subject more tangible, lively and fun.\nThe intent of introducing R is not to avoid a deep and rigorous understanding of statistics or to use R simply as a calculator. There are other sources that skim the surface of statistical concepts and instead focus on the coding side of things. These materials use R as an additional tool for further exploring statistics to gain a deeper insight into statistical models. Some of the objectives of implementing R code cells into the materials are to:\n\nEasily import and analyze real data from interesting studies and experiments.\nCreate insightful data visualizations.\nExplore features of the data to generate statistical questions worth investigating.\nDiscover patterns and relationships between variables.\nIntroduce and implement resampling methods.\nDevelop and test conjectures.\nConfirm analytically derived results.\nGain further insight and intuition.\nBridge the divide between theory and practice."
  },
  {
    "objectID": "index.html#how-to-access-edit-and-save-notebooks",
    "href": "index.html#how-to-access-edit-and-save-notebooks",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "How to Access, Edit and Save Notebooks",
    "text": "How to Access, Edit and Save Notebooks\n\nThis html version of the materials is not dynamic. You cannot edit the text or run code with the html version.\n \n\nAt the top of each notebook is a “button” such as the one above.\nClick the button to open an interactive Jupyter notebook version initialized to run R code in Google Colaboratory (or Colab).\nYou can begin working with the notebook right away in Colab! There is no software to install (or purchase!).\nYou can also access the materials directly on GitHub at https://github.com/CU-Denver-MathStats-OER/Statistical-Theory.git.\n\nEach Jupyter notebook contains both narrative text (in Markdown cells) and R code cells that you can create, modify, and run.\n\n\n\n\n\n\nCaution\n\n\n\nAlthough you do not need a Google account to interact with the notebooks, the Colab notebooks are “shared”, meaning you cannot save any changes to the initial shared document that opens. If you would like to save your changes, you first need to save a copy to your Google Drive. Then you can edit and save changes to your own version.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn addition to Jupyter notebooks versions that run in Colab, the original Quarto markdown documents (with file extension .qmd) are also included in the repo if you prefer to run and edit the labs using Posit, RStudio or other available applications. Both the Jupyter (.ipynb) and Quarto markdown (.qmd) versions are identical and both use R."
  },
  {
    "objectID": "index.html#what-programming-background-is-required",
    "href": "index.html#what-programming-background-is-required",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "What Programming Background is Required?",
    "text": "What Programming Background is Required?\n\nNo prior experience or knowledge of R, Markdown, LaTeX, or Colaboratory is assumed or required to begin working with these materials. After working with these materials, you will have some knowledge and experience with R, Markdown, LaTeX, and Colaboratory!\n\nWelcome to Colaboratory is a helpful notebook (with videos) to help introduce you to Colab.\nHere’s a helpful Markdown guide.\n\n\nWhat is R?\n\nR is a programming language used largely for statistical computing, data wrangling and visualization. We will be using R as a tool for exploring statistical theory. The first stable version of R was released in 2000, and after all of this time, there is a large community of R users that have already created tons of useful packages and shared interesting data sets that are frequently updated. We will create, modify, and run R code in Jupyter notebooks. No prior programming experience is required to begin working in R in these materials.\nThe goal of these materials are to investigate statistical theory, not learn how to be an expert R programmer. The hope is that we can use R as a tool for experimenting, gaining a deeper insight, and implementing resampling methods. In the process, hopefully you gain a familiarity with R and coding so it is no longer a barrier to future work! Have I mentioned no prior programming experience is required to begin working in R in these materials?\n\n\nWhat is LaTex?\n\nLaTeX is a system for rendering nice looking mathematical symbols, expressions, and equations. All of the mathematical notation in these materials are created using LaTeX. You can view and edit all of the LaTeX code in the Markdown cells. You do not need to become an expert in LaTeX, but having a familiarity with LaTeX is quite helpful and LaTeX can be used to typeset math in a number of different applications.\n\nHere is a useful dictionary of LaTeX math symbols to get a glimpse of LaTeX."
  },
  {
    "objectID": "index.html#how-to-contact-me",
    "href": "index.html#how-to-contact-me",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "How to Contact Me",
    "text": "How to Contact Me\n\nIf you have any questions, comments, or suggestions about these materials, please feel free to reach out to me (Adam) at adam.spiegler@ucdenver.edu.\n\nConsidering using these materials in your course? Please let me know if I can help.\nIf you do use some of these materials in your course, your feedback is welcome and appreciated.\nIf you materials that you would like to share or contribute to this project, great!"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nThis project was supported by the Colorado Department of Higher Education (CDHE) OER Grant Program. A big thank you to Megan Patnott for reviewing the materials and providing many great corrections and suggestions to improve the materials!"
  },
  {
    "objectID": "index.html#creative-commons-license-information",
    "href": "index.html#creative-commons-license-information",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler (University of Colorado Denver) is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This work is funded by an Institutional OER Grant from the Colorado Department of Higher Education (CDHE).\nFor similar interactive OER materials in other courses funded by this project in the Department of Mathematical and Statistical Sciences at the University of Colorado Denver, visit https://github.com/CU-Denver-MathStats-OER."
  },
  {
    "objectID": "index.html#quarto-books",
    "href": "index.html#quarto-books",
    "title": "Statistical Methods: Exploring the Uncertain",
    "section": "Quarto Books",
    "text": "Quarto Books\n\nThese materials are creating using Quarto books. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "13-Estimation-MLE.html",
    "href": "13-Estimation-MLE.html",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "",
    "text": "Case Study: Slot Machine Jackpots\nA strategic gambler believes they have identified a faulty slot machine which pays out significantly more money than the other slot machines. She and her friends watch the machine 24 hours a day for 7 days and observed the slot machine paid out the $\\(1,\\!000,\\!000\\) jackpot prize 10 times during the week. How can she figure out whether the machine is faulty or whether the number of jackpot prizes are within reason?\nWe motivate maximum likelihood estimation with the following question:\nThe likelihood function \\[\\color{dodgerblue}{L(\\theta)= L( \\theta \\mid x_1, x_2, \\ldots x_n)}\\] gives the likelihood of the parameter \\(\\theta\\) given the observed sample data. A maximum likelihood estimate (MLE), denoted \\(\\color{dodgerblue}{\\mathbf{\\hat{\\theta}_{\\rm MLE}}}\\), is the value of \\(\\theta\\) that gives the maximum value of the likelihood function \\(L(\\theta)\\).\nLet \\(f(x; \\theta)\\) denote the pdf of a random variable \\(X\\) with associated parameter \\(\\theta\\). Suppose \\(X_1, X_2, \\ldots , X_n\\) are random samples from this distribution, and \\(x_1, x_2, \\ldots , x_n\\) are the corresponding observed values.\n\\[\\color{dodgerblue}{\\boxed{L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta).}}\\]\nIn Question 2 we derived an expression for the likelihood function \\(L(\\lambda)\\) given the random sample of \\(n=4\\) values we picked from \\(X \\sim \\mbox{Pois}( \\lambda)\\) and stored in the vector x. Recall Poisson distributions have pmf\n\\[f(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\qquad \\mbox{for } x = 0, 1, 2, \\ldots .\\]\nIf we pick a sample of \\(n=4\\) values we denote \\(X_1 = x_1\\), \\(X_2 = x_2\\), \\(X_3 = x_3\\), and \\(X_4 = x_4\\), then the likelihood function is\n\\[L(\\lambda) = L(\\theta \\mid x_1, x_2, \\ldots , x_n) = \\left( \\frac{\\lambda^{x_1} e^{-\\lambda}}{x_1!} \\right) \\left( \\frac{\\lambda^{x_2} e^{-\\lambda}}{x_2!} \\right) \\left( \\frac{\\lambda^{x_3} e^{-\\lambda}}{x_3!} \\right) \\left( \\frac{\\lambda^{x_4} e^{-\\lambda}}{x_4!} \\right).\\]\nWe will use the random sample generated by the code cell below. Note x is a vector consisting of values x[1] \\(=9\\), x[2] \\(=8\\), x[3] \\(=6\\), and x[4] \\(=5\\).\nset.seed(612)\n\nx &lt;- rpois(4, true.mean)\nx\n\n[1] 9 8 6 5\nThe random sample \\((9,8,6,5)\\) picked from \\(X \\sim \\mbox{Pois}(\\lambda)\\) gave \\(\\hat{\\lambda}_{\\rm{MLE}} = 7\\). If we pick another random sample \\(n=4\\) from the population \\(X \\sim \\mbox{Pois}(\\lambda)\\), how much will our estimate for \\(\\hat{\\lambda}_{\\rm{MLE}}\\) change? Some desirable properties for the distribution of \\(\\hat{\\lambda}_{\\rm{MLE}}\\) values from different random samples would be:\nSteps for finding MLE, \\(\\hat{\\theta}_{\\rm MLE}\\):\n\\[L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\\]\nLogarithmic functions such as \\(y = \\ln{x}\\) are increasing functions. The larger the input \\(x\\), the larger the output \\(y = \\ln{x}\\) becomes. Thus, the value of \\(\\theta\\) that gives the maximum value of \\(L(\\theta)\\) will also correspond to the value of \\(\\theta\\) that gives the maximum value of the function \\(y = \\ln{(L(\\theta))}\\), and vice versa:\nWe call the the natural log of the likelihood function, \\(\\color{dodgerblue}{y=\\ln{(L(\\theta}))}\\), the log-likelihood function.\n(a) Maximum of Likelihood\n\n\n\n\n\n\n\n(b) Maximum of Log-Likelihood\n\n\n\n\nFigure 10.1: Comparing Maxima\nSteps for finding MLE, \\(\\hat{\\theta}_{\\rm MLE}\\), using a log-likelihood function:\n\\[L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\\] 2. Apply the natural log to \\(L(\\theta)\\) to derive the log-likelihood function \\(y = \\ln{(L(\\theta))}\\). Simplify using properties of the natural log before moving to the next step.\nIn Question 11, we found a general formula for \\(\\hat{\\lambda}_{\\rm{MLE}}\\), the MLE of exponential distributions in general. We cannot use R to numerically check our analytic results since our result is a formula that depends on the values of the \\(x_i\\)’s. We can test our formula on many different random samples and check to make sure our formula gives consistent answers with numeric solutions in R. Using calculus to derive the general formula for \\(\\hat{\\lambda}_{\\rm{MLE}}\\) in Question 11 is incredibly convenient since now we have a “shortcut” formula that we can use for finding MLE estimates for any random sample from an exponential distributions.\nSo far we have observed:\nFor normal distributions \\(X \\sim N(\\mu, \\sigma)\\), the maximum likelihood estimates of \\(\\mu\\) and \\(\\theta\\) are \\[\\hat{\\mu}_{\\rm{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x} \\quad \\mbox{ and } \\quad \\hat{\\sigma}_{\\rm{MLE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2}.\\]\nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "13-Estimation-MLE.html#question-1",
    "href": "13-Estimation-MLE.html#question-1",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 1",
    "text": "Question 1\n\nThey decide to compare the performance of the suspect slot machine to other slot machines. They pick a random sample of \\(n=4\\) other slot machines and record how many jackpot prizes each machine pays over a one week time frame. Let random variable \\(X\\) denote the number of jackpots a randomly selected slot machine pays out in week. What distribution do you think best models \\(X\\)? Give a corresponding formula for the probability mass function of \\(X\\).\n\nHint: Should we use a discrete or continuous random variable?\nHint: See either appendix of common discrete random variables or appendix of common continuous random variables for additional help.\n\n\nSolution to Question 1"
  },
  {
    "objectID": "13-Estimation-MLE.html#collecting-data",
    "href": "13-Estimation-MLE.html#collecting-data",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Collecting Data",
    "text": "Collecting Data\n\nWe will simulate collecting a data sample.\n\nRun the code cell below to “secretly” generate a population mean that we store in true.mean.\nThe command set.seed(827) will seed the randomization so we all have the same population mean.\nDo not print the output to screen. Keep true.mean secret for now!\n\n\n# set randomization for seeding population mean\nset.seed(827)  \n\n# pick a population mean that will be fixed but unknown to us\ntrue.mean &lt;- sample(3:8, size=1)\n\nNext, we generate a sample size \\(n=4\\).\n\nRun the code cell below to generate your random sample.\nEach observation \\(x_i\\) in the vector x corresponds to the number of jackpots a randomly selected slot machine paid out in one week.\nThe command set.seed(612) will seed the randomization so my sample x remains fixed.\n\nYou can delete the command set.seed(612) to generate a different random sample picked from the same population.\nThen you can compare the estimate obtained from your sample with the estimate based on the sample generated below.\n\nInspect the values in your sample after running.\n\n\nset.seed(612)\n\nx &lt;- rpois(4, true.mean)\nx\n\n[1] 9 8 6 5\n\n\n\nThe sample generated by set.seed(612) is\n\n\\[x_1=9 \\ ,\\  x_2=8\\ ,\\  x_3=6 \\ , \\ x_4=5.\\]"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-2",
    "href": "13-Estimation-MLE.html#question-2",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the probability mass function from Question 1 and your sample generated by the code cell above stored in x, what is the probability of picking the random sample \\(x_1\\), \\(x_2\\), \\(x_3\\), \\(x_4\\) stored in x? Your answer will be a formula that depends on the parameter \\(\\lambda\\).\n\nSolution to Question 2"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-3",
    "href": "13-Estimation-MLE.html#question-3",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 3",
    "text": "Question 3\n\nFind the value of \\(\\lambda\\) that maximizes the likelihood function from Question 2.\n\n\n\n\n\n\nTip\n\n\n\nRecall from calculus that global maxima occur at end points or critical points where \\(\\frac{d L}{d \\lambda} = 0\\) or is undefined.\n\n\n\nSolution to Question 3\n\n  \n\n\nPlotting the Likelihood Function for Question 3\n\nGiven the random sample \\(x_1=9, x_2=8, x_3=6, x_4=5\\), the resulting likelihood derived in Question 2 is\n\\[L({\\color{tomato}\\lambda}) = \\frac{{\\color{tomato}\\lambda}^{28} e^{-4{\\color{tomato}\\lambda}}}{(9!)(8!)(6!)(5!)}.\\]\nIn Question 3, we find the value of \\(\\lambda\\) that maximizes the likelihood function \\(L(\\lambda)\\) using optimization methods from calculus. It is always a good idea to check our work. If we have access to technology, we can plot the likelihood function and identify the approximate value of \\(\\lambda\\) that gives the maximum value of \\(L(\\lambda)\\).\n\nlam &lt;- seq(3, 11, 0.1)  # values of lambda on x-axis\nlike.est &lt;- lam^(sum(x)) * exp(-4*lam)/prod(factorial(x))  # values of L(lambda)\n\nplot(lam, like.est,  # plot lam and likelihood on x and y axes\n     type = \"l\",  # connect plotted points with a curve\n     ylab = \"L(lambda)\",  # y-axis label\n     xlab = \"lambda\",  # x-axis label\n     main = \"Plot of Likelihood Function\")  # main label\n\npoints(x = 7, y = 0.0002515952, cex = 2, pch = 20, col = \"tomato\")  # point at max\n\naxis(1, at=c(7), col.axis = \"tomato\")  # marking MLE estimate\nabline(v = 7, col = \"tomato\", lwd = 2, lty = 2)  # marking MLE estimate"
  },
  {
    "objectID": "13-Estimation-MLE.html#revealing-the-actual-value-of-lambda",
    "href": "13-Estimation-MLE.html#revealing-the-actual-value-of-lambda",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Revealing the Actual Value of \\(\\lambda\\)",
    "text": "Revealing the Actual Value of \\(\\lambda\\)\n\nWe picked a value for \\(\\lambda\\) and stored it in true.mean. We have not revealed what the actual value of \\(\\lambda\\) is. Run the code cell below to see that actual value of \\(\\lambda\\), and compare your answer for \\(\\hat{\\lambda}_{\\rm{MLE}}\\) in Question 3 with the actual value of \\(\\lambda\\).\n\ntrue.mean\n\n[1] 8"
  },
  {
    "objectID": "13-Estimation-MLE.html#defining-the-likelihood-function-as-product",
    "href": "13-Estimation-MLE.html#defining-the-likelihood-function-as-product",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Defining the Likelihood Function as Product",
    "text": "Defining the Likelihood Function as Product\n\nIn the code cell below, we input the likelihood function.\n\nTo define a symbolic function, we use the command function(lam) [expr].\n\nWe use lam to denote our variable, \\(\\lambda\\).\nWe enter an appropriate formula in place of [expr].\n\n[expr] is the product of the \\(4\\) pmf’s of the Poisson distribution.\nWe name the newly created function like.\nTo evaluate the function like at \\(\\lambda = 7\\), we use the command like(7).\n\n\nlike &lt;- function(lam) lam^x[1] * exp(-lam)/factorial(x[1]) *\n                      lam^x[2] * exp(-lam)/factorial(x[2]) *\n                      lam^x[3] * exp(-lam)/factorial(x[3]) *\n                      lam^x[4] * exp(-lam)/factorial(x[4])\n\nlike(7)\n\n[1] 0.0002515952"
  },
  {
    "objectID": "13-Estimation-MLE.html#improving-the-code-for-a-likelihood-function",
    "href": "13-Estimation-MLE.html#improving-the-code-for-a-likelihood-function",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Improving the Code for a Likelihood Function",
    "text": "Improving the Code for a Likelihood Function\n\nIf we have a sample size \\(n=100\\) instead of \\(n=4\\), we would not want to code the likelihood as we did in the previous code cell. We can streamline the process using a for loop that utilizes the structure of likelihood functions:\n\nEach term in the product uses the same formula for the pmf.\nThe likelihood function is a product of all the pmf’s.\n\nIn the slot machine example, we have \\(X \\sim \\mbox{Pois}(\\lambda)\\) and a sample \\(x_1=9\\), \\(x_2=8\\) , \\(x_3=6\\), and \\(x_4=5\\). The vectors x and pmf are therefore\n\\[ x = (9, 8, 6, 5) \\quad \\mbox{and} \\quad \\mbox{pmf} = \\left( \\frac{\\lambda^{9} e^{-\\lambda}}{9!} , \\frac{\\lambda^{8} e^{-\\lambda}}{8!} , \\frac{\\lambda^{6} e^{-\\lambda}}{6!} , \\frac{\\lambda^{5} e^{-\\lambda}}{5!} \\right).\\]\nThe likelihood function like is the product of the entries in the vector pmf. We can substitute different values for the parameter \\(\\lambda\\) into the function like and compute different values of the likelihood function.\n\nRun the code cell below to compute the likelihood that \\(\\lambda = 7\\) given the sample x.\n\n\nlike &lt;- function(lam){\n  pmf &lt;- lam^x * exp(-lam)/factorial(x)\n  prod(pmf)\n}\n\nlike(7)\n\n[1] 0.0002515952\n\n\n\nUsing Built-In Distribution Functions\n\nFor many common distributions, R has built in functions to compute the values of pmf’s for many discrete random variables and pdf’s for continuous random variables. For Poisson distributions, the function dpois(x, lam) calculates the value of \\(f(x; \\lambda) = \\frac{\\lambda^{x} e^{-\\lambda}}{x!}\\).\n\nTherefore, we can use dpois(x, lam) in place of the expression lam^x * exp(-lam)/factorial(x).\nThe code cell below makes use of the dpois(x, lam) function and saves us the trouble of typing the formula out ourselves!\nRun the code to evaluate the function at \\(\\lambda = 7\\) to make sure the result is consistent with our previous functions.\n\n\nlike &lt;- function(lam){\n  pmf &lt;- dpois(x, lam)\n  prod(pmf)\n}\n\nlike(7)\n\n[1] 0.0002515952"
  },
  {
    "objectID": "13-Estimation-MLE.html#sec-opt-r",
    "href": "13-Estimation-MLE.html#sec-opt-r",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Optimizing the Likelihood Function in R",
    "text": "Optimizing the Likelihood Function in R\n\nIn Question 3 we used methods from calculus to find the value of \\(\\theta\\) that maximizes the likelihood function \\(L(\\theta)\\). We can check those results using the command optimize(function, interval, maximum = TRUE).\n\nfunction is the name of the function where we stored the likelihood function.\ninterval is the interval of parameter values over which we maximize the likelihood function.\n\nUsing c(0,100) means we will find the maximum of \\(L(\\theta)\\) over \\(0 &lt; \\lambda &lt; 100\\).\nBased on the values in our sample, we can narrow the interval to save a little computing time.\n\nmaximum = TRUE option means optimize() will identify the maximum of the function.\n\nNote the default for optimize() is to find the minimum value.\n\nRun the command below to calculate \\(\\hat{\\lambda}_{\\rm{MLE}}\\) for the slot machine example.\n\n\noptimize(like, c(0,100), maximum = TRUE)\n\n$maximum\n[1] 7.000001\n\n$objective\n[1] 0.0002515952"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-4",
    "href": "13-Estimation-MLE.html#question-4",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 4",
    "text": "Question 4\n\nIf population \\(X \\sim \\mbox{Pois}(\\lambda)\\) is the number of jackpot payouts a randomly selected slot machine has in one week:\n\nWhat is the practical interpretation of the value of \\(\\lambda\\)?\nIf we pick a random sample of 4 slot machines and find \\(x_1=9\\), \\(x_2=8\\) , \\(x_3=6\\), and \\(x_4=5\\), explain why an estimate \\(\\hat{\\lambda}_{\\rm{MLE}} = 7\\) makes practical sense.\n\n\nSolution to Question 4"
  },
  {
    "objectID": "13-Estimation-MLE.html#picking-another-random-sample",
    "href": "13-Estimation-MLE.html#picking-another-random-sample",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Picking Another Random Sample",
    "text": "Picking Another Random Sample\n\nThe random sample \\((9,8,6,5)\\) picked from \\(X \\sim \\mbox{Pois}(\\lambda)\\) gave \\(\\hat{\\lambda}_{\\rm{MLE}} = 7\\). The actual value of \\(\\lambda\\) we revealed the true.mean we used was \\(\\lambda = 8\\). Below we simulate picking another random sample of \\(n=4\\) values from the same population, \\(X \\sim \\mbox{Pois}(8)\\). Then we will compute \\(\\hat{\\lambda}_{\\rm{MLE}}\\) for this sample and see if we can start to pick up on a pattern.\n\nRun the code cell below to generate a new random sample stored in new.x.\n\n\nset.seed(012)  # fixes randomization\n\nnew.x &lt;- rpois(4, 8)  # pick another random sample n=4 from Pois(8)\nnew.x  # print results\n\n[1]  4 11 13  6\n\n\nThe new sample is \\(x_1=4\\), \\(x_2=11\\) , \\(x_3=13\\), and \\(x_4=6\\).\n\nRun the code cell to compute the value of \\(\\hat{\\lambda}_{\\rm{MLE}}\\) for this new sample.\n\n\n# be sure to first run code cell above to create new.x\nnew.like &lt;- function(lam){\n  new.pmf &lt;- dpois(new.x, lam)\n  prod(new.pmf)\n}\n\noptimize(new.like, c(0,100), maximum = TRUE)\n\n$maximum\n[1] 8.500015\n\n$objective\n[1] 1.589466e-05\n\n\n\nComparing Estimates\n\nLet’s compare the two random samples and their corresponding values for the MLE estimate.\n\n\n\n\n\n\n\nSample           \nValue of MLE\n\n\n\n\n\\((9, 8, 6, 5)\\)\n\\(7\\)\n\n\n\\((4, 11, 13, 6)\\)\n\\(8.5\\)\n\n\n\n\nNeither gives the correct value for \\(\\lambda\\) which is actually 8.\n\nOne estimate is too small and the other is too large.\n\nWe hope if we average all such MLE estimates together, we get the actual value \\(\\lambda = 8\\).\nWe have some sense of the variation, but generating many (10,000) random samples and looking at the distribution of many more MLE estimates will tell us more information about the variability."
  },
  {
    "objectID": "13-Estimation-MLE.html#analyzing-a-distribution-of-mles",
    "href": "13-Estimation-MLE.html#analyzing-a-distribution-of-mles",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Analyzing a Distribution of MLE’s",
    "text": "Analyzing a Distribution of MLE’s\n\nThe for loop in the code cell below generates a distribution of MLE’s for \\(\\lambda\\) based on 10,000 random samples size \\(n=4\\) picked from \\(X \\sim \\mbox{Pois}(8)\\). Inside the for loop we:\n\nPick a random sample size \\(n=4\\) stored in temp.x.\nCalculate the MLE based on temp.x that we store in the vector pois.mle.\n\nThen we plot a histogram to display pois.mle, the distribution of MLE’s from the 10,000 random samples each size \\(n=4\\)\n\nRun the code cell below to generate and plot a distribution of MLE’s.\n\n\npois.mle &lt;- numeric(10000)\n\nfor (i in 1:10000)\n{\n  temp.x &lt;- rpois(4, 8)  # given random sample\n  like.pois &lt;- function(lam){  # define likelihood function\n    pmf.pois &lt;- dpois(temp.x, lam)  \n    prod(pmf.pois)  \n}\n  pois.mle[i] &lt;- optimize(like.pois, c(0,100), maximum = TRUE)$maximum  # find max of likelihood function\n}\n\nhist(pois.mle, \n     breaks = 20,\n     xlab = \"MLE\",\n     main = \"Dist. of MLE's for Poisson Dist\")\nabline(v = 8, col = \"blue\", lwd = 2)  # plot at actual value of lambda"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-5",
    "href": "13-Estimation-MLE.html#question-5",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 5",
    "text": "Question 5\n\nCalculate the mean and variance of the distribution of MLE’s stored in mle.pois and interpret the results. How would you describe the shape of the distribution? What would you expect to happen to the distribution as \\(n\\) gets larger?\n\nSolution to Question 5\n\n\n# use code cell to answer questions above"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-6",
    "href": "13-Estimation-MLE.html#question-6",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 6",
    "text": "Question 6\n\nA sample \\((x_1, x_2, x_3, x_4) = (1,3,3,2)\\) is randomly selected from \\(X \\sim \\mbox{Binom}(3,p)\\). Give a formula the likelihood function.\n\nSolution to Question 6"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-7",
    "href": "13-Estimation-MLE.html#question-7",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 7",
    "text": "Question 7\n\nGive a formula the likelihood function given the sample \\(x_1, x_2, x_3, \\ldots, x_n\\) is randomly selected from \\(X \\sim \\mbox{Exp}(\\lambda)\\).\n\nSolution to Question 7"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-8",
    "href": "13-Estimation-MLE.html#question-8",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 8",
    "text": "Question 8\n\nUsing your answer from Question 6, find the MLE for \\(p\\) when \\((x_1, x_2, x_3, x_4) = (1,3,3,2)\\) is randomly selected from \\(X \\sim \\mbox{Binom}(3,p)\\).\n\nSolution to Question 8"
  },
  {
    "objectID": "13-Estimation-MLE.html#plotting-the-likelihood-function-for-question-8",
    "href": "13-Estimation-MLE.html#plotting-the-likelihood-function-for-question-8",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Plotting the Likelihood Function for Question 8",
    "text": "Plotting the Likelihood Function for Question 8\n\nRunning the code cell below will generate a plot of the likelihood function from Question 8. We should verify the maximum of the graph coincides with our answer to Question 8. There is nothing to edit in the code cell below.\n\np &lt;- seq(0, 1, 0.01)  # values of p on x-axis\nlike.binom &lt;- 9 * p^9 * (1-p)^3  # values of L(p)\ncv &lt;- 9 * (0.75)^9 * (1-0.75)^3\n\nplot(p, like.binom,  # plot p and likelihood on x and y axes\n     type = \"l\",  # connect plotted points with a curve\n     ylab = \"L(p)\",  # y-axis label\n     xlab = \"p\",  # x-axis label\n     main = \"Plot of Likelihood Function\")  # main label\n\npoints(x = 0.75, y = cv, cex = 2, pch = 20, col = \"tomato\")  # point at max\n\naxis(1, at=c(0.75), label=\"theta = 0.75\", col.axis = \"tomato\", pos=0.0015, cex = 1.5)  # marking MLE estimate\nabline(v = 0.75, col = \"tomato\", lwd = 2, lty = 2)  # marking MLE estimate"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-9",
    "href": "13-Estimation-MLE.html#question-9",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 9",
    "text": "Question 9\n\nRecall the sample from Question 6. Complete the code cell below to build a formula for the likelihood function and find the value of \\(\\hat{p}_{\\rm{MLE}}\\). Run the completed code cell to check your answer in Question 8.\n\n\n\n\n\n\nTip\n\n\n\n\nSee earlier code for constructing the likelihood function and finding the maximum.\nWhen considering the interval option for optimize(), keep in mind we are estimating the value of a proportion, \\(p\\).\n\n\n\n\nSolution to Question 9\n\nReplace each of the four ?? in the code cell below with appropriate code. Then run the completed code to compute the MLE estimate \\(\\hat{p}_{\\rm{MLE}}\\) for the sample x picked from \\(X \\sim \\mbox{Binom}(3,p)\\).\n\nx &lt;- c(1, 3, 3, 2)  # given random sample\n\nlike.binom &lt;- function(p){\n  pmf.binom &lt;- ??  # replace ??\n  prod(??)  # replace ??\n}\n\noptimize(??, ??, maximum = TRUE)  # replace both ??"
  },
  {
    "objectID": "13-Estimation-MLE.html#why-maximize-ylnltheta-instead-of-ltheta",
    "href": "13-Estimation-MLE.html#why-maximize-ylnltheta-instead-of-ltheta",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Why Maximize \\(y=\\ln{(L(\\theta}))\\) Instead of \\(L(\\theta)\\)?",
    "text": "Why Maximize \\(y=\\ln{(L(\\theta}))\\) Instead of \\(L(\\theta)\\)?\n\nConsider the likelihood function from Question 7,\n\\[L({\\color{tomato}\\lambda}) = {\\color{\\tomato}\\lambda}^n e^{- {\\color{tomato}\\lambda} \\sum_i x_i}.\\] To find the critical values, we first need to find an expression for the derivative \\(\\frac{d L}{d \\lambda}\\).\n\nWe need to apply the product rule.\nWe need to apply the chain rule to compute the derivative of \\(e^{- {\\color{tomato}\\lambda} \\sum_i x_i}\\).\nAfter finding an expression for the derivative, we would then need to solve a complicated equation.\nWe can use key properties of the natural log to help make the differentiation easier!"
  },
  {
    "objectID": "13-Estimation-MLE.html#sec-log-prop",
    "href": "13-Estimation-MLE.html#sec-log-prop",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Useful Properties of the Natural Log",
    "text": "Useful Properties of the Natural Log\n\nThe four properties of natural logs listed below will be helpful to recall when working with log-likelihood functions.\n\n\\(\\ln{(A \\cdot B)} = \\ln{A} + \\ln{B}\\)\n\\(\\ln{\\left( \\frac{A}{B} \\right)} = \\ln{A} - \\ln{B}\\)\n\\(\\ln{(A^k)} = k \\ln{A}\\)\n\\(\\ln{e^k} = k\\)\n\nLikelihood functions are by definition a product of functions and often involve \\(e\\). Taking the natural log of the likelihood function converts a product to a sum. It is much easier to take the derivative of sums than products!"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-10",
    "href": "13-Estimation-MLE.html#question-10",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 10",
    "text": "Question 10\n\nGive a simplified expression for the log-likelihood function corresponding to the likelihood function from the exponential distribution in Question 7,\n\\[L({\\color{tomato}\\lambda}) = {\\color{\\tomato}\\lambda}^n e^{- {\\color{tomato}\\lambda} \\sum_i x_i}.\\]\n\nSolution to Question 10"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-11",
    "href": "13-Estimation-MLE.html#question-11",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 11",
    "text": "Question 11\n\nFind a general formula for the MLE of \\(\\lambda\\) when \\(x_1, x_2, x_3, \\ldots, x_n\\) comes from \\(X \\sim \\mbox{Exp}(\\lambda)\\). Your answer will depend on the \\(x_i\\)’s.\n\n\n\n\n\n\nTip\n\n\n\n\nMaximize the log-likelihood function from Question 10.\nBe sure you simplify the log-likelihood before taking the derivative.\nRecall \\(\\lambda\\) is the variable when differentiating, and treat each \\(x_i\\) as a constant."
  },
  {
    "objectID": "13-Estimation-MLE.html#solution-to-question-11",
    "href": "13-Estimation-MLE.html#solution-to-question-11",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Solution to Question 11",
    "text": "Solution to Question 11"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-12",
    "href": "13-Estimation-MLE.html#question-12",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 12",
    "text": "Question 12\n\nFind a general formula for the MLE of \\(\\lambda\\) when \\(x_1, x_2, x_3, \\ldots, x_n\\) comes from \\(X \\sim \\mbox{Pois}(\\lambda)\\). Your answer will depend on the \\(x_i\\)’s.\n\nSolution to Question 12"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-13",
    "href": "13-Estimation-MLE.html#question-13",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 13",
    "text": "Question 13\n\nSuppose a random variable with \\(X_1=5\\), \\(X_2=9\\), \\(X_3=9\\), and \\(X_4=10\\) is drawn from a distribution with pdf\n\\[f(x; \\theta) = \\frac{\\theta}{2\\sqrt{x}}e^{-\\theta \\sqrt{x}}, \\quad \\mbox{where x $&gt;0$}.\\]\nFind an MLE for \\(\\theta\\).\n\nSolution to Question 13"
  },
  {
    "objectID": "13-Estimation-MLE.html#question-14",
    "href": "13-Estimation-MLE.html#question-14",
    "title": "4.1: Maximum Likelihood Estimation",
    "section": "Question 14",
    "text": "Question 14\n\nConsider the random sample of \\(n=40\\) values picked from a geometric distribution \\(X \\sim \\mbox{Geom}(p)\\) that are stored in the vector x.geom. Note the proportion true.p is unknown for now.\n\nRun the code cell below to generate a random value for true.p (which is hidden) and create x.geom which is printed to the screen.\n\n\nset.seed(117)  # fixes randomization of true.p and x.geom\ntrue.p &lt;- sample(seq(0.1, 0.9, 0.1), size=1)  # true.p hidden for now\n\nx.geom &lt;- rgeom(40, true.p)  # generate a random sample n=40  \nx.geom\n\n [1]  8  0  5  2  1  1  4  2 12  3  0  1  7  2  0  0  3 14  2  3  1  0  2  5  2\n[26]  6  4  3  2  1  2  5  0  3  1  4  0  1  0  0\n\n\n\nQuestion 14a\n\nBased on the sample stored in x.geom in the previous code cell, find the MLE estimate for \\(\\hat{p}_{\\rm{MLE}}\\).\n\nComplete and run the partially completed R code cell below.\n\n\nSolution to Question 14a\n\nReplace each of the four ?? in the code cell below with appropriate code. Then run the completed code to compute the MLE estimate \\(\\hat{p}_{\\rm{MLE}}\\) for the sample (size \\(n=40)\\) x.geom randomly selected from \\(X \\sim \\mbox{Geom}(p)\\).\n\n# be sure you first run code cell above to define x.geom\nlike.geom &lt;- function(p){\n  pmf.geom &lt;- ??  # replace ??\n  prod(??)  # replace ??\n}\n\n\noptimize(??, ??, maximum = TRUE)  # replace both ??\n\n\n\n\nQuestion 14b\n\nComplete the partially completed code cell below to generate a plot of a distribution of MLE’s for \\(\\hat{p}_{\\rm{MLE}}\\) based on 10,000 randomly selected samples from \\(X \\sim \\mbox{Geom}(p)\\).\n\nSolution to Question 14b\n\nReplace each of the five ?? in the code cell below with appropriate code. Then run the completed code to create and plot a distribution of MLE’s for samples size \\(n=40\\) from \\(X \\sim \\mbox{Geom}(p)\\).\n\nmle.geom &lt;- numeric(10000)\n\nfor (i in 1:10000)\n{\n  x.temp &lt;- ??  # replace ??, pick random sample size n=40\n  geom.like &lt;- function(p){\n    geom.pmf &lt;- ??  # replace ??\n    prod(??)  # replace ??\n}\n  mle.geom[i] &lt;- optimize(??, ??, maximum = TRUE)$maximum  # replace both ??\n}\n\nhist(mle.geom, \n     breaks = 20,\n     xlab = \"MLE\",\n     main = \"Dist. of MLE's\")\nabline(v = true.p, col = \"dodgerblue\", lwd = 2)  # actual value of p\nabline(v = true.p, col = \"tomato\", lwd = 2)  # expected value of MLE\n\n\n\n\nQuestion 14c\n\nBased on the distribution of MLE’s in Question 14b, do you believe the estimator \\(\\hat{p}_{\\rm{MLE}}\\) is unbiased or biased? Explain why or why not.\n\n\nSolution to Question 14c"
  },
  {
    "objectID": "14-Estimation-MOM.html",
    "href": "14-Estimation-MOM.html",
    "title": "4.2: Method of Moments Estimates",
    "section": "",
    "text": "Building a Model for Bear Cub Weight\nA biologist is studying black bears. In particular, what distribution best fits the weight (in ounces) of newborn black bear cubs? Their sample data contains 10 observations, \\(x_1, x_2, \\ldots , x_{10}\\), corresponding the birth weight of 10 randomly selected black bear cubs.\nset.seed(113)  # fix randomization\n\nmu.cub &lt;- sample(seq(8.6, 9.8, 0.1), size=1)  # set value of mu\nsigma.cub &lt;- sample(seq(0.9, 1.3, 0.05), size=1)  # set value of sigma\n\npicked &lt;- rnorm(10, mu.cub, sigma.cub)  # pick a random sample n=10\ncub &lt;- data.frame(wt = picked)  # save sample to the cub data frame\nround(cub$wt, 2)  # print sample to screen\n\n [1]  9.71  7.77  8.47  7.35  7.83  9.06  8.66  8.74 10.82  8.27\nLet \\(X\\) be a random variable with pdf \\(f(x)\\). For a positive integer \\(k\\), the kth theoretical moment of \\(X\\) is \\(\\color{dodgerblue}{\\mu_k = E \\left( X^k \\right) }\\).\n\\[\\color{dodgerblue}{\\boxed{\\mu_k = E \\left( X^k \\right) = \\int_{-\\infty}^{\\infty} x^kf(x) \\, dx \\ \\ \\ \\mbox{(for continuous)} \\qquad \\mbox{or} \\qquad  \\mu_k = E \\left( X^k \\right) = \\sum_X x^kp(x) \\ \\ \\ \\mbox{(for discrete)}}},\\]\nLet \\(X\\) be a random variable with pdf \\(f(x; \\theta_1, \\theta_2, \\ldots, \\theta_k)\\) and let \\(X_1\\), \\(X_2\\), \\(\\ldots\\), \\(X_n\\) be a random sample.\n\\[\\begin{aligned}\n\\mu_1 = \\int_{-\\infty}^{\\infty} xf(x) \\, dx &= \\frac{1}{n} \\sum_{i=1}^n X_i = M_1\\\\\n\\mu_2 = \\int_{-\\infty}^{\\infty} x^2f(x) \\, dx &= \\frac{1}{n} \\sum_{i=1}^n X_i^2 = M_2\\\\\n& \\vdots \\\\\n\\mu_k = \\int_{-\\infty}^{\\infty} x^kf(x) \\, dx &= \\frac{1}{n} \\sum_{i=1}^n X_i^k = M_k\n\\end{aligned}\\]"
  },
  {
    "objectID": "14-Estimation-MOM.html#what-is-the-best-fitting-model",
    "href": "14-Estimation-MOM.html#what-is-the-best-fitting-model",
    "title": "4.2: Method of Moments Estimates",
    "section": "What is the Best Fitting Model?",
    "text": "What is the Best Fitting Model?\n\nFrom the code cell above, we have generated the following sample of cub birth weights (in ounces) that are stored in cub$wt,\n\\[x = (9.71, 7.77, 8.47, 7.35, 7.83, 9.06, 8.66, 8.74, 10.82, 8.27 ).\\]\nOur goal is to find the “best” description of the distribution ofOur goal is to find the “best” description of the distribution of all black bear cub birth weights. The interpretation of “best” depends on the context of the question and can mean different things to different statisticians."
  },
  {
    "objectID": "14-Estimation-MOM.html#question-1",
    "href": "14-Estimation-MOM.html#question-1",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 1",
    "text": "Question 1\n\nThe figure below shows a dot plot of the selected sample (size \\(n=10\\)) of cub birth weights along with the plots of 4 different models we could choose for our data. Answer the questions based on plot figure below.\n\n\n\n\n\n\nFigure 3.1: Comparing Models for Bear Cub Weights\n\n\n\n\n\n\nQuestion 1a\n\nWhich of the models labeled 1-4 in the plot above do you believe best fits the sample data cub birth weights?\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nWhat type of continuous distribution best matches the graph you selected? Explain why in terms of birth weights of black bear cubs this distribution is reasonable and makes practical sense.\n\nHint: See the appendix of common continuous random variables section for some options.\n\n\nSolution to Question 1b\n\n\n\n\n\n\n\nQuestion 1c\nUsing the sample of birth weights cub$wt, give estimates for each of the parameter(s) in the distribution you identified in Question 1b.\n\nSolution to Question 1c\n\n\n# be sure you have already run the first code cell and \n# stored sample weights to variable `wt` in data frame `cub`\n\nBased on your code above, what are the values of the parameters of the distribution in Question 1b?"
  },
  {
    "objectID": "14-Estimation-MOM.html#identifying-key-properties-for-our-model",
    "href": "14-Estimation-MOM.html#identifying-key-properties-for-our-model",
    "title": "4.2: Method of Moments Estimates",
    "section": "Identifying Key Properties for Our Model",
    "text": "Identifying Key Properties for Our Model\n\nLet \\(X\\) be a random variable with pdf \\(f(x; \\theta_1, \\theta_2, \\ \\ldots \\theta_k)\\) that depends on parameters \\(\\theta_1, \\theta_2, \\ldots , \\theta_k\\). If we independently pick a random variables \\(X_1, X_2, \\ldots X_n\\) from population \\(X\\), we can determine the values of \\(\\theta_1, \\theta_2, \\ldots , \\theta_k\\) that best fit the data in the following sense:\n\nThe mean \\(\\mu_X = E(X)\\) of the population \\(X\\) equals the sample mean, \\(\\bar{x}\\).\nThe variance \\(\\sigma^2_X = \\mbox{Var}(X)\\) of the population equals the variance of the sample, \\(s^2\\).\nThe skewness of the population equals the skewness of the sample.\nThe “peakiness” (kurtosis) of the population equals the “peakiness” of the sample.\n\\(\\ldots\\) and so on.\n\nFor example, in the birth weight of black bear cubs example, we assumed the population of birth weights \\(X\\) is normally distributed. Normal distributions are determined by two parameters, \\(\\mu\\) and \\(\\sigma\\).\n\nThe random sample has \\(\\bar{x} = 8.668\\). We estimate \\(E(X) = \\mu=\\bar{x} = 8.668\\).\nThe random sample has \\(s^2 = 1.032\\). We estimate \\(\\mbox{Var(X)} = \\sigma^2=s^2=1.032\\).\n\n\nWe find values of the parameters so the properties of random variable \\(X\\) are equal to corresponding statistics from our sample."
  },
  {
    "objectID": "14-Estimation-MOM.html#interpretation-of-theoretical-moments",
    "href": "14-Estimation-MOM.html#interpretation-of-theoretical-moments",
    "title": "4.2: Method of Moments Estimates",
    "section": "Interpretation of Theoretical Moments",
    "text": "Interpretation of Theoretical Moments\n\n\nThe first moment is \\(\\color{dodgerblue}{\\mu_1 = E \\left( X \\right) }\\).\n\n\\(\\mu_1\\) is the mean.\n\nThe second moment is \\(\\color{tomato}{\\mu_2 = E \\left( X^2 \\right) }\\).\n\n\\(\\mu_2\\) is related (but not equal) to the variance.\nIf we can find \\(\\mbox{Var}(X)\\) and have computed the first theoretical moment, \\(\\mu_1\\), we have:\n\n\n\\[{\\color{tomato}{\\mu_{2}}} = \\mbox{Var}(X) + \\mu_1^2 \\qquad \\mbox{since} \\qquad \\mbox{Var}(X) = E \\big( (X-\\mu_1)^2 \\big) = {\\color{tomato}{E(X^2)}} - \\mu_1^2 = {\\color{tomato}{\\mu_{2}}} - \\mu_1^2.\\]\n\nThe third moment is \\(\\color{mediumseagreen}{\\mu_3 = E \\left( X^3 \\right) }\\).\n\n\\(\\mu_3\\) is related to the skewness of \\(X\\) which is defined as \\(E \\big( (X-\\mu_1)^3 \\big)\\)\n\n\n\n\n\nCredit: Diva Jain, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\nThe fourth moment is \\(\\color{mediumpurple}{\\mu_4 = E \\left( X^4 \\right) }\\).\n\n\\(\\mu_4\\) is related to the kurtosis of \\(X\\) which is defined as \\(E \\big( (X-\\mu_1)^4 \\big)\\).\n\nInformally, the kurtosis measures how “peaky” or flat the distribution is.\n\n\n\n\n\n\n\n\n\nFigure 4.1: A Graphical Overview of Kurtosis"
  },
  {
    "objectID": "14-Estimation-MOM.html#sample-moments",
    "href": "14-Estimation-MOM.html#sample-moments",
    "title": "4.2: Method of Moments Estimates",
    "section": "Sample Moments",
    "text": "Sample Moments\n\nFor a sample \\(X_1=x_1, X_2=x_2, \\ldots , X_n=x_n\\), we can calculate the corresponding sample moments.\n\nThe first sample moment is \\(\\displaystyle M_1 = \\frac{1}{n} \\sum_{i=1}^n x_i\\).\nThe second sample moment is $M_2 = _{i=1}^n x_i^2 $.\nThe kth sample moment is \\(\\color{dodgerblue}{\\displaystyle M_k = \\frac{1}{n} \\sum_{i=1}^n x_i^k }\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWe use Latin letters \\(\\color{dodgerblue}{M_k}\\) to denote sample moments and Greek letters \\(\\color{tomato}{\\mu_k}\\) to denote theoretical moments for the population."
  },
  {
    "objectID": "14-Estimation-MOM.html#question-2",
    "href": "14-Estimation-MOM.html#question-2",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 2",
    "text": "Question 2\n\nLet \\(X\\) be a random variable with pdf\n\\[f(x; \\lambda, \\delta)=\\lambda e^{-\\lambda(x-\\delta)}\\]\nfor \\(x &gt; \\delta\\) with parameters \\(\\lambda, \\delta &gt;0\\).\n\nQuestion 2a\n\nWrite out (but do not evaluate) integrals that represent the first and second theoretical moments.\n\nSolution to Question 2a\n\n\\[\\mu_1 = E(X) = \\int_{\\delta}^{\\infty} \\left(   ?? \\right)  \\, dx\\] \\[\\mu_2 = E(X^2) = \\int_{\\delta}^{\\infty} \\left(   ?? \\right)  \\, dx\\]\n\n\n\n\n\nQuestion 2b\n\nApplying integration methods, we can evaluate the integrals in Question 2a to get the following expressions for the first and second theoretical moments.\n\\[\\mu_1 = E(X) = \\delta + \\frac{1}{\\lambda}.\\]\n\\[\\mu_2 = E(X^2) = \\left( \\delta + \\frac{1}{\\lambda} \\right)^2 + \\frac{1}{\\lambda^2}.\\]\nWhat integration methods do you believe will be useful to integrate the formulas in Question 2a? Explain in words how you could evaluate each of the integrals.\n\nTime permitting: Refresh your integration skills by verifying the formulas for \\(\\mu_1\\) and \\(\\mu_2\\).\n\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nLet \\(X_1=3\\), \\(X_2=4\\), \\(X_3 = 5\\), and \\(X_4 = 8\\) be a random sample from the random variable \\(X\\) from Question 2. Find the first and second sample moments.\n\nSolution to Question 2c"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-3",
    "href": "14-Estimation-MOM.html#question-3",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 3",
    "text": "Question 3\n\nLet \\(X\\) be a random variable from Question 2 with pdf \\(\\displaystyle f(x; \\lambda, \\delta)=\\lambda e^{-\\lambda(x-\\delta)}\\) for \\(x &gt; \\delta\\) with parameters \\(\\lambda, \\delta &gt;0\\). The first and second theoretical moments (see Question 2a and Question 2b) are\n\\[\\mu_1 = E(X) = \\delta + \\frac{1}{\\lambda} \\qquad \\mbox{and} \\qquad\n\\mu_2 = E(X^2) = \\left( \\delta + \\frac{1}{\\lambda} \\right)^2 + \\frac{1}{\\lambda^2}.\\]\nLet \\(X_1=3\\), \\(X_2=4\\), \\(X_3 = 5\\), and \\(X_4 = 8\\) be a random sample from \\(X\\). Find \\(\\hat{\\lambda}_{\\rm{MoM}}\\) and \\(\\hat{\\delta}_{\\rm{MoM}}\\), the MoM estimates for parameters \\(\\lambda\\) and \\(\\delta\\). Hint: Use the sample moments from Question 2c.\n\nSolution to Question 3"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-4",
    "href": "14-Estimation-MOM.html#question-4",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 4",
    "text": "Question 4\n\nLet \\(X_1=1, X_2=3, X_3=7, X_4=10\\) be four numbers picked at random from a continuous uniform distribution on \\(\\lbrack \\alpha , \\beta \\rbrack\\). Find the MoM estimates of \\(\\alpha\\) and \\(\\beta\\).\n\n\n\n\n\n\nTip\n\n\n\nYou do not need to evaluate any integrals to find expressions for \\(\\mu_1\\) and \\(\\mu_2\\). Recall if \\(X\\) is a continuous uniform distribtion, we have\n\\[E(X) = \\frac{\\alpha + \\beta}{2}  \\qquad \\mbox{and} \\qquad \\mbox{Var}(X) = \\frac{\\beta- \\alpha}{12}.\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\\(\\mu_2 \\ne \\mbox{Var}(X)\\). However, you can derive a formula for \\(\\mu_2 = E(X^2)\\) from formulas for both \\(\\mbox{Var}(X)\\) and \\(E(X)\\).\n\n\n\nSolution to Question 4"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-5",
    "href": "14-Estimation-MOM.html#question-5",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(X_1=1, X_2=3, X_3=3, X_4=2\\) be four values picked at random from a binomial distribution \\(X \\sim \\mbox{Binom}(n,p)\\). Find the MoM estimates of \\(n\\) and \\(p\\).\n\nSolution to Question 5"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-6",
    "href": "14-Estimation-MOM.html#question-6",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 6",
    "text": "Question 6\n\nLet \\(X_1=x_1, X_2=x_2, \\ldots X_n=x_n\\) denote a random sample size \\(n\\) from the continuous uniform distribution on \\(\\lbrack \\alpha , \\beta \\rbrack\\).\n\nQuestion 6a\n\nDerive the following general formulas for the MoM estimates of \\(\\alpha\\) and \\(\\beta\\):\n\\[\\hat{\\alpha}_{\\rm{MoM}} = M_1 - \\sqrt{3} \\left( \\sqrt{ M_2 - M_1^2} \\right)\\] \\[\\hat{\\beta}_{\\rm{MoM}} = M_1 + \\sqrt{3} \\left( \\sqrt{ M_2 - M_1^2} \\right)\\]\nwhere \\(M_1=\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) and \\(M_2= \\frac{1}{n} \\sum_{i=1}^n x_i^2\\) denote the first and second sample moments, respectively. Find the MoM estimates of \\(\\alpha\\) and \\(\\beta\\).\n\nSolution to Question 6a\n\n\n\n\n\n\n\nQuestion 6b\n\nVerify your solution to Question 4 using the formulas for the MoM estimates for \\(\\alpha\\) and \\(\\beta\\) in Question 6a.\n\nComplete and run the partially completed R code cell below.\n\n\nSolution to Question 6b\n\nReplace each of the two ?? in the code cell below with appropriate code. Then run the completed code to check the MoM estimates for \\(\\hat{\\alpha}_{\\rm{MoM}}\\) and \\(\\hat{\\beta}_{\\rm{MoM}}\\) obtained Question 4.\n\nx.unif &lt;- c(1, 3, 7, 10)  # sample from question 4\nn &lt;- length(x.unif)  # length of sample\n\nm1 &lt;- sum(x.unif)/n  # first sample moment\nm2 &lt;- sum(x.unif^2)/n  # second sample moment\n\nalpha.hat &lt;- ??  # enter formula for MoM estimate for alpha\nbeta.hat &lt;- ??   # enter formula for MoM estimate for beta\n\n# print results to screen\nalpha.hat\nbeta.hat"
  },
  {
    "objectID": "14-Estimation-MOM.html#question-7",
    "href": "14-Estimation-MOM.html#question-7",
    "title": "4.2: Method of Moments Estimates",
    "section": "Question 7",
    "text": "Question 7\n\nThe code below generates sampling distributions for MoM estimates for the parameters \\(\\alpha\\) and \\(\\beta\\) for random variable \\(X \\sim \\mbox{Unif}(\\alpha, \\beta)\\) using sample size \\(n=4\\).\n\nA total of 1,000 random samples each of size \\(n\\) are generated in the for loop.\nThe distribution of \\(\\hat{\\alpha}_{\\rm{MoM}}\\) values are stored in the vector mom.alpha.\nThe distribution of \\(\\hat{\\beta}_{\\rm{MoM}}\\) values are stored in the vector mom.beta.\n\n\n#############################\n# do not edit\n# run the code cell as is\n#############################\nn &lt;- 4  # sample size\n\nmom.alpha &lt;- numeric(1000)\nmom.beta &lt;- numeric(1000)\n\nfor (i in 1:1000)\n{\n  x.temp &lt;- runif(n, 0, 11)  # generate random sample\n  m1 &lt;- sum(x.temp)/n  # first sample moment\n  m2 &lt;- sum(x.temp^2)/n  # second sample moment\n  k &lt;- sqrt(3) * sqrt(m2 - m1^2)  # compute sqrt(3)*(m2 - m1^2)\n  mom.alpha[i] &lt;- m1 - k  # enter formula for MoM estimate for alpha\n  mom.beta[i] &lt;- m1 + k  # enter formula for MoM estimate for beta\n}\n\nThe distribution of \\(\\hat{\\alpha}_{\\rm{MoM}}\\) values generated by the code above is plotted in the histogram below.\n\nA blue vertical line is drawn at the actual value of \\(\\color{dodgerblue}{\\alpha=0}\\).\nA red vertical line is drawn at the value of \\(\\color{tomato}{\\hat{\\alpha}_{\\rm{MoM}}=-0.797}\\) we found for the sample in Question 4.\n\n\n#############################\n# do not edit\n# run the code cell as is\n#############################\nhist(mom.alpha, \n     breaks = 20,\n     xlab = \"MoM for alpha\",\n     main = \"Dist. of MoM's for alpha\")\nabline(v = 0, col = \"dodgerblue\", lwd = 2)  # plot at actual value of alpha\nabline(v = -0.797, col = \"tomato\", lwd = 2)  # plot at estimated value of alpha\n\n\n\n\nThe distribution of \\(\\hat{\\beta}_{\\rm{MoM}}\\) values generated by the code above is plotted in the histogram below.\n\nA blue vertical line is drawn at the actual value of \\(\\color{dodgerblue}{\\beta=11}\\).\nA red vertical line is drawn at the value of \\(\\color{tomato}{\\hat{\\beta}_{\\rm{MoM}}=11.297}\\) we found for the sample in Question 4.\n\n\n#############################\n# do not edit\n# run the code cell as is\n#############################\nhist(mom.beta, \n     breaks = 20,\n     xlab = \"MoM for beta\",\n     main = \"Dist. of MoM's for beta\")\nabline(v = 11, col = \"dodgerblue\", lwd = 2)  # plot at actual value of beta\nabline(v = 11.297, col = \"tomato\", lwd = 2)  # plot at estimated value of alpha\n\n\n\n\n\nQuestion 7a\n\nBased on inspecting the the sampling distributions plotted for \\(\\hat{\\alpha}_{\\rm{MoM}}\\) and \\(\\hat{\\beta}_{\\rm{MoM}}\\) in Question 7:\n\nDo you believe the MoM estimator for \\(\\alpha\\) is biased? Explain why or why not.\nDo you believe the MoM estimator for \\(\\beta\\) is biased? Explain why or why not.\nBase your answers on the distribution of all estimates, not just the red vertical lines corresponding to the sample from Question 4.\n\n\nSolution to Question 7a\n\n\n\n\n\n\n\nQuestion 7b\n\nCheck your answers in Question 7a more carefully using the MoM estimates stored in mom.alpha and mom.beta.\n\nHint: Recall an estimator \\(\\hat{\\theta}\\) is unbiased if \\(E(\\hat{\\theta}) = \\theta\\).\n\n\nSolution to Question 7b\n\n\n# check whether or not each estimator is biased\n\n\n\n\n\n\nQuestion 7c\n\nWhich estimator, \\(\\hat{\\alpha}_{\\rm{MoM}}\\) or \\(\\hat{\\beta}_{\\rm{MoM}}\\), is more precise?\n\nHint: Recall the precision of an estimator \\(\\hat{\\theta}\\) is often measured by \\(\\mbox{Var}(\\hat{\\theta}) = \\theta\\).\nHint: Use R code and the MoM estimates stored in mom.alpha and mom.beta.\n\n\nSolution to Question 7c\n\n\n# check which estimator is more precise\n\n\n\n\n\n\nQuestion 7d\n\nAdjust the sample size in the first line in the first code cell below to investigate what happens to the distributions of estimators \\(\\hat{\\alpha}_{\\rm{MoM}}\\) and \\(\\hat{\\beta}_{\\rm{MoM}}\\). In particular, as \\(n\\) gets larger and larger:\n\nDoes each estimator seem to get more, less, or no change in bias?\nDoes each estimator get more, less, or no change in variability?\nDoes the shape of each distribution change?\n\n\nSolution to Question 7d\n\n\n\nExperiment with different sample sizes, \\(n\\).\n\n\n#######################\n# adjust sample size\n#######################\nn &lt;- 4  # sample size\n\n#####################################\n# do not edit the rest of the code\n#####################################\n\nmom.alpha &lt;- numeric(1000)\nmom.beta &lt;- numeric(1000)\n\nfor (i in 1:1000)\n{\n  x.temp &lt;- runif(n, 0, 11)  # generate random sample\n  m1 &lt;- sum(x.temp)/n  # first sample moment\n  m2 &lt;- sum(x.temp^2)/n  # second sample moment\n  k &lt;- sqrt(3) * sqrt(m2 - m1^2)  # compute sqrt(3)*(m2 - m1^2)\n  mom.alpha[i] &lt;- m1 - k  # enter formula for MoM estimate for alpha\n  mom.beta[i] &lt;- m1 + k  # enter formula for MoM estimate for beta\n}\n\n\n##########################################\n# sampling distribution for MoM of alpha\n# do not edit cell, just run\n##########################################\nhist(mom.alpha, \n     breaks = 20,\n     xlab = \"MoM for alpha\",\n     main = \"Dist. of MoM's for alpha\")\nabline(v = 0, col = \"dodgerblue\", lwd = 2)  # plot at actual value of alpha\n\n\n##########################################\n# sampling distribution for MoM of beta\n# do not edit cell, just run\n##########################################\nhist(mom.beta, \n     breaks = 20,\n     xlab = \"MoM for beta\",\n     main = \"Dist. of MoM's for beta\")\nabline(v = 11, col = \"dodgerblue\", lwd = 2)  # plot at actual value of beta\n\n\n\n\nExploring Bias of Each Estimator\n\n\nAs \\(n\\) gets larger, does each estimator seem to get more, less, or no change in bias?\n\n\n# check for change to bias\n\n\n\n\n\nExploring Variability of Estimators\n\n\nAs \\(n\\) gets larger, does each estimator get more, less, or no change in variability?\n\n\n# check for change in variability\n\n\n\n\n\nExploring the Shape of Sampling Distributions\n\n\nDoes the shape of each distribution change as \\(n\\) gets larger?\n\n\n# check for change in shape\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "15-Properties-Estimators.html",
    "href": "15-Properties-Estimators.html",
    "title": "4.3: Properties of Estimators",
    "section": "",
    "text": "Comparing Estimators\nAt first glance, the question of which estimator is best may seem like a simple question.\nHowever, choosing the “best” estimator is not as straightforward as simply choosing the estimator that gives the value closest to the actual value. The parameters we are estimating are unknown values! We do not know where the center of the dart board is located. We cannot be certain which estimator leads to the closest estimate. Different samples will give different estimates even if we use the same formula for the estimator, and each estimate has some uncertainty due to sampling.\nWe can still choose a method that is more likely to give a better estimate (such as MLE) and/or minimizes the effect of the uncertainty due to sampling. Which properties are most important to consider depend on many factors. For example:\nThere are many properties of estimators worth considering when deciding between different estimators. In this section, we explore properties relating to accuracy (bias), precision (variability), and the mean squared error (MSE) which takes both bias and variability into consideration.\nNo matter what formula we choose as an estimator, the estimate we obtain will vary from sample to sample. We like an estimator to be, on average, equal to the parameter it is estimating. The bias of an estimator \\(\\hat{\\theta }\\) for parameter \\(\\theta\\) is defined as the difference in the average (expected) value of the estimator and the parameter \\(\\theta\\),\n\\[{\\large \\color{dodgerblue}{\\boxed{ \\mbox{Bias} = E(\\hat{\\theta}) - \\theta.}}}\\]\nEstimates that are perfectly unbiased may be impossible or unreasonable at times. In practice, we are satisfied when estimates are approximately unbiased, or when the bias gets closer and closer to 0 as the sample size, \\(n\\), gets larger.\nLet \\(\\hat{\\theta}\\) be an estimator for a parameter \\(\\theta\\). We can measure how precise \\(\\hat{\\theta}\\) is by considering how “spread out” the estimates obtained by selecting many random samples (each size \\(n\\)) and calculating an estimate \\(\\hat{\\theta}\\). The variance of the sampling distribution, \\(\\mbox{Var}(\\hat{\\theta})\\), measures the variability in estimates due to the uncertainty in random sampling. The standard error of \\(\\hat{\\theta}\\) is the standard deviation of the sampling distribution for \\(\\hat{\\theta}\\) and also commonly used.\nWe have explored bias and variability of estimators. It is not always possible or reasonable to use an unbiased estimator. Moreover, in some cases an estimator with a little bit of bias and very little variability might be preferred over an unbiased estimator that has a lot of variability. Choosing which estimator is preferred often involves a trade-off between bias and variability.\nThe Mean Squared Error (MSE) of an estimator \\(\\hat{\\theta}\\) measures the average squared distance between the estimator and the parameter \\(\\theta\\),\n\\[{\\color{dodgerblue}{\\mbox{MSE} \\big[ \\hat{\\theta} \\big] = E \\big[ (\\hat{\\theta}-\\theta)^2 \\big]}}.\\]\n\\[\\boxed{\\large {\\color{dodgerblue}{ \\mbox{MSE} \\big[ \\hat{\\theta} \\big] }} = {\\color{tomato}{\\mbox{Var} \\big[ \\hat{\\theta} \\big]}} + {\\color{mediumseagreen}{\\left( \\mbox{Bias}(\\hat{\\theta}) \\right)^2. }}}\\]"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-1",
    "href": "15-Properties-Estimators.html#question-1",
    "title": "4.3: Properties of Estimators",
    "section": "Question 1",
    "text": "Question 1\n\nSuppose our population parameter of interest is the center of a dart board. We use four different methods for throwing darts and the results of those four different methods are displayed in Figure 3.1.\n\n\n\n\n\n\n\nDart Method 1\n\n\n\n\n\n\n\nDart Method 2\n\n\n\n\n\n\n\nDart Method 3\n\n\n\n\n\n\n\nDart Method 4\n\n\n\n\nFigure 3.1: Comparing the results of four different methods for throwing darts.  Credit: Arbeck, CC BY 4.0, via Wikimedia Commons\n\n\n\nQuestion 1a\n\nRank the results of the four dart methods in terms of accuracy, from most to least accurate. Explain your reasoning.\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nRank the results of the four dart methods in terms of precision, from most to least precise. Explain your reasoning.\n\nSolution to Question 1b\n\n\n\n\n\n\n\nQuestion 1c\n\nRank the results of the four dart methods from best to worst overall. Explain your reasoning.\n\nSolution to Question 1c\n\n\n\n\n\n\n\nQuestion 1d\n\nFour different sampling distributions of the results of the four dart throwing methods are plotted in Figure 3.2. The location of the population parameter (the center of the dart board) is indicated by the dashed red line. The mean of the sampling distribution is indicated by the solid blue vertical line. Match each of the distributions labeled A-D below to one of the four dart boards displayed in Question 1.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\nFigure 3.2: Matching Distributions to Dart Boards\n\n\n\nSolution to Question 1d\n\n\nDart Distribution A matches dart method ??.\nDart Distribution B matches dart method ??.\nDart Distribution C matches dart method ??.\nDart Distribution D matches dart method ??."
  },
  {
    "objectID": "15-Properties-Estimators.html#question-2",
    "href": "15-Properties-Estimators.html#question-2",
    "title": "4.3: Properties of Estimators",
    "section": "Question 2",
    "text": "Question 2\n\nLet \\(X\\) denote the diastolic blood pressure (in mm Hg) of a randomly selected woman from the Pima Indian Community. The Pima Indian Community is mostly located outside of Phoenix, Arizona. The data1 used in this example is from the data frame Pima.tr in the MASS package.\nIf we suppose blood pressure is normally distributed, then we have \\(X \\sim N(\\mu, \\sigma)\\). We would like to decide which estimator is best for the population mean \\(\\mu\\). We pick a random sample of \\(n=20\\) women in the code cell below.\n\nlibrary(MASS)  # load MASS package to access data\n\n\nset.seed(20)  # fix randomization of sample\nx &lt;- sample(Pima.tr$bp, size=20)  # pick a random sample of 20 blood pressures\nx\n\n [1]  58  64  64  80  68  76  72 102  68  52  90  70  56  62  74  74  76  82  85\n[20]  52\n\n\nThe random sample of diastolic blood pressure values is \\[\\mathbf{x} = (58, 64, 64, 80, 68, 76, 72, 102, 68, 52, 90, 70, 56, 62, 74, 74, 76, 82, 85, 52).\\] Consider the following estimates for the population mean \\(\\mu\\):\n1. The sample mean: \\(\\hat{\\mu}_1 = \\bar{X} = \\dfrac{\\sum_{i=1}^n X_i}{n}\\).\n\n# 1. calculate sample mean\nmu.hat1 &lt;- mean(x)\nmu.hat1\n\n[1] 71.25\n\n\n2. The sample median, denoted \\(\\hat{\\mu}_2 = \\mbox{median}\\).\n\n# 2. calculate sample median\nmu.hat2 &lt;- median(x)\nmu.hat2 \n\n[1] 71\n\n\n3. The mid-range of the sample, \\(\\hat{\\mu}_3 = \\dfrac{X_{\\rm{max}} + X_{\\rm{min}}}{2}\\).\n\n# 3. calculate sample mid range\nmu.hat3 &lt;- (max(x) + min(x)) / 2\nmu.hat3\n\n[1] 77\n\n\n4. The sample trimmed (10%) mean, \\(\\bar{x}_{\\rm{tr}(10)}\\).\n\nWe exclude the smallest 10% of the values. The smallest 2 values, 52 and 52, are excluded.\nWe exclude the largest 10% of the values. The largest 2 values, 102 and 90, are excluded.\nWe compute the mean of the remaining 16 values.\n\n\n# 4. calculate sample trimmed (10%) mean\nmu.hat4 &lt;- mean(x, trim = 0.1)\nmu.hat4\n\n[1] 70.5625\n\n\n\nQuestion 2a\n\nWhich estimator (mean, median, mid-range, or trimmed mean) do you believe is best? Which estimator do you believe is the worst? Explain your reasoning.\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nTo help decide which estimator performs best, we can consider the sampling distribution of estimates obtained from many different random samples (each of size \\(n=20\\)) chosen independently from the same population. Based on the sampling distributions for \\(\\hat{\\mu}_1\\), \\(\\hat{\\mu}_2\\), \\(\\hat{\\mu}_3\\), and \\(\\hat{\\mu}_4\\) (mean, median, mid-range, and trimmed mean, respectively) in Figure 4.1, rank the four estimators from least to most bias and from most to least precise. Explain your reasoning.\n\n\n\n\n\n\n\n(a) Distribution of Sample Means\n\n\n\n\n\n\n\n(b) Distribution of Sample Medians\n\n\n\n\n\n\n\n\n\n(c) Distribution of Sample Mid-ranges\n\n\n\n\n\n\n\n(d) Distribution of Sample Trimmed Means\n\n\n\n\nFigure 4.1: Comparing Estimators for Mean Blood Pressure\n\n\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nWe still let \\(X\\) denote the diastolic blood pressure (in mm Hg) of a randomly selected woman from the Pima Indian Community. However, now we suppose blood pressure is uniformly distributed over the interval \\(\\lbrack 41.26, 101.26 \\rbrack\\). Although a uniform distribution would not make practical sense for blood pressure, we make this assumption in order to investigate how the shape of the population may affect which estimator works best.\nConsider the sampling distribution of estimates obtained from many different random samples (each of size \\(n=20\\)) chosen independently from a uniformly distributed population. Based on the sampling distributions for \\(\\hat{\\mu}_1\\), \\(\\hat{\\mu}_2\\), \\(\\hat{\\mu}_3\\), and \\(\\hat{\\mu}_4\\) (mean, median, mid-range, and trimmed mean, respectively) in Figure 4.2, rank the four estimators again from least to most bias and from most to least precise. Compare your updated rankings to those from Question 2b. Did your rankings change?\n\n\n\n\n\n\n\n(a) Distribution of Sample Means\n\n\n\n\n\n\n\n(b) Distribution of Sample Medians\n\n\n\n\n\n\n\n\n\n(c) Distribution of Sample Mid-ranges\n\n\n\n\n\n\n\n(d) Distribution of Sample Trimmed Means\n\n\n\n\nFigure 4.2: Comparing Estimators for a Uniform Population\n\n\n\nSolution to Question 2c"
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-prop-bias",
    "href": "15-Properties-Estimators.html#sec-prop-bias",
    "title": "4.3: Properties of Estimators",
    "section": "Question 3",
    "text": "Question 3\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown. Consider the following two estimators for parameter \\(p\\):\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\). This is equivalent to adding 4 more trials to the sample, 2 of which are successes.\n\n\n\n\n\n\n\nTip\n\n\n\nUse properties of expected value and recall these useful formulas for \\(X \\sim \\mbox{Binom}(n,p)\\),\n\\[E(X) = np \\quad \\mbox{and} \\quad \\mbox{Var}(X) = np(1-p).\\]\n\n\n\nQuestion 3a\n\nDetermine whether the estimator \\(\\hat{p} = \\frac{X}{n}\\) is biased or unbiased.\n\nSolution to Question 3a\n\n\n\n\n\n\n\nQuestion 3b\n\nDetermine whether the estimator \\(\\tilde{p} = \\frac{X+2}{n+4}\\) is biased or unbiased.\n\nSolution to Question 3b"
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-var-bias",
    "href": "15-Properties-Estimators.html#sec-var-bias",
    "title": "4.3: Properties of Estimators",
    "section": "Question 4",
    "text": "Question 4\n\nLet \\(X \\sim N(\\mu, \\sigma)\\). In Question 2, we consider several estimators for the parameter \\(\\mu\\). We now consider two possible estimators for the parameter \\(\\sigma^2\\), the variance of the population.\n\nUsing the estimator \\(\\displaystyle s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}\\).\nUsing the estimator \\(\\displaystyle \\hat{\\sigma}^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}\\).\n\n\nQuestion 4a\n\nProve the following statement:\nIf \\(X_1\\), \\(X_2\\), \\(\\ldots\\) , \\(X_n\\) are independently and identically distributed random variables with \\(E(X_i) = \\mu\\) and \\(\\mbox{Var}(X_i) = \\sigma^2\\), then\n\\[{\\color{dodgerblue}{\\boxed{E \\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] = (n-1)\\sigma^2.}}}\\]\n\n\n\n\n\n\nTip\n\n\n\nUse the result of Theorem 15.1 that states the following:\nIf \\(X_1\\), \\(X_2\\), \\(\\ldots\\) , \\(X_n\\) are independently and identically distributed random variables with \\(\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\), then\n\\[\\boxed{ E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n  E \\big[ X_i^2 \\big] - n E \\big[ \\overline{X}^2 \\big]}\\]\n\n\n\nSolution to Question 4a\n\nProof:\nWe first apply Theorem 15.1 to begin simplifying the expected value of the sum of the squared deviations,\n\\[E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}}\\]\nNext we simplify using properties of random variables and summations as follows,\n\\[\\begin{aligned}\nE\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] &= \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}} & \\mbox{by Theorem 15.1}\\\\\n&=  \\sum_{i=1}^n \\bigg( {\\color{dodgerblue}{ \\mbox{Var} \\big[ X_i \\big] + \\left( E \\big[ X_i \\big] \\right)^2 }} \\bigg) - n \\left( {\\color{tomato}{\\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2}} \\right) & \\mbox{Justification 1 ??}\\\\\n&=  \\sum_{i=1}^n {\\color{dodgerblue}{ \\left( \\sigma^2 + \\mu^2 \\right)}} - n \\left( \\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2 \\right) & \\mbox{Justification 2 ??}\\\\\n&=  \\sum_{i=1}^n\\left( \\sigma^2 + \\mu^2 \\right) - n \\left( {\\color{tomato}{\\frac{\\sigma^2}{n}}} + \\left( {\\color{tomato}{ \\mu }}\\right)^2 \\right) & \\mbox{Justification 3 ??} \\\\\n&= {\\color{dodgerblue}{n(\\sigma^2 + \\mu^2)}} - \\sigma^2 - n\\mu^2 & \\mbox{Justification 4 ??}\\\\\n&= (n-1) \\sigma^2. & \\mbox{Algebraically simplify}\n\\end{aligned}\\]\nThis concludes our proof!\n\nJustifications for Proof\n\n\nJustification 1:\nJustification 2:\nJustification 3:\nJustification 4:\n\n\n\n\n\n\n\nQuestion 4b\n\nDetermine whether the estimator \\(\\displaystyle s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}\\) is biased or unbiased.\n\n\n\n\n\n\nTip\n\n\n\nIf we apply the theorem we proved in Question 4a, this question should not require much more additional work!\n\n\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nDetermine whether the estimator \\(\\displaystyle \\hat{\\sigma}^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}\\) is biased or unbiased.\n\nSolution to Question 4c"
  },
  {
    "objectID": "15-Properties-Estimators.html#estimating-variance-and-standard-deviation",
    "href": "15-Properties-Estimators.html#estimating-variance-and-standard-deviation",
    "title": "4.3: Properties of Estimators",
    "section": "Estimating Variance and Standard Deviation",
    "text": "Estimating Variance and Standard Deviation\n\nThe variance of random variable \\(X\\) is defined as \\(\\sigma^2=\\mbox{Var} (X) = E\\big[ (X- \\mu)^2 \\big]\\). If we pick a random sample \\(X_1, X_, \\ldots , X_n\\) and want to approximate \\(\\sigma^2\\), then a reasonable recipe for estimating \\(\\sigma^2\\) could be to approximate \\(E \\big[ (X - \\mu)^2 \\big]\\) using the following process:\n\nUse the sample mean \\({\\color{tomato}{\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i}}\\) in place of the unknown value of the parameter \\({\\color{tomato}{\\mu}}\\).\nBased on the sample data, calculate the average value of \\((X_i - {\\color{tomato}{\\overline{X}}})^2\\).\n\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n \\left( X_i- \\overline{X} \\right)^2}{n}.\\]\nHowever, in Question 4 we showed this estimator is biased. For this reason:\n\nThe unbiased estimator \\(s^2\\) (that has \\(n-1\\) in the denominator) is usually used to estimate the population variance \\(\\sigma^2\\).\n\n\\[s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}.\\]\n\nAnd the estimator \\(s\\) (that also has \\(n-1\\) in the denominator) is usually used to estimate the population standard deviation \\(\\sigma\\).\n\n\\[s = \\sqrt{ \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}}.\\]\n\nThe R commands sd(x) and var(x) use the formulas for \\(s\\) and \\(s^2\\), respectively, with \\(n-1\\) in the denominator.\nFor large samples, the difference is very minimal whether we use the estimator with \\(n\\) or \\(n-1\\) in the denominator.\n\n\n\n\n\n\n\nWarning\n\n\n\nAlthough the sample variance \\(s^2\\) is an unbiased estimator for the population variance \\(\\sigma^2\\), the sample standard deviation \\(s\\) is in general a biased estimator for the population variance \\(\\sigma\\) since it is not true that \\(\\sqrt{E(X)} = E(\\sqrt{X})\\)."
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-mean-eff",
    "href": "15-Properties-Estimators.html#sec-mean-eff",
    "title": "4.3: Properties of Estimators",
    "section": "Question 5",
    "text": "Question 5\n\nLet \\(X_1, X_2, X_3\\) be independent random variables from an identical distribution with mean and variance \\(\\mu\\) and \\(\\sigma^2\\), respectively, and consider two possible estimators for \\(\\mu\\):\n\nThe usual sample mean, \\(\\hat{\\mu}_1 = \\overline{X} = \\frac{X_1 + X_2 + X_3}{3}\\).\nA weighted sample mean, \\(\\hat{\\mu}_2 = \\frac{1}{6}X_1 + \\frac{1}{3}X_2 + \\frac{1}{2} X_3\\).\n\n\nQuestion 5a\n\nProve both estimators \\(\\hat{\\mu}_1\\) and \\(\\hat{\\mu}_2\\) are unbiased estimators of \\(\\mu\\).\n\nSolution to Question 5a\n\n\n\n\n\n\n\nQuestion 5b\n\nCalculate \\(\\mbox{Var}( \\hat{\\mu}_1)\\) and \\(\\mbox{Var}( \\hat{\\mu}_2)\\), the variances of the estimators \\(\\hat{\\mu}_1\\) and \\(\\hat{\\mu}_2\\). Which estimator is more precise?\n\n\n\n\n\n\nTip\n\n\n\nRecall properties we can apply when finding the variance of a linear combination of independent random variables, and note the variances will depend on the unknown value of the population variance, \\(\\mathbf{\\sigma^2}\\).\n\n\n\nSolution to Question 5b"
  },
  {
    "objectID": "15-Properties-Estimators.html#efficiency-of-unbiased-estimators",
    "href": "15-Properties-Estimators.html#efficiency-of-unbiased-estimators",
    "title": "4.3: Properties of Estimators",
    "section": "Efficiency of Unbiased Estimators",
    "text": "Efficiency of Unbiased Estimators\n\nIf \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) are both unbiased estimators of \\(\\theta\\), then \\(\\hat{\\theta}_1\\) is said to be more efficient than \\(\\hat{\\theta}_2\\) if \\({\\color{dodgerblue}{\\mbox{Var} ( \\hat{\\theta}_1) &lt; \\mbox{Var} (\\hat{\\theta}_2)}}\\). For example, in Question 5 we show the usual sample mean \\(\\hat{\\mu}_1=\\overline{X}\\) is a more efficient estimator than the weighted mean \\(\\hat{\\mu}_2\\)."
  },
  {
    "objectID": "15-Properties-Estimators.html#question-6",
    "href": "15-Properties-Estimators.html#question-6",
    "title": "4.3: Properties of Estimators",
    "section": "Question 6",
    "text": "Question 6\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown. Consider the following two estimators for parameter \\(p\\):\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\).\n\nRecall in Question 3 we determined \\(\\hat{p}\\) is an unbiased estimator for \\(p\\) while \\(\\tilde{p}\\) is a slightly biased estimator.\n\n\n\n\n\n\nTip\n\n\n\nUse properties of variance and recall these useful formulas for \\(X \\sim \\mbox{Binom}(n,p)\\),\n\\[E(X) = np \\quad \\mbox{and} \\quad \\mbox{Var}(X) = np(1-p).\\]\n\n\n\nQuestion 6a\n\nFind \\(\\mbox{Var}(\\hat{p}) = \\mbox{Var} \\left( \\frac{X}{n} \\right)\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 6a\n\n\n\n\n\n\n\nQuestion 6b\n\nFind \\(\\mbox{Var}(\\tilde{p}) = \\mbox{Var} \\left( \\frac{X+2}{n+4} \\right)\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 6b"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-7",
    "href": "15-Properties-Estimators.html#question-7",
    "title": "4.3: Properties of Estimators",
    "section": "Question 7",
    "text": "Question 7\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown. Consider the following two estimators for parameter \\(p\\):\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\).\n\nUsing previous results regarding the bias and variability of the estimators we derived in Question 3 and Question 6, respectively, answer Question 7a and Question 7b to compare the MSE of the estimators.\n\nQuestion 7a\n\nGive a formula for \\(\\mbox{MSE}(\\hat{p})\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 7a\n\n\n\n\n\n\n\nQuestion 7b\n\nGive a formula for \\(\\mbox{MSE}(\\tilde{p})\\). Your answer will depend on the sample size \\(n\\) and the parameter \\(p\\).\n\nSolution to Question 7b"
  },
  {
    "objectID": "15-Properties-Estimators.html#sec-compare-prop",
    "href": "15-Properties-Estimators.html#sec-compare-prop",
    "title": "4.3: Properties of Estimators",
    "section": "Choosing a Sample Proportion",
    "text": "Choosing a Sample Proportion\n\nIn the case of \\(X \\sim \\mbox{Binom}(n,p)\\) with \\(n\\) known and parameter \\(p\\) unknown, we have considered two possible estimators for the population proportion \\(p\\).\n\nThe usual sample proportion, \\(\\hat{p} = \\frac{X}{n}\\).\n\nThis estimator makes the most practical sense.\nThis is the estimator obtained using MLE or MoM.\n\\(\\hat{p}\\) is unbiased.\nBut \\(\\hat{p}\\) can be less precise depending on the value of \\(p\\).\n\nA modified proportion, \\(\\tilde{p} = \\frac{X+2}{n+4}\\).\n\n\\(\\tilde{p}\\) is biased, but this may not be a problem:\n\nAs \\(n\\) gets larger and larger, the bias of this estimator gets smaller and smaller.\nThe bias is towards \\(0.5\\), so if \\(p\\) is close to \\(0.5\\) this is not a big issue.\n\n\\(\\tilde{p}\\) is a more precise estimator when \\(p\\) is not close to 0 or 1.\n\n\nFor example if \\(n=16\\), we have \\(X \\sim \\mbox{Binom}(16,p)\\). Figure 7.1 compares the values of \\(\\mbox{MSE}(\\hat{p})\\) and \\(\\mbox{MSE}(\\tilde{p})\\) for \\(n=16\\).\n\np &lt;- seq(0, 1, length.out = 100)  # values of p\nn &lt;- 16  # sample size\n\n# formula for MSE of p-hat\nmse.phat &lt;- (p * (1 - p)) / n\n\n# formula for MSE of p-tilde\nmse.ptilde &lt;- (n * p * (1 - p))/(n + 2)^2 + (1 - 2*p)^2/(n + 2)^2\n\n# plot of MSE(p-hat)\nplot(p, mse.phat,  \n     type = \"l\",\n     lwd =2,\n     col = \"firebrick2\",\n     main = \"Comparing MSE of Estimators for p when n=16\",\n     ylab = \"MSE\")\n\n# add plot of MSE(p-tilde)\nlines(p, mse.ptilde,  \n      lty=2, \n      lwd =2,\n      col = \"blue\")\n\n# add legend to plot\nlegend(0.37, 0.005, \n       legend=c(\"MSE(p.hat)\",\"MSE(p.tilde\"), \n       col=c(\"firebrick2\",\"blue\"), \n       lty=c(1,2), \n       ncol=1)\n\n\n\n\nFigure 7.1: Comparing MSE of p-hat and p-tilde for X ~ Binom(16,p)"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-8",
    "href": "15-Properties-Estimators.html#question-8",
    "title": "4.3: Properties of Estimators",
    "section": "Question 8",
    "text": "Question 8\n\nUsing the plots of \\({\\color{\\tomato}{\\mbox{MSE}(\\hat{p})}}\\) and \\({\\color{dodgerblue}{\\mbox{MSE}(\\tilde{p})}}\\) in Figure 7.1, identify the interval of \\(p\\) values where the MSE of \\(\\tilde{p}\\) is less than the MSE of \\(\\hat{p}\\).\n\nSolution to Question 8"
  },
  {
    "objectID": "15-Properties-Estimators.html#question-9",
    "href": "15-Properties-Estimators.html#question-9",
    "title": "4.3: Properties of Estimators",
    "section": "Question 9",
    "text": "Question 9\n\nRun run the code below for different values of \\(n\\) and say what happens to your choice of estimator as \\(n\\) gets larger.\n\n#########################################\n# adjust sample size n and run again\n# what happens to mse as n gets larger?\n#########################################\nn &lt;- ??  # sample size\n\n####################################################\n# you do not need to edit the rest of the code cell\n####################################################\n\np &lt;- seq(0, 1, length.out = 100)  # values of p\n\n# formula for MSE of p-hat\nmse.phat &lt;- (p * (1 - p)) / n\n\n# formula for MSE of p-tilde\nmse.ptilde &lt;- (n * p * (1 - p))/(n + 2)^2 + (1 - 2*p)^2/(n + 2)^2\n\n# plot of MSE(p-hat)\nplot(p, mse.phat,  \n     type = \"l\",\n     lwd =2,\n     col = \"firebrick2\",\n     main = \"Comparing MSE of Estimators for p\",\n     ylab = \"MSE\")\n\n# add plot of MSE(p-tilde)\nlines(p, mse.ptilde,  \n      lty=2, \n      lwd =2,\n      col = \"blue\")\n\n# add legend to plot\nlegend(0.37, 0.005, \n       legend=c(\"MSE(p.hat)\",\"MSE(p.tilde\"), \n       col=c(\"firebrick2\",\"blue\"), \n       lty=c(1,2), \n       ncol=1)\n\n\nSolution to Question 9\n\n\nInterpret the plots generated by the code above and answer the question."
  },
  {
    "objectID": "15-Properties-Estimators.html#footnotes",
    "href": "15-Properties-Estimators.html#footnotes",
    "title": "4.3: Properties of Estimators",
    "section": "",
    "text": "Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. and Johannes, R. S. (1988) “Using the ADAP learning algorithm to forecast the onset of diabetes mellitus”. In Proceedings of the Symposium on Computer Applications in Medical Care (Washington, 1988), ed. R. A. Greenes, pp. 261–265. Los Alamitos, CA: IEEE Computer Society Press.↩︎"
  },
  {
    "objectID": "16-Bootstrap-Dist.html",
    "href": "16-Bootstrap-Dist.html",
    "title": "5.1: Bootstrap Distributions",
    "section": "",
    "text": "Sampling From a Population\nRarely, we have access to data from the entire population of interest, in which case we are able to calculate the actual value(s) of population parameter(s). We can generate a sampling distribution by simulating the selection of many different random samples from the population data, and we can compute the standard error of the sampling distribution to measure how much uncertainty we can expect due to the randomness of sampling. If we have access to data from the entire population, there is no need for statistics to estimate parameters since we know the values of the parameters! Recall the distinction and connection and between parameters and statistics:\nIn some situations, we have a known probability distribution that we can use to build a model and make predictions. For example, characteristics such as height (normal), time between successive events (exponential), and counting the number of times an event occurs over an interval of time all behave predictably (Poisson). We can pick random sample and use point estimators such as MLE and MoM to estimate unknown population parameters. What happens if the data does not follow a known distribution?\nSuppose we would like to estimate the value of a parameter for a population about which we know very little information (this is often the case). We collect data from a single random sample of size \\(n\\), and then we can use statistics from the sample to make predictions about the population:\nIn any of these cases, how certain can/should we be in our estimate? In practice, we do not repeatedly pick 1000’s of random samples from the population. That is likely impractical, expensive, and time consuming. We have only collected data from a single random sample.\nThe data frame1 jackal in the permute package contains a sample of \\(n=20\\) mandibles from male and female golden jackals. For each of the 20 observations, two variables are recorded:\nWe have explored the sampling distributions of sample means, proportions, medians, variances and other estimators as a tool to assess the variability in those statistics and measure the level of uncertainty or precision in the estimate we obtain from the sample. In particular, the variance of a sampling distribution or the standard error (which is the square root of the variance of a sampling distribution) are commonly used to assess the variability in sample statistics.\nIn the case of the mean mandible length of all golden jackals, we have collected one sample of \\(n=20\\) adult golden jackals. We do not have access to data from the entire population, so we cannot construct a sampling distribution by picking many different random samples each size \\(n=20\\). Collecting unbiased samples can be quite expensive, time-consuming, and logistically difficult. If we only have one sample and know very little about the population, how can we generate a sampling distribution from this limited information?\nMonte Carlo methods are computational algorithms that rely on repeated random sampling. A bootstrap distribution is one example of a Monte Carlo method. A bootstrap distribution theoretically would contain the sample statistics from all possible bootstrap resamples. If we pick an initial sample size \\(n\\), then there exists a total of \\(n^n\\) possible bootstrap resamples. In the case of \\(n=20\\), we have \\(20^{20} \\approx 1.049 \\times 10^{26}\\) possible resamples. If we ignore the ordering in which we pick the sample, when \\(n=20\\), we have a total of \\(68,\\!923,\\!264,\\!410\\) (almost 69 billion!) distinct bootstrap resamples.\nFor small samples, we could write out all possible bootstrap resamples. For larger values of \\(n\\) (and we see \\(n=20\\) is already extremely large), it is really not practical or feasible to generate all possible bootstrap resamples while avoiding duplicates. Instead, we use Monte Carlo methods to repeatedly pick random samples that we use to approximate a sampling distribution. The Monte Carlo method of generating many (but necessarily all) bootstrap resamples introduces additional uncertainty and variability into the analysis. The more bootstrap resamples we choose, the less uncertainty we have.\nMonte Carlo methods were first explored by the Polish mathematician Stanislow Ulam in the 1940s while working on the initial development of nuclear weapons at Los Alamos National Lab in New Mexico. The research required evaluating extremely challenging integrals. Ulam devised a numerical algorithm based on resampling to approximate the integrals. The method was later named “Monte Carlo”, a gambling region in Monaco, due to the randomness involved in the computations.\nRecall if \\({\\color{tomato}{\\widehat{\\theta}}}\\) is an estimator for the parameter \\({\\color{mediumseagreen}{\\theta}}\\), then we define the bias of an estimator as\n\\[{\\large \\color{dodgerblue}{ \\boxed{\\mbox{Bias}(\\widehat{\\theta}) = {\\color{tomato}{\\widehat{\\theta}}} - {\\color{mediumseagreen}{\\theta}}}.}}\\]\nIn the case of bootstrapping:\n\\[{\\large \\color{dodgerblue}{ \\boxed{\\mbox{Bias}_{\\rm{boot}} \\big( \\hat{\\mu}_{\\rm{boot}} \\big) = {\\color{tomato}{\\hat{\\mu}_{\\rm{boot}}}} - {\\color{mediumseagreen}{\\bar{x}}}.}}}\\]\nLet \\(X\\) be the mandible length (in mm) of a randomly selected adult golden jackal. Based on the sample data in jaw.sample, we could come up with unbiased estimates for the population mean and population variance using:\nWe now have an estimate for the population, namely \\(X \\sim N(111, 3.88)\\). We can apply the Central Limit Theorem (CLT) for Means to construct another estimate for the sampling distribution. Although our sample size is relatively small (\\(n=20 &lt; 30\\)), we can apply CLT in this situation since the population is assumed to be symmetric (normally distributed).\nConsider the theoretical population \\(X \\sim N(23,7)\\). Below we compare the sampling distribution for the mean obtained using the central limit theorem on the top row with one random sample and a corresponding bootstrap distribution for the sample mean on the bottom row.\n(a) Distribution of Population\n\n\n\n\n\n\n\n(b) Sampling Distribution using CLT\n\n\n\n\n\n\n\n\n\n(c) Distribution of Sample\n\n\n\n\n\n\n\n(d) Bootstrap Dist Approximation of the Sampling Dist\n\n\n\n\nFigure 11.1: Comparing Bootstrap Approximation of Sampling Distribution to CLT"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-1",
    "href": "16-Bootstrap-Dist.html#question-1",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 1",
    "text": "Question 1\n\n\n\n\n\n\n\n\nCredit: Вых Пыхманн, CC BY-SA 3.0, Wikimedia Commons\n\n\n\n\n\n\n\nMariomassone, CC BY-SA 4.0, Wikimedia Commons\n\n\n\n\nFigure 5.1: Left: Golden Jackal (Canis aureus) Right: Mandible bones of wolf and jackal\n\n\nA zoologist would like to answer the following question?\n\nWhat is the average mandible (jaw) length of all golden jackals (Canis aureus)?\n\nDevise a method for collecting and analyzing data to help them answer this question.\n\nSolution to Question 1"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#loading-the-data",
    "href": "16-Bootstrap-Dist.html#loading-the-data",
    "title": "5.1: Bootstrap Distributions",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nIt is very likely you do not have the package permute installed. You will need to first install the permute package.\n\nGo to the R console window.\nRun the command &gt; install.packages(\"permute\").\n\nYou will only need to run the install.package() command one time. You can now access permute anytime you like! However, you will need to run the command library(permute) during any R session in which you want to access data from the permute package. Be sure you have first installed the permute package before executing the code cell below.\n\n# be sure you have already installed the permute package\nlibrary(permute)  # loading permute package\n\n\nSummarizing and Storing the Data\n\nIn the code cell below we load the jackal data from the permute package and provide a numerical summary of the two variables in the sample.\n\ndata(jackal)  # load jackal data\nsummary(jackal)  # numerical summary of each variable\n\n     Length          Sex    \n Min.   :105.0   Male  :10  \n 1st Qu.:107.8   Female:10  \n Median :111.0              \n Mean   :111.0              \n 3rd Qu.:113.2              \n Max.   :120.0              \n\n\nThe code cell below displays the distribution of mandible lengths separately for males and females.\n\n# side by side box plots\nplot(Length ~ Sex, data = jackal, \n     col = c(\"dodgerblue\", \"mediumseagreen\"),\n     main = \"Mandible Length of Golden Jackals\",\n     ylab = \"Length (in mm)\")  \n\n\n\n\nWe will be analyzing mandible lengths for both adult male and female golden jackals. In the code cell below, we save the \\(n=20\\) mandible lengths to a vector called jaw.sample.\n\njaw.sample &lt;- jackal$Length  # store mandible lengths to vector\njaw.sample  # print sample to screen\n\n [1] 120 107 110 116 114 111 113 117 114 112 110 111 107 108 110 105 107 106 111\n[20] 111"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-2",
    "href": "16-Bootstrap-Dist.html#question-2",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 2",
    "text": "Question 2\n\nBased on the sample above, what is your estimate for \\(\\mu\\), the mean mandible length of all adult golden jackals?\n\nSolution to Question 2\n\n\n# use jaw.sample to estimate population mean"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-3",
    "href": "16-Bootstrap-Dist.html#question-3",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 3",
    "text": "Question 3\n\nHow much confidence do you have in your estimate in Question 2? Any suggestions on how we can measure the uncertainty in our estimate due to the randomness of sampling?\n\nSolution to Question 3"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#what-is-a-statistical-question",
    "href": "16-Bootstrap-Dist.html#what-is-a-statistical-question",
    "title": "5.1: Bootstrap Distributions",
    "section": "What is a Statistical Question?",
    "text": "What is a Statistical Question?\n\nA statistical question is one that can be answered by collecting data and where there will be variability in that data.\n\nBased on a random sample of \\(n=20\\) adult golden jackals, what is the mean mandible length of all adult golden jackals?\n\n\nEach time we pick a different sample we have a different subset of data.\nDifferent samples have different sample means, leading to different estimates.\nThis is a statistical question!\nHow can we account for this variability in our estimate?\n\n\nUsing a database that contains information on all registered voters in Colorado, what proportion of all Colorado voters are over 50 years old?\n\n\nThe database includes information from the population of all registered voters in Colorado.\nWe can use the population data to calculate the proportion.\nThe population data does not change, so there is no variability in the value of the proportion.\nThis is not an example of a statistical question."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#bootstrap-distributions",
    "href": "16-Bootstrap-Dist.html#bootstrap-distributions",
    "title": "5.1: Bootstrap Distributions",
    "section": "Bootstrap Distributions",
    "text": "Bootstrap Distributions\n\nBootstrapping is the process of generating many different random samples from one random sample to obtain an estimate for a population parameter. For each randomly selected resample, we calculate a statistic of interest. Then we construct a new distribution of bootstrap statistics that approximates a sampling distribution for some sample statistic (such as a mean, proportion, variance, and others). We can use bootstrapping with any sample, even small ones. We can bootstrap any statistic. Thus, bootstrapping provides a robust method for performing statistical inference that we can adapt to many different situations in statistics and data science.\n\nA Bootstrapping Algorithm\n\nGiven an original sample of size \\(n\\) from a population:\n\nDraw a bootstrap resample of the same size, \\(n\\), with replacement from the original sample.\nCompute the relevant statistic (mean, proportion, max, variance, etc) of that sample.\nRepeat this many times (say \\(100,\\!000\\) times).\n\n\nA distribution of statistics from the bootstrap samples is called a bootstrap distribution.\nA bootstrap distribution gives an approximation for the sampling distribution.\nWe can inspect the center, spread and shape of the bootstrap distribution and do statistical inference."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-4",
    "href": "16-Bootstrap-Dist.html#question-4",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 4",
    "text": "Question 4\n\nConsider a random sample of 4 golden jackal mandible lengths (in mm):\n\\[120, 107, 110, \\mbox{ and } 116.\\]\nWhich of the following could be a possible bootstrap resample? Explain why or why not.\n\nQuestion 4a\n\n120, 107, 116\n\nSolution to Question 4a\n\n\n\n\n\n\n\nQuestion 4b\n\n110, 110, 110, 110\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\n120, 107, 110, 116\n\nSolution to Question 4c\n\n\n\n\n\n\n\nQuestion 4d\n\n120, 107, 110, 116, 120\n\nSolution to Question 4d\n\n\n\n\n\n\n\nQuestion 4e\n\n110, 130, 120, 107\n\nSolution to Question 4e"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-5",
    "href": "16-Bootstrap-Dist.html#question-5",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 5",
    "text": "Question 5\n\nHow many possible bootstrap resamples can be constructed from an original sample that has \\(n=20\\) values?\n\nSolution to Question 5\n\n\n# How many possible resamples are there for n=20?"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#sec-jawboot",
    "href": "16-Bootstrap-Dist.html#sec-jawboot",
    "title": "5.1: Bootstrap Distributions",
    "section": "Creating a Bootstrap Distribution in R",
    "text": "Creating a Bootstrap Distribution in R\n\nLet’s return to our statistical question:\n\nWhat is the average mandible (jaw) length of all golden jackals?\n\nWe have already picked one random sample of \\(n=20\\) adult golden jackals. The mandible lengths of our sample are stored in the vector jaw.sample.\n\nStep 1: Pick a Bootstrap Resample\n\nWe use the sample() function in R to pick a random sample of values out of the values in jaw.sample.\n\nNotice the resample has size \\(n=20\\), the same as the original sample.\nWe use the option replace = TRUE since we want to sample with replacement.\nRunning the code cell below creates one bootstrap resample stored in temp.samp.\n\n\ntemp.samp &lt;- sample(jaw.sample, size=20, replace = TRUE)  # sample with replacement\ntemp.samp  # print sample to screen\n\n [1] 114 120 106 111 114 111 107 111 113 110 110 107 106 107 110 120 112 111 110\n[20] 110\n\n\n\n\nStep 2: Calculate Statistic(s) from the Bootstrap Sample\n\nIn the golden jackal mandible length example, we want to use information about the distribution of sample means to estimate a population mean. Thus, we calculate the mean of the bootstrap resample temp.samp that we picked in the previous code cell.\n\nmean(temp.samp)  # mean of bootstrap resample\n\n[1] 111\n\n\n\n\nStep 3: Repeat Over and Over Again\n\nIn the code cell below, we repeat steps 1 and 2 over and over again. The sample means we calculate from each bootstrap resample are stored in a vector named boot.dist. Run the code cell below to generate a bootstrap distribution for the sample mean.\n\nA solid red line marks the location of the sample mean from the original sample.\nA dashed blue line marks the location of the mean of the bootstrap distribution.\nA solid green line marks the location of another published estimate for the population mean2.\n\n\n##########################\n# cell is ready to run\n# no need for edits\n##########################\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dist &lt;- numeric(N)  # create vector to store bootstrap means\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(jaw.sample, 20, replace = TRUE)  # pick a bootstrap resample\n  boot.dist[i] &lt;- mean(x)  # compute mean of bootstrap resample\n}\n\n# plot bootstrap distribution\nhist(boot.dist,  \n     breaks=20, \n     xlab = \"x-bar, mandible length (in mm)\",\n     main = \"Bootstrap Distribution for Sample Mean (n=20)\")\n\n# red line at the observed sample mean\nabline(v = mean(jaw.sample), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = mean(boot.dist), col = \"blue\", lwd = 2, lty = 2)\n\n# green line at the population mean, 112 mm\nabline(v = 112, col = \"mediumseagreen\", lwd = 2, lty = 1)"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-6",
    "href": "16-Bootstrap-Dist.html#question-6",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 6",
    "text": "Question 6\n\nWhat are the mean and standard error of the bootstrap distribution? Use the code below to compute each value.\n\n# calculate center of bootstrap dist\n\n\n# calculate bootstrap standard error\n\n\nSolution to Question 6"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-7",
    "href": "16-Bootstrap-Dist.html#question-7",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 7",
    "text": "Question 7\n\nCompute the bootstrap estimate of bias if we use the mean of the bootstrap distribution from Question 6 as our estimate for the mean mandible length of all adult golden jackals.\n\nSolution to Question 7"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-8",
    "href": "16-Bootstrap-Dist.html#question-8",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 8",
    "text": "Question 8\n\nWhat common distribution do you believe is the best model for mandible lengths of all golden jackals? Explain your reasoning.\n\nSolution to Question 8"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-9",
    "href": "16-Bootstrap-Dist.html#question-9",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 9",
    "text": "Question 9\n\nUsing the CLT with the population model \\(X \\sim N(111, 3.88)\\), we can derive a theoretical model for the distribution of sample means for \\(n=20\\). Using the CLT for means, give the mean and standard error for the sampling distribution for \\(\\overline{X}\\). How do your answers compare to approximations you found in Question 6 using the bootstrap distribution to estimate the sampling distribution for sample means?\n\nSolution to Question 9"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-10",
    "href": "16-Bootstrap-Dist.html#question-10",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 10",
    "text": "Question 10\n\n\nCompare the population and sample distributions. What is similar about the two distributions? What are the differences?\n\n\n\nCompare the CLT sampling distribution and bootstrap sampling distribution. What is similar about the two distributions? What are the differences?\n\n\nSolution to Question 10\n\nSolution to part a:\n\n\nSolution to part b:"
  },
  {
    "objectID": "16-Bootstrap-Dist.html#sec-plugin",
    "href": "16-Bootstrap-Dist.html#sec-plugin",
    "title": "5.1: Bootstrap Distributions",
    "section": "The Plug-in Principle",
    "text": "The Plug-in Principle\n\nThe Plug-in Principle: If something (such as a characteristic of a population) is unknown, substitute (plug-in) an estimate. For example, if we do not know the population mean \\(\\mu\\), the sample mean \\(\\bar{x}\\) is a nice, unbiased substitute. If a population standard deviation \\(\\sigma\\) is unknown, we can use substitute the sample standard deviation, \\(s\\).\n\nBootstrapping is an extreme application of this principle.\nWe replace the entire population (not just one parameter) by the entire set of data from the sample.\n\nEach bootstrap resample is picked from the same “population”, the original sample, to generate a bootstrap distribution that can be used to estimate a sampling distribution constructed from the entire population."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#question-11-arsenic-case-study",
    "href": "16-Bootstrap-Dist.html#question-11-arsenic-case-study",
    "title": "5.1: Bootstrap Distributions",
    "section": "Question 11: Arsenic Case Study",
    "text": "Question 11: Arsenic Case Study\n\nArsenic is a naturally occurring element in the groundwater in Bangladesh. Much of this water is used for drinking in rural areas, so arsenic poisoning is a serious health issue. The data set Bangladesh in the resampledata package3 contains measurements on arsenic, chlorine, and cobalt levels (in parts per billion, ppb) present in each of 271 groundwater samples.\n\nLoading the Data\n\nIt is very likely you do not have the package resampledata installed. You will need to first install the resampledata package.\n\nGo to the R console.\nRun the command &gt; install.packages(\"resampledata\").\n\nYou will only need to run the install.package() command one time. You can now access resampledata anytime you like! However, you will need to run the command library(resampledata) during any R session in which you want to access data from the resampledata package. Be sure you have first installed the resampledata package before executing the code cell below.\n\n# be sure you have already installed the resampledata package\nlibrary(resampledata)  # loading resampledata package\n\n\n\nQuestion 11a\n\nComplete the code cell below to calculate the mean and standard deviation and size of the arsenic level of the sample.\n\nSolution to Question 11a\n\n\n# be sure you have already installed the resampledata package\narsenic &lt;- Bangladesh$Arsenic  # store arsenic data in vector\nn.arsenic &lt;- length(arsenic)  # how many observations in arsenic\n\n################################\n# complete each command below\n################################\nmean.arsenic &lt;- ??  # sample mean\nsd.arsenic &lt;- ??  # sample standard deviation\nmean.arsenic  # print result to screen\nsd.arsenic  # print result to screen\n\n\n\n\n\n\nQuestion 11b\n\nCreate a histogram to show the shape of the distribution of the sample data. How would you describe the shape?\n\nSolution to Question 11b\n\n\n# create a histogram of the sample arsenic data\nhist(??)\n\n\n\n\n\n\nQuestion 11c\n\nComplete the code cell below to generate a bootstrap distribution for the sample mean. What are the mean and standard error of the bootstrap distribution?\n\nSolution to Question 11c\n\nReplace all eight ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution.\n\nN &lt;- 10^5  # number of bootstrap samples\nboot.arsenic &lt;- numeric(N)  # create vector to store bootstrap means\n\n# Set up a for loop!\n\nfor (i in 1:N)\n{\n  x &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.arsenic[i] &lt;- ??  # calculate relevant sample statistic\n}\n\nboot.mean &lt;- mean(??)  # calculate center of bootstrap dist\nboot.se &lt;- sd(??)  # calculate bootstrap standard error\n\n# plot bootstrap distribution\nhist(boot.arsenic,  xlab = \"xbar\",\n     main = \"Bootstrap Distribution\")\n\n# add a red line at the observed sample mean\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# add a blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n# print bootstrap estimate and standard error to screen\nboot.mean  # mean (center) of bootstrap dist\nboot.se  # standard error (spread) of bootstrap dist\n\n\n# compare bootstrap dist to normal dist\n# run to create a qq-plot\nqqnorm(boot.dist)\nqqline(boot.dist)\n\n\n\n\n\n\nQuestion 11d\n\nCalculate the bootstrap estimate for bias.\n\nSolution to Question 11d\n\n\n# calculate bootstrap estimate of bias\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "16-Bootstrap-Dist.html#footnotes",
    "href": "16-Bootstrap-Dist.html#footnotes",
    "title": "5.1: Bootstrap Distributions",
    "section": "",
    "text": "Manly, B.F.J. (2007) Randomization, bootstrap and Monte Carlo methods in biology. Third Edition. Chapman & Hall/CRC, Boca Raton.↩︎\nAli Louei Monfared, “Macro-Anatomical Investigation of the Skull of Golden Jackal (Canis aureus) and its Clinical Application during Regional Anesthesia”, Global Veterinaria 10 (5): 547-550, 2013.↩︎\nLaura M. Chihara and Tim C. Hesterberg (2019) Mathematical Statistics with Resampling and R. Second Edition. John Wiley & Sons, Hoboken, NJ.↩︎"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html",
    "href": "17-Bootstrap-Confidence-Int.html",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "",
    "text": "Case Study: Ozone Concentration in New York City\nAccoording to the Unites States Environmental Protection Agency (EPA)1:\nWhen the ozone concentration is greater, respiratory illnesses such as asthma, pneumonia, and bronchitis can become exacerbated. While the effects of short-term exposure to high ozone concentration are reversible, long-term exposure may not be reversible. The EPA sets Ozone National Ambient Air Quality Standards (NAAQS)2. “The existing primary and secondary standards, established in 2015, are 0.070 parts per million (ppm)”, or equivalently 70 parts per billion (pbb).\nWe will begin todays work with bootstrap distributions investigating the following question:\nThe sample mean for ozone concentration is below the 70 ppb limit. However, from the plot in Question 1a, we see there are number of observations in the sample with an ozone concentration over 70 ppb.\nsum(nyc.oz &gt; 70)\n\n[1] 24\nThe sample proportion of observations with an ozone concentration greater than 70 pbb is \\(24/111 \\approx 0.2162\\). We calculate the sample proportion in two different ways below. In both methods, we use the logical test nyc.oz &gt; 70 to help count the number of observations greater than 70 pbb.\n# method 1\nn &lt;- length(nyc.oz)  # number of observations in sample\nsum(nyc.oz &gt; 70) / n  # sample proportion over 70 ppb\n\n[1] 0.2162162\n# method 2\nmean(nyc.oz &gt; 70)  # sample proportion over 70 ppb\n\n[1] 0.2162162\nBoth methods are equivalent and we see that\n\\[\\hat{p} = 0.2162 = 21.62 \\%.\\] Thus use the plug-in principle, a reasonable estimate for the proportion of all time that the ozone concentration in NYC is over 70 ppb (we denote the population proportion \\(p\\)) is\n\\[ p \\approx \\hat{p} = 0.2612.\\] How certain can we be in this estimate? If we pick another sample of observations, would we get a similar estimate for \\(p\\), or should we expect a lot of variability?\nRecall the Central Limit Theorem (CLT) for Proportions,\n\\[\\widehat{P} \\sim N \\left( \\mu_{\\hat{P}}, \\mbox{SE}(\\widehat{P}) \\right) = \\left( {\\color{tomato}{p}}, \\sqrt{\\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}} \\right).\\]\nThe population proportion \\(p\\) is unknown, but we can use the plug-in principle and use the sample proportion \\({\\color{tomato}{\\hat{p} = 0.2162}}\\) in place to estimate the sampling distribution for the sample proportion:\n\\[\\begin{aligned}\n\\widehat{P} \\sim N \\left( \\mu_{\\hat{P}}, \\mbox{SE}(\\widehat{P}) \\right) &= \\left( {\\color{tomato}{p}}, \\sqrt{\\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}} \\right) \\\\\n& \\approx N\\left( {\\color{tomato}{0.2162}}, \\sqrt{\\frac{{\\color{tomato}{0.2162}}(1-{\\color{tomato}{0.2162}})}{111}} \\right) \\\\\n& \\approx N( 0.2162, 0.0391).\n\\end{aligned}\\]\nThe two methods, bootstrap distribution and the estimate from using the CLT, give us consistent results!\nUsually when estimating an unknown population parameter, we give an interval estimate that gives range of plausible values for the parameter by accounting for the uncertainty due to the variability in sampling.\n\\[ p_{\\rm over} \\approx \\hat{p}_{\\rm boot} \\color{dodgerblue}{\\pm \\mbox{SE}_{\\rm boot} \\left( \\widehat{P} \\right)} = 0.216 \\color{dodgerblue}{\\pm 0.039}.\\]\nThe interval between the \\(2.5\\) and \\(97.5\\) percentiles (or \\(0.025\\) and \\(0.975\\) quantiles) of the bootstrap distribution of a statistic is a 95% bootstrap percentile confidence interval for the corresponding parameter.\nFigure 10.1: Finding Cutoffs for a 95% Bootstrap Percentile Confidence Interval\nWe would expect the actual value of the unknown population parameter to equal the corresponding statistic calculated from one of the 100,000 bootstrap resamples in our distribution. Since 95% of the bootstrap statistics are inside the confidence interval:\nThe confidence level is the success rate of success of the interval estimate. We can choose differenet confidence levels for an interval estimate:\nOften in a study, we may be interested in determining whether there is an association between different variables. For example, we can ask:\nTo help explore this question, we will use a sample of data in the data frame5 birthwt that is in the package MASS which should already be installed. In the code cell below, the MASS package is loaded and numerical summaries for all variables in birthwt are computed and displayed.\nGiven independent samples of sizes \\(m\\) and \\(n\\) from two independent populations:\nm.non &lt;- length(non$bwt)  # m, size of sample 1\ntemp.non &lt;- sample(non$bwt, size = m.non, replace = TRUE)\nn.smoker &lt;- length(smoker$bwt)  # n, size of sample 2\ntemp.smoker &lt;- sample(smoker$bwt, size = n.smoker, replace = TRUE)\ndiff.resample &lt;- mean(temp.non) - mean(temp.smoker)\ndiff.resample\n\n[1] 209.3382\nGiven matched samples each of size \\(n\\):\nWe do a resample of the differences of each pair as opposed to two resamples from each individual sample.\nThe data frame darwin.maize in the agridat package contains results from experiment6 by Charles Darwin in 1876."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#loading-nyc-ozone-data",
    "href": "17-Bootstrap-Confidence-Int.html#loading-nyc-ozone-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Loading NYC Ozone Data",
    "text": "Loading NYC Ozone Data\n\nIt is very likely you do not have the package lattice installed. You will need to first install the lattice package.\n\nGo to the R Console window.\nRun the command &gt; install.packages(\"lattice\").\n\nYou will only need to run the install.package() command one time. You can now access lattice anytime you like! However, you will need to run the command library(lattice) during any R session in which you want to access data from the lattice package. Be sure you have first installed the lattice package before executing the code cell below.\n\n# be sure you have already installed the lattice package\nlibrary(lattice)  # loading permute package\n\n\nSummarizing and Storing the Data\n\nIn the code cell below we summarize a data frame3 named environmental from the lattice package and store the ozone concentration data environmental$ozone to a vector named nyc.oz.\n\nsummary(environmental)  # numerical summary of each variable\n\n     ozone         radiation      temperature         wind       \n Min.   :  1.0   Min.   :  7.0   Min.   :57.00   Min.   : 2.300  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.:71.00   1st Qu.: 7.400  \n Median : 31.0   Median :207.0   Median :79.00   Median : 9.700  \n Mean   : 42.1   Mean   :184.8   Mean   :77.79   Mean   : 9.939  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:84.50   3rd Qu.:11.500  \n Max.   :168.0   Max.   :334.0   Max.   :97.00   Max.   :20.700  \n\nnyc.oz &lt;- environmental$ozone  # store ozone data to a vector"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-1",
    "href": "17-Bootstrap-Confidence-Int.html#question-1",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 1",
    "text": "Question 1\n\nWe will use the sample data stored in nyc.oz to construct a bootstrap distribution that we can use to make predictions about the population of all times in New York City. Answer the questions below to get acquainted with the sample data.\n\nQuestion 1a\n\nHow many observations are in the sample stored in nyc.oz? Describe the shape of the data in nyc.oz.\n\nSolution to Question 1a\n\n\n\n\n\n\n\nQuestion 1b\n\nBased on the sample data in nyc.oz, give an estimate for the mean ozone concentration in New York City (over all days and times).\n\nSolution to Question 1b"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-2",
    "href": "17-Bootstrap-Confidence-Int.html#question-2",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 2",
    "text": "Question 2\n\nAnswer each part below to construct a bootstrap distribution for the sample proportion. Then use the result to answer the questions that follow.\n\nQuestion 2a\n\nComplete the code cell below to construct a bootstrap distribution for the sample proportion of observations with ozone concentration greater than 70 ppb.\n\n\n\n\n\n\nTip\n\n\n\nThere are two operations to complete inside the for loop:\n\nPick a bootstrap resample from the observed sample nyc.oz.\nCalculate the proportion of observations in the bootstrap resample with ozone concentration greater than 70 ppb.\n\n\n\n\nSolution to Question 2a\n\nReplace all six ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed sample proportion (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.prop &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.prop[i] &lt;- ??  # compute bootstrap sample proportion\n}\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 2b\n\nBased on your answer to Question 2a, calculate bootstrap estimate for bias. Note answers will vary due to the randomness of bootstrapping in Question 2a.\n\nSolution to Question 2b\n\n\n# calculate bootstrap estimate of bias\n\n\n\n\n\n\nQuestion 2c\n\nBased on your answer to Question 2a, calculate the bootstrap estimate for the standard error of the sampling distribution for sample proportions. Note answers will vary due to the randomness of bootstrapping in Question 2a.\n\nSolution to Question 2c\n\n\n# calculate bootstrap standard error"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-3",
    "href": "17-Bootstrap-Confidence-Int.html#question-3",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 3",
    "text": "Question 3\n\nIn Question 2, we created a bootstrap distribution (stored in the vector boot.prop) for the sample proportion of times the ozone concentration exceeds 70 ppb. One possible bootstrap distribution is plotted below. Bootstrap distributions will vary slightly depending on the 100,000 bootstrap resamples that are randomly selected.\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = mean(boot.prop), col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nQuestion 3a\n\nUsing the bootstrap statistics stored in boot.prop, find the lower and upper cutoffs for a 95% bootstrap percentile confidence interval for the proportion of all times the ozone concentration in NYC exceeds 70 ppb.\n\n\n\n\n\n\nTip\n\n\n\nRecall the quantile() function in R. Run the command ?quantile for a refresher!\n\n\n\nSolution to Question 3a\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 95% bootstrap percentile confidence interval.\n\n# find cutoffs for 95% bootstrap CI\nlower.boot.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.boot.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print values to screen\nlower.boot.95\nupper.boot.95\n\nBased on the output above, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n\n\nQuestion 3b\n\nThe code cell below plots a bootstrap distribution corresponding of the sample proportions stored in boot.prop along with two blue vertical lines to mark the lower and upper cutoffs for a 95% bootstrap percentile confidence interval. A red vertical line marks the value of the sample proportion we calculated from the original sample.\nRun the code cell below to illustrate the confidence interval on the bootstrap distribution. There is nothing to edit in the code cell. Then in the space below, explain the practical meaning of the interval to a person with little to no background in statistics.\n\n#################################\n# code is ready to run!\n# no need to edit the code cell\n#################################\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.95, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.95, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 3b\n\nInterpret the confidence interval from Question 3a.\n\n\n\n\n\n\nQuestion 3c\n\nSometimes, it is desirable to describe the interval as some value plus or minus some margin of error. Construct a symmetric 95% bootstrap confidence interval for the proportion of time the ozone concentration in NYC exceeds 70 ppb. Compare the symmetric confidence interval to your percentile confidence interval in Question 3a.\n\n\n\n\n\n\nTip\n\n\n\nRecall for normal distributions, approximately 95% of the data is within 2 standard deviations of center of the distribution.\n\n\n\nSolution to Question 3c\n\nReplace each ?? in the code cell below with appropriate code.\n\n?? - 2*??  # going 2 SE's below the observed sample proportion\n?? + 2*??  # going 2 SE's above the observed sample proportion\n\nBased on the output above, a symmetric 95% bootstrap confidence interval is from ?? to ??."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-4",
    "href": "17-Bootstrap-Confidence-Int.html#question-4",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 4",
    "text": "Question 4\n\nIn Question 2, we created a bootstrap distribution (stored in the vector boot.prop) for the sample proportion of times the ozone concentration exceeds 70 ppb. In Question 3, we used the bootstrap distribution to construct a 95% bootstrap percentile confidence interval. In this question, we will investigate what happens when we change the confidence level.\n\nQuestion 4a\n\nComplete the first code cell below to give a 90% bootstrap percentile confidence interval to estimate the proportion of all time in NYC when the ozone concentration exceeds 70 ppb.\nThen complete the second code cell to plot a histogram of the bootstrap distribution from with the upper and lower confidence interval cutoffs marked with vertical lines similar to the plot in Question 3b.\n\nSolution to Question 4a\n\nBased on the output below, a 90% bootstrap percentile confidence interval is from ?? to ??.\n\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 90% bootstrap percentile confidence interval.\n\n# find cutoffs for 90% bootstrap CI\nlower.prop.90 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.prop.90 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.prop.90\nupper.prop.90\n\nNothing to edit in the code cell below. Just be sure you first run the code cell above to calculate and store the cutoffs lower.prop.90 and upper.prop.90.\n\n##################################\n# code is ready to run!\n# no need to edit the code cell\n##################################\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.90, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.90, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 4b\n\nComplete the first code cell below to give a 99% bootstrap percentile confidence interval to estimate the proportion of all time in NYC when the ozone concentration exceeds 70 ppb.\nThen complete the second code cell to plot a histogram of the bootstrap distribution from with the upper and lower confidence interval cutoffs marked with vertical lines similar to the plot in Question 3b.\n\nSolution to Question 4b\n\nBased on the output below, a 99% bootstrap percentile confidence interval is from ?? to ??.\n\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 90% bootstrap percentile confidence interval.\n\n# find cutoffs for 99% bootstrap CI\nlower.prop.99 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.prop.99 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.prop.99\nupper.prop.99\n\nNothing to edit in the code cell below. Just be sure you first run the code cell above to calculate and store the cutoffs lower.prop.99 and upper.prop.99.\n\n##################################\n# code is ready to run!\n# no need to edit the code cell\n##################################\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# red line at the observed sample proportion\nabline(v = mean(nyc.oz &gt; 70), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.99, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.99, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 4c\n\nWhen we decreased the confidence level from 95% in Question 3a to 90% in Question 4a, did the interval estimate get wider or more narrow? Explain why this makes practical sense. Feel free to explain using the fishing analogy from earlier.\n\nSolution to Question 4c\n\n\n\n\n\n\n\nQuestion 4d\n\nExplain the trade-off between choosing a confidence level and the precision of the interval estimate. In particular, why would choose a 95% confidence interval over an interval that has a greater chance of success, such as a 99% confidence interval?\n\nSolution to Question 4d"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-5",
    "href": "17-Bootstrap-Confidence-Int.html#question-5",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 5",
    "text": "Question 5\n\nExplain how you could design a study to collect data that could help determine whether smoking during pregnancy has an affect on the weight of the baby at birth.\n\nSolution to Question 5"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#loading-the-birth-weight-sample",
    "href": "17-Bootstrap-Confidence-Int.html#loading-the-birth-weight-sample",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Loading the Birth Weight Sample",
    "text": "Loading the Birth Weight Sample\n\n\nlibrary(MASS)  # load MASS package\nsummary(birthwt)  # summary of data frame\n\n      low              age             lwt             race      \n Min.   :0.0000   Min.   :14.00   Min.   : 80.0   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:19.00   1st Qu.:110.0   1st Qu.:1.000  \n Median :0.0000   Median :23.00   Median :121.0   Median :1.000  \n Mean   :0.3122   Mean   :23.24   Mean   :129.8   Mean   :1.847  \n 3rd Qu.:1.0000   3rd Qu.:26.00   3rd Qu.:140.0   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :45.00   Max.   :250.0   Max.   :3.000  \n     smoke             ptl               ht                ui        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.3915   Mean   :0.1958   Mean   :0.06349   Mean   :0.1481  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :3.0000   Max.   :1.00000   Max.   :1.0000  \n      ftv              bwt      \n Min.   :0.0000   Min.   : 709  \n 1st Qu.:0.0000   1st Qu.:2414  \n Median :0.0000   Median :2977  \n Mean   :0.7937   Mean   :2945  \n 3rd Qu.:1.0000   3rd Qu.:3487  \n Max.   :6.0000   Max.   :4990"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-6",
    "href": "17-Bootstrap-Confidence-Int.html#question-6",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 6",
    "text": "Question 6\n\nHow many observations are in the data set? How many variables? Which variables are categorical and which are quantitative? Which variables are most important in helping determine whether smoking during pregnancy has an affect on the weight of the baby at birth?\n\nSolution to Question 6"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#cleaning-the-birth-weight-data",
    "href": "17-Bootstrap-Confidence-Int.html#cleaning-the-birth-weight-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Cleaning the Birth Weight Data",
    "text": "Cleaning the Birth Weight Data\n\nThe variable smoke is being stored as a quantitative variable.\n\nPregnant parents that did not smoke have a smoke value equal to 0.\nPregnant parents that were smokers have a smoke value equal to 1.\nRun the code cell below to make these categories more clearly labeled.\n\nNon-smokers are assigned a value of no.\nSmokers are assigned a value of smoker.\nWe use the factor() command to convert the smoke variable to a categorical variable.\nThe output tells us out of 189 parents, 115 self-identified as non-smokers and 74 as smokers.\n\n\n\nbirthwt$smoke[birthwt$smoke == 0]  &lt;- \"no\"\nbirthwt$smoke[birthwt$smoke == 1]  &lt;- \"smoker\"\nbirthwt$smoke &lt;- factor(birthwt$smoke)\nsummary(birthwt$smoke)\n\n    no smoker \n   115     74"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-7",
    "href": "17-Bootstrap-Confidence-Int.html#question-7",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 7",
    "text": "Question 7\n\nComplete the code cell below to create a side-by-side box plots to compare the distribution of weights for smokers and non-smokers.\n\nSolution to Question 7\n\nReplace each ?? in the code cell below to generate side-by-side box plots for comparison.\n\n# create side by side box plots\nplot(?? ~ ??, data = ??,\n     col = c(\"springgreen4\", \"firebrick2\"),\n     main = \"Comparison of Birth Weights from Smokers and Non-Smokers\",\n     xlab = \"Smoking Status of Pregnant Parent\",\n     ylab = \"Birth Weight (in grams)\",\n     names = c(\"Non-smoker\", \"Smoker\"))"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#difference-in-two-independent-means",
    "href": "17-Bootstrap-Confidence-Int.html#difference-in-two-independent-means",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Difference in Two Independent Means",
    "text": "Difference in Two Independent Means\n\nOur statistical question is:\n\nDoes smoking during pregnancy have an affect on the weight of the baby at birth?\n\nIn this example, we have two independent populations of parents to consider.\n\nAll parents that did no smoke during pregnancy.\nAll parents that smoke during pregnancy.\nYou are either in one group or the other, not both!\n\nIdeally, if we had access to data on every baby that has been born, then we could:\n\nCalculate \\(\\mu_{\\rm{non}}\\), the mean birth weight of all children of non-smokers.\nCalculate \\(\\mu_{\\rm{smoker}}\\), the mean birth weight of all children of smokers.\nConsider how large is the difference in the two means, \\(\\mu_{\\rm{non}} - \\mu_{\\rm{smoker}}\\).\n\nIf the difference in populations means is 0, there is no difference in mean birth weights.\nIf the difference is not 0, then there is a difference in mean birth weights.\n\n\nWe have data from a random sample, but we plug that data in place of the population and perform an equivalent analysis.\n\nCalculate \\(\\bar{x}_{\\rm{non}}\\), the sample mean birth weight of children of non-smokers in the sample.\nCalculate \\(\\bar{x}_{\\rm{smoker}}\\), the sample mean birth weight of children of smokers in the sample.\nConsider how large is the difference in the two means, \\(\\bar{x}_{\\rm{non}} - \\bar{x}_{\\rm{smoker}}\\).\n\nIf the difference is close to zero, this indicates there is likely no difference in the population means.\nIf the difference is not close to zero, this indicates there likely is a difference in the population means."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#subsetting-the-sample",
    "href": "17-Bootstrap-Confidence-Int.html#subsetting-the-sample",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Subsetting the Sample",
    "text": "Subsetting the Sample\n\nAt the moment, we have all of the sample data for smokers and non-smokers stored in the same data frame named birthwt. How can we calculate the sample mean birth weights for the non-smoking group separate from the smoking group? One way to compare the two samples is to split the sample into two independent subsets based on whether or not the child was birthed by a smoker or not."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-8",
    "href": "17-Bootstrap-Confidence-Int.html#question-8",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 8",
    "text": "Question 8\n\nAnswer the questions to find an initial estimate for the difference in the mean birth weights of all children born to a non-smoking parent compared to the mean birth weight of all children born to a parent that did smoke while pregnant.\n\nQuestion 8a\n\nComplete each of the subset() commands below to subset the data into two independent samples: parent was a smoker and parent was a non-smoker.\n\nSolution to Question 8a\n\n\n# subset the sample into two independent samples\nnon &lt;- subset(??, smoke == ??)\nsmoker &lt;- subset(??, smoke == ??)\n\n\n\n\n\n\nQuestion 8b\n\nComplete the code cell below to calculate, store, and print the difference in sample means based on the data in our original sample.\n\nSolution to Question 8b\n\n\n# calculate difference in sample means\nobs.diff &lt;- ??\nobs.diff  # print observed difference to screen\n\n\n\n\n\n\nQuestion 8c\n\nBased on your answer to [Quesiton 8b], given an estimate for the difference in the mean birth weights of all children born to a non-smoking parent compared to the mean birth weight of all children born to a parent that did smoke while pregnant. Include units in your answer.\n\nSolution to Question 8c\n\n\n\n\n\n\n\nQuestion 8d\n\nBased on your estimate in Question 8c, do you believe there is a difference in the mean birth weight of all babies whose parent smoked while pregnant compared to the mean birth weight of all babies whose parent did not smoke while pregnant?\n\nSolution to Question 8d"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#accounting-for-uncertainty-in-sampling",
    "href": "17-Bootstrap-Confidence-Int.html#accounting-for-uncertainty-in-sampling",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Accounting for Uncertainty in Sampling",
    "text": "Accounting for Uncertainty in Sampling\n\nIn the case of comparing samples, we do need to be mindful of the randomness involved in the sampling process. If we pick another sample of 189 babies and compare the difference in sample means, we will likely get another value for the difference in sample means. How can we determine whether the difference in sample means is larger than the variability we might expect due to sampling?"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-9",
    "href": "17-Bootstrap-Confidence-Int.html#question-9",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 9",
    "text": "Question 9\n\nFollow the steps below to generate a bootstrap distribution for the difference in sample means and obtain a 95% bootstrap percentile confidence interval for the difference in population means.\n\nQuestion 9a\n\nComplete the code cell below to construct a bootstrap distribution for the difference in the sample mean birth weights of of babies born to non-smokers compared to smokers.\n\nSolution to Question 9a\n\nReplace all six ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed sample proportion (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.diff.mean &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x.non &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  x.smoker &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.diff.mean[i] &lt;- ??  # compute difference in sample means\n}\n\n# plot bootstrap distribution\nhist(boot.diff.mean,  \n     breaks=20, \n     xlab = \"x.bar.non - x.bar.smoker (in grams)\",\n     main = \"Bootstrap Distribution for Difference in Means\")\n\n# red line at the observed  difference in sample means\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 9b\n\nComplete the code cell below to give a 95% bootstrap percentile confidence interval to estimate the difference in the mean birth weight of all babies born to non-smokers compared the to mean birth of all babies born to smokers.\n\nSolution to Question 9b\n\nBased on the output below, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n# find cutoffs for 95% bootstrap CI\nlower.bwt.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.bwt.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.bwt.95\nupper.bwt.95\n\n\n\n\nQuestion 9c\n\nInterpret the practical meaning of your interval estimate in Question 9b. Do you think it is plausible to conclude smoking does have an effect on the weight of a newborn? Explain why or why not.\n\nSolution to Question 9c\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#loading-the-data",
    "href": "17-Bootstrap-Confidence-Int.html#loading-the-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nIt is very likely you do not have the package agridat installed. You will need to first install the agridat package.\n\nGo to the R Console window.\nRun the command &gt; install.packages(\"agridat\").\n\nYou will only need to run the install.package() command one time. You can now access agridat anytime you like! However, you will need to run the command library(agridat) during any R session in which you want to access data from the agridat package. Be sure you have first installed the agridat package before executing the code cell below.\n\n# be sure you have already installed the agridat package\nlibrary(agridat)  # loading agridat package\n\n\nSummarizing and Storing the Data\n\nThe the help manual for the data frame darwin.maize has a nice summary of the experiment and the data. Run the code cell below to learn about the data and context of the experiment.\n\n# be sure you have already loaded the agridat package\n?darwin.maize\n\nBelow is an excerpt from the help manual:\n\nCharles Darwin, in 1876, reported data from an experiment that he had conducted on the heights of corn plants. The seeds came from the same parents, but some seeds were produced from self-fertilized parents and some seeds were produced from cross-fertilized parents. Pairs of seeds were planted in pots. Darwin hypothesized that cross-fertilization produced produced more robust and vigorous offspring.\n\nIn the code cell below we provide a numerical summary of the two variables in the sample.\n\nsummary(darwin.maize)  # numerical summary of each variable\n\n  pot          pair       type        height     \n I  : 6   a      : 2   cross:15   Min.   :12.00  \n II : 6   b      : 2   self :15   1st Qu.:17.53  \n III:10   c      : 2              Median :18.88  \n IV : 8   d      : 2              Mean   :18.88  \n          e      : 2              3rd Qu.:21.38  \n          f      : 2              Max.   :23.50  \n          (Other):18                             \n\n\n\npot is a categorical variable with 4 levels: I, II, III, and IV.\n\npair is a categorical variable with 15 levels: a, b, \\(\\ldots\\) , n, o.\ntype cagtegorical with 2 levels: cross and self.\nheight plant height in inches\n\nThe code cell below displays the distribution of mandible lengths separately for males and females.\n\n# side by side box plots\nplot(height ~ type, data = darwin.maize, \n     col = c(\"dodgerblue\", \"mediumseagreen\"),\n     main = \"Darwin Cross Fertilization Results\",\n     ylab = \"Hength (in inches)\")"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#wranginling-the-data",
    "href": "17-Bootstrap-Confidence-Int.html#wranginling-the-data",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Wranginling the Data",
    "text": "Wranginling the Data\n\nIn the code cell below, we reorganize the data from corn.maize so it is structured in a way that will be more convenient for our analysis. The result is stored in a new data frame named corn, and the first 6 rows are printed to the screen. In total, there are 18 pairs of data.\n\nself &lt;- subset(darwin.maize, select = c(pair, type, height), type == \"self\")\ncross &lt;- subset(darwin.maize, select = c(pair, type, height), type == \"cross\")\ncorn &lt;- data.frame(letters[1:15], self$height, cross$height)\nnames(corn) &lt;- c(\"pair\", \"self\", \"cross\")\nkbl(head(corn, 18), format = \"html\")\n\n\n\nTable 16.1: Corn Heights\n\n\npair\nself\ncross\n\n\n\n\na\n17.375\n23.500\n\n\nb\n20.375\n12.000\n\n\nc\n20.000\n21.000\n\n\nd\n20.000\n22.000\n\n\ne\n18.375\n19.125\n\n\nf\n18.625\n21.500\n\n\ng\n18.625\n22.125\n\n\nh\n15.250\n20.375\n\n\ni\n16.500\n18.250\n\n\nj\n18.000\n21.625\n\n\nk\n16.250\n23.250\n\n\nl\n18.000\n21.000\n\n\nm\n12.750\n22.125\n\n\nn\n15.500\n23.000\n\n\no\n18.000\n12.000"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#question-4-wetsuit-case-study",
    "href": "17-Bootstrap-Confidence-Int.html#question-4-wetsuit-case-study",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "Question 4: Wetsuit Case Study",
    "text": "Question 4: Wetsuit Case Study\n\nExample 20.5. Researchers are concerned about the impact of vitamin C content reduction due to storage and ship- ment. To test this, researchers randomly chose a collection of bags of wheat soy blend bound for Haiti, marked them, and measured vitamin C from a sample of the contents. Five months later, the bags were opened and a second sample was measured for vitamin C content. The units are milligrams of vitamin C per 100g of wheat soy blend. - Watkins page 366\n\n\n\nPhelps Wetsuit\n\n\nIn the 2008 Olympics there was a lot of controversy over new swimsuits that possibly provided an unfair advantage to swimmers which led to new international rules regarding swimsuit materials and coverage. Can a swimsuit really make a swimmer faster?7\nA study8 tested whether wearing wetsuits influences swimming velocity. Twelve competitive swimmers swam 1500 meters at maximum speed twice each.\n\nOnce wearing a wetsuit and once wearing a regular bathing suit.\nThe order of the trials was randomized.\nEach time, the maximum velocity in meters/sec of the swimmer was recorded.\nThe max velocity with and without the wetsuit are recorded in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwimmer\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nNew Wetsuit\n\\(1.57\\)\n\\(1.47\\)\n\\(1.42\\)\n\\(1.35\\)\n\\(1.22\\)\n\\(1.75\\)\n\\(1.64\\)\n\\(1.57\\)\n\\(1.56\\)\n\\(1.53\\)\n\\(1.49\\)\n\\(1.51\\)\n\n\nRegular Swimsuit\n\\(1.49\\)\n\\(1.37\\)\n\\(1.35\\)\n\\(1.27\\)\n\\(1.12\\)\n\\(1.64\\)\n\\(1.59\\)\n\\(1.52\\)\n\\(1.50\\)\n\\(1.45\\)\n\\(1.44\\)\n\\(1.41\\)\n\n\nDifference\n\\(0.08\\)\n\\(0.10\\)\n\\(0.07\\)\n\\(0.08\\)\n\\(0.10\\)\n\\(0.11\\)\n\\(0.05\\)\n\\(0.05\\)\n\\(0.06\\)\n\\(0.08\\)\n\\(0.05\\)\n\\(0.10\\)\n\n\n\n\nQuestion 4a\n\nThe code cell below contains ordered vectors of maximum velocities (with the wetsuit and with a regular swimsuit) for each of the 12 swimmers. The ordering in each vector must be consistent since we have natural pairing of values between the two samples. Compute the observed mean matched-pair difference.\n\nSolution to Question 4a\n\nComplete the code cell below by replacing each ?? with appropriate code.\n\n# Vectors containing the times of each swimmer. Ordering is critical\nwetsuit &lt;- c(1.57, 1.47, 1.42, 1.35, 1.22, 1.75, 1.64, 1.57, 1.56, 1.53, 1.49, 1.51)\nnone &lt;- c(1.49, 1.37, 1.35, 1.27, 1.12, 1.64, 1.59, 1.52, 1.50, 1.45, 1.44, 1.41)\nDiff &lt;- ??  # compute vector of matched-pair differences\nobs.match &lt;- ??  # compute mean of matched-pair differences\n\n# Print to screen\nobs.match\n\n\n\n\nQuestion 4b\n\nGenerate one possible bootstrap resample and print the values in your resample to the screen.\n\nSolution to Question 4b\n\nComplete the code cell below by replacing each ?? with appropriate code.\n\nsample(??, ??, replace = ??)\n\n\n\n\nQuestion 4c\n\nConstruct a 95% bootstrap percentile confidence interval to estimate this difference.\n\nSolution to Question 4c\n\nComplete the code cell below by replacing each ?? with appropriate code.\n\nn &lt;- length(Diff) # number of matched-pairs in sample\n\nN &lt;- 10^5\nboot.match &lt;- numeric(N)\n\n# Create bootstrap samples by picking n difference from original sample\n# With replacement, and then find mean of those differences.\nfor (i in 1:N)\n{\n  samp.diff &lt;- ??  # pick a bootstrap resample\n  boot.match[i] &lt;- ??  # compute mean of resampled matched-pair differences\n}\n\n\n# Bootstrap mean and bootstrap standard error\nmean.match &lt;- ??  # compute bootstrap mean\nse.match &lt;- ??  # compute bootstrap standard error\n\n\n# Calculate lower and upper cutoffs\n# for the 95% Bootstrap CI\nlower.match &lt;- quantile(boot.match, probs = ??)\nupper.match &lt;- quantile(boot.match, probs = ??)\nlower.match\nupper.match\n\n# Display bootstrap distribution\nhist(boot.match, xlab = \"difference in max velocity in m/sec (wetsuit - none)\",\n      main = \"Bootstrap Dist for Mean Matched-Pair Difference (n=12)\")\n\n\n# Add a red line at the observed mean matched-pair difference\nabline(v = ??, col = \"red\", lwd = 2, lty = 2)\n\n# Add blue lines at cutoffs for 95 percentile\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\nQuestion 4d\n\nInterpret the practical meaning of your answer to Question 4c. Do you think it is plausible to conclude the wetsuit gives a competitive advantage? Explain why or why not.\n\nSolution to Question 4d"
  },
  {
    "objectID": "17-Bootstrap-Confidence-Int.html#footnotes",
    "href": "17-Bootstrap-Confidence-Int.html#footnotes",
    "title": "5.2: Bootstrap Confidence Intervals",
    "section": "",
    "text": "https://www.epa.gov/ozone-pollution-and-your-patients-health/what-ozone accessed June 26, 2023↩︎\nOzone Air Quality Standards acccessed from EPA June 26, 2023↩︎\nBruntz, Cleveland, Kleiner, and Warner. (1974). “The Dependence of Ambient Ozone on Solar Radiation, Wind, Temperature, and Mixing Height”. In Symposium on Atmospheric Diffusion and Air Pollution. American Meterological Society, Boston.↩︎\nAnswers from Question 2 may vary↩︎\nBaystate Medical Center, Springfield, Massachusetts↩︎\nDarwin, C. R. 1876. “The effects of cross and self fertilisation in the vegetable kingdom”. London: John Murray.↩︎\nBased on content from Statistics: Unlocking the Power of Statistics by R. Lock, P. Lock, K. Lock, E. Lock, and D. Lock.↩︎\nde Lucas, Balidan, Neiva, Grecco, and Denadai. “The effects of wetsuits on physiological and biomechanical indices during swimming’’, Journal of Science and Medicine in Sport.↩︎"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html",
    "href": "18-Bootstrap-Other-Stats.html",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "",
    "text": "A Recap of Bootstrapping\nIn this section, we will continue exploring bootstrap distributions. Thus far, we have analyzed statistical questions where bootstrapping different statistics has been constructive, such as a single sample mean and a sample proportion. In many situations, we wish to investigate the possible association between two or more variables. In these cases, comparing two or more sample statistics is more useful than a single mean or a single proportion.\nFor example, we investigated whether there is an association between the smoking during pregnancy and birth weight of the baby.\nWe used a bootstrap distribution on a difference of two independent sample means to construct a bootstrap confidence interval to estimate the difference in mean birth weights to help determine whether a difference of \\(0\\) is plausible or not.\nLet’s recap and compare data sets from two different studies investigating potential side-effects of smoking during pregnancy:\nStudy A: There is a sample of 189 observations corresponding to 189 different pregnancies. For each observation, we have two values corresponding to smoking status and birth weight of baby. The sample is split based on whether or not the parent smoked during their pregnancy. For any single observation in the non-smoking group, there is no inherent connection to any of the observations in the smoking group. The non-smoker and smoker groups are independent samples.\nStudy B: There is a sample of \\(n=10\\) observations corresponding to \\(10\\) different people who each gave birth to two children. During one of the pregnancies they smoked, and they did not smoke during the other. For each observation, we have two variables: birth weight of baby when they smoked and the birth weight of the baby during the non-smoking pregnancy. For any single observation in the non-smoker group of birth weights, there is exactly one observation (their sibling) in the smoker group of birth weights that naturally form a pair of observations. This is an example of a matched pairs design since observations from each sample are matched based on a key variable. In this case, we pair non-smoking and smoking birth weights based on parent of the baby.\nWhen we bootstrap matched pairs, we do not want to randomness to affect the natural pairing between the two samples! Below is an algorithm for bootstrapping matched pairs. In order to preserve the pairing of the smoking and non-smoking birth weights, we do a single resample from the original sample of differences as opposed to two resamples from two independent samples.\nGiven a matched pairs sample of size \\(n\\):\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking birth weights\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight\n\n\ndiff &lt;- no - smoker  # storing the original sample of differences\nobs.mean &lt;- mean(diff)  # observed mean difference between matched pairs\n\n# print observed mean to screen\nobs.mean\n\n[1] 394\nresamp &lt;- sample(diff, size = 10, replace = TRUE)\nresamp\n\n [1]  539 1125   14   85   14  539   85  199  199  960\nmean(resamp)\n\n[1] 375.9\nWhen constructing sampling distributions for with means and proportions, in addition to bootstrapping, we also have a theoretical model available with the Central Limit Theorem (CLT). For example, we when working with golden jackal data, we constructed a bootstrap distribution to approximate the sampling distribution for the sample mean mandible length. Then we compared the bootstrap results to those obtained using the CLT for means. However, not all statistics have a Central Limit Theorem or formulas we can use to model a sampling distribution.\nWhen investigating the possible effect of smoking during pregnancy on birth weight, we initially used the birthwt data frame in the MASS package to construct a bootstrap distribution for the difference of means (DoM). From the bootstrap distribution, we obtained a confidence interval to estimate the difference in the mean birth weights between children of those that smoked and those that did not smoke during pregnancy. One possible 95% bootstrap percentile confidence interval for the difference of means is \\(80 \\mbox{ g} &lt; \\mu_{\\rm{non}} - \\mu_{s} &lt; 486 \\mbox{g }\\).\nAnother comparative analysis we can do is compare the ratio of means (RoM) instead of the difference of means. Let \\(R = \\dfrac{\\mu_{\\rm{non}}}{\\mu_{\\rm{s}}}\\) denote the ratio of population means. \\(R\\) is unknown population parameter that we can analyze as follows:\nWhether we use a difference or ratio of means can highlight or minimize certain characteristics of the data. With ratios, we generally get sample ratios close 1. With differences, we can get a large spread of values in the differences of sample means. Thus, one potentially nice outcome of using ratios is the magnitude of the sample statistics will be smaller and easier to compare with other ratios from other contexts that will also have values close to 1. Another nice advantage is the results are independent of the units we choose to measure birth weight. If we choose to measure birth weights in pounds instead of grams, birth weights will be closer to 7, 8, 9 pounds as opposed to values such as 2000 or 3000 grams. We get different confidence intervals for the difference in mean birth weights depending on whether we use grams or pounds. The bootstrap confidence intervals we get for a ratio of means will be the same regardless of the units for weight.\nRecall the data set birthwt from the MASS package we worked with earlier. In the code cell below, we load and clean the data in birthwt.\nlibrary(MASS)  # load MASS package\nbirthwt$smoke[birthwt$smoke == 0]  &lt;- \"no\"  # non-smokers assigned \"no\"\nbirthwt$smoke[birthwt$smoke == 1]  &lt;- \"smoker\"  # smokers assigned \"smoker\"\nbirthwt$smoke &lt;- factor(birthwt$smoke)  # convert smoke variable to categorical factor\nsummary(birthwt$smoke)  # summary of data frame\n\n    no smoker \n   115     74\nBefore we can bootstrap, we next subset the original sample into two independent samples based on smoking status. The output is stored in data frames non and smoker, and no output will be printed to the screen.\n# subset the sample into two independent samples\nnon &lt;- subset(birthwt, smoke == \"no\")\nsmoker &lt;- subset(birthwt, smoke == \"smoker\")\nRecall if \\(\\widehat{\\theta}\\) is an estimator for a parameter \\(\\theta\\), then we define the bias of the estimator as\n\\[\\mbox{Bias}(\\widehat{\\theta}) = (\\mbox{Estimated Value}) - (\\mbox{Actual Value}) = \\widehat{\\theta} - \\theta.\\]\nIn the case of bootstrapping, we used the following estimate:\n\\[\\mbox{Bias}_{\\rm{boot}} \\big( \\hat{\\theta}_{\\rm{boot}} \\big) = {\\color{dodgerblue}{\\hat{\\theta}_{\\rm{boot}}}} - {\\color{tomato}{(\\mbox{observed statistic})}}.\\]"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-1",
    "href": "18-Bootstrap-Other-Stats.html#question-1",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 1",
    "text": "Question 1\n\nThe data used in the smoking and birth weight analysis is from an observational study. We determined the observed difference in sample means is significant, and thus concluded smoking during pregnancy is likely associated with lower baby birth weights. However, with observational studies we should be mindful of confounding variables. Based on our analysis we can conclude smoking is associated with a change in birth weight, but not whether the smoking itself is the cause of the change in birth weight. What are some possible confounding variables?\n\nSolution to Question 1"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#designing-a-randomized-controlled-experiment",
    "href": "18-Bootstrap-Other-Stats.html#designing-a-randomized-controlled-experiment",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Designing a Randomized Controlled Experiment",
    "text": "Designing a Randomized Controlled Experiment\n\nIn order to see whether smoking during pregnancy causes lower birth weights, we could try to design a randomized controlled experiment. Designing such an experiment that is humane in this context might not be possible. For example, here is an example of a highly unethical experiment we would not implement in practice:\n\nCollect volunteers early in their pregnancy who agree to the conditions of the experiment.\nRandomly assign each pregnant person to a smoking or non-smoking group.\n\nEach person in the smoking group is required to smoke 1 pack of cigarettes per day.\nEach person in the non-smoking group is forbidden from smoking any tobacco during pregnancy.\n\nRecord the birth weight of each person’s baby.\nCompare mean birth weights of the two groups (smokers and non-smokers).\n\nAlthough such an experiment does eliminate the impact of many confounding variables, it is not possible to conduct this experiment due to ethical concerns."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#collecting-pairs-of-data",
    "href": "18-Bootstrap-Other-Stats.html#collecting-pairs-of-data",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Collecting Pairs of Data",
    "text": "Collecting Pairs of Data\n\nLet’s consider one more study that we could use to help determine whether smoking during pregnancy is associated with lower birth weight. In this study, we solicit volunteers that have already given birth to two babies. During one of the pregnancies, the parent smoked. During the other pregnancy, they did not smoke. This study is ethical and controls for some confounding variables though not all. Below is hypothetical data from such a study. A sample of \\(n=10\\) people volunteer to share their data with the researchers from which we have 10 different pairs of birth weights (in grams) summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-2",
    "href": "18-Bootstrap-Other-Stats.html#question-2",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the data from this study, give a possible sample statistic that can be used to determine if smoking is associated with lower birth weight?\n\nSolution to Question 2\n\n\n# data from study\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking births weights\n\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-3",
    "href": "18-Bootstrap-Other-Stats.html#question-3",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 3",
    "text": "Question 3\n\nDevise a bootstrapping method that could be used to construct a bootstrap distribution for the comparison statistic you identified in Question 2.\n\nSolution to Question 3"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-4",
    "href": "18-Bootstrap-Other-Stats.html#question-4",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 4",
    "text": "Question 4\n\nFollow the steps below to generate a bootstrap distribution for the mean difference between matched pairs of smoking and non-smoking birth weights. Then use the bootstrap distribution to obtain a 95% bootstrap percentile confidence interval for the mean difference between all matched pairs in the population.\n\nQuestion 4a\n\nComplete the code cell below to construct a bootstrap distribution for the sample mean difference between matched pairs.\n\nSolution to Question 4a\n\nReplace the six ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed mean difference between matched pairs (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.matched &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample\n  boot.matched[i] &lt;- ??  # compute sample mean of matched pair differences\n}\n\n# plot bootstrap distribution\nhist(boot.matched,  \n     breaks=20, \n     xlab = \"Sample Mean of Matched Pair Differenes\",\n     main = \"Bootstrap Distribution for Matched Pairs\")\n\n# red line at the observed mean difference between matched pairs\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\n\nQuestion 4b\n\nComplete the code cell below to give a 95% bootstrap percentile confidence interval to estimate the mean difference between all matched pairs in the population. Include units in your answer.\n\nSolution to Question 4b\n\nBased on the output below, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n# find cutoffs for 95% bootstrap CI\nlower.matched.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.matched.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.matched.95\nupper.matched.95\n\n\n\n\n\n\nQuestion 4c\n\nInterpret the practical meaning of your interval estimate in Question 4b. Do you think it is plausible to conclude smoking does have an effect on the weight of a newborn? Explain why or why not.\n\nSolution to Question 4c"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-5",
    "href": "18-Bootstrap-Other-Stats.html#question-5",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 5",
    "text": "Question 5\n\nA random sample of \\(n=20\\) mandible jaw lengths (in mm) of golden jackals is stored in the vector jaw.sample. Run the code cell below to load the sample data, and then answer the questions that follow.\n\njaw.sample &lt;- c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112,\n                110, 111, 107, 108, 110, 105, 107, 106, 111, 111)\n\n\nQuestion 5a\n\nBelow is code we previously used to create bootstrap distribution for the sample mean mandible length. Adjust the code to create a bootstrap distribution for the sample median mandible length. Then run the updated code cell to store the bootstrap medians in the vector named boot.dist. No output will printed to the screen since the output is being stored in boot.dist.\n\nSolution to Question 5a\n\nAdjust the code cell below to create a bootstrap distribution for the sample median.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dist &lt;- numeric(N)  # create vector to store bootstrap stats\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(jaw.sample, 20, replace = TRUE)  # pick a bootstrap resample\n  boot.dist[i] &lt;- mean(x)  # compute mean of bootstrap resample\n}\n\n\n\n\n\n\nQuestion 5b\n\nUsing the bootstrap distribution boot.dist created in Question 5a, give a 90% bootstrap percentile confidence interval for the median mandible length of all golden jackals. Include units in your answer.\n\nSolution to Question 5b\n\nReplace all four ?? in the code cell below with appropriate code. Then run the completed code to compute lower and upper cutoffs for a 90% bootstrap percentile confidence interval.\n\n# find cutoffs for 90% bootstrap CI\nlower.median.90 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.median.90 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.median.90\nupper.median.90\n\nBased on the output above, a 90% bootstrap percentile confidence interval for the median is from ?? to ??."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-6",
    "href": "18-Bootstrap-Other-Stats.html#question-6",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 6",
    "text": "Question 6\n\nCalculate the observed ratio of sample mean birth weights of smokers and non-smokers and store the value as obs.ratio.\n\nSolution to Question 6\n\n\nobs.ratio &lt;- ??  # calculate the observed ratio of sample means\nobs.ratio  # print observed ratio to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-7",
    "href": "18-Bootstrap-Other-Stats.html#question-7",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 7",
    "text": "Question 7\n\nFollow the steps below to generate a bootstrap distribution for the ratio of sample mean birth weights for babies born to non-smokers compared to babies born to smokers. Then use the bootstrap distribution to obtain a 95% bootstrap percentile confidence interval to estimate the ratio in population means.\n\nQuestion 7a\n\nComplete the code cell below to construct a bootstrap distribution for the ratio of the sample mean birth weights of babies born to non-smokers compared to babies born to smokers.\n\nSolution to Question 7a\n\nReplace all nine ?? in the code cell below with appropriate code. Then run the completed code to generate a bootstrap distribution and mark the observed ratio of sample means (in red) and the mean of the bootstrap distribution (in blue) with vertical lines.\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.rom &lt;- numeric(N)  # create vector to store bootstrap proportions\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x.non &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample of non-smokers\n  x.smoker &lt;- sample(??, size = ??, replace = ??)  # pick a bootstrap resample of smokers\n  boot.rom[i] &lt;- ??  # compute ratio of sample means\n}\n\n# plot bootstrap distribution\nhist(boot.rom,  \n     breaks=20, \n     xlab = \"Ratio = x.bar.non/x.bar.smoker\",\n     main = \"Bootstrap Distribution for Ratio of Means\")\n\n# red line at the observed  difference in sample means\nabline(v = ??, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = ??, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\n\nQuestion 7b\n\nComplete the code cell below to give a 95% bootstrap percentile confidence interval to estimate the ratio of the mean birth weight of all babies born to non-smokers compared the to mean birth of all babies born to smokers.\n\nSolution to Question 7b\n\nBased on the output below, a 95% bootstrap percentile confidence interval is from ?? to ??.\n\n\n\n# find cutoffs for 95% bootstrap CI\nlower.rom.95 &lt;- quantile(??, probs = ??)  # find lower cutoff\nupper.rom.95 &lt;- quantile(??, probs = ??)  # find upper cutoff\n\n# print to screen\nlower.rom.95\nupper.rom.95\n\n\n\n\n\n\nQuestion 7c\n\nInterpret the practical meaning of your interval estimate in Question 7b. Do you think it is plausible to conclude smoking does have an effect on the weight of a newborn? Explain why or why not.\n\nSolution to Question 7c\n\n\n\n\n\n\n\nQuestion 7d\n\nIs the shape of the bootstrap distribution for the ratio of means normal? Create a QQ-Plot to compare the bootstrap distribution to a standard normal distribution.\n\nSolution to Question 7d\n\nReplace each ?? with appropriate code.\n\nqqnorm(??)\nqqline(??)\n\n\n\nInterpret the plot above to answer the question."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-8",
    "href": "18-Bootstrap-Other-Stats.html#question-8",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 8",
    "text": "Question 8\n\nThe code cell below constructs a bootstrap distribution for the sample mean from the sample data stored in jaw.sample. The corresponding bootstrap sample means are stored in the vector boot.dist.\n\nRead the code cell below.\nAfter interpreting the code, run the code cell. No edits are needed.\nIn the code cell provided in the solution space, calculate the bootstrap estimate of bias.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the case of bootstrap estimate for a mean, we have \\({\\displaystyle \\mbox{Bias}_{\\rm{boot}} \\big( \\hat{\\mu}_{\\rm{boot}} \\big) = {\\color{dodgerblue}{\\hat{\\mu}_{\\rm{boot}}}} - {\\color{tomato}{\\bar{x}}}}\\). Use the output from the code below to identify the values of \\({\\color{dodgerblue}{\\hat{\\mu}_{\\rm{boot}}}}\\) and \\({\\color{tomato}{\\bar{x}}}\\).\n\n\n\n##############################\n# no edits needed\n# read and run the code\n#############################\njaw.sample &lt;- c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112,\n                110, 111, 107, 108, 110, 105, 107, 106, 111, 111)\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dist &lt;- numeric(N)  # create vector to store bootstrap stats\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(jaw.sample, 20, replace = TRUE)  # pick a bootstrap resample\n  boot.dist[i] &lt;- mean(x)  # compute mean of bootstrap resample\n}\n\n# plot bootstrap distribution\nhist(boot.dist,  \n     breaks=20, \n     xlab = \"x-bar, mandible length (in mm)\",\n     main = \"Bootstrap Distribution for Sample Mean (n=20)\")\n\n# bootstrap mean and standard error\nboot.dist.mean &lt;- mean(boot.dist)  # mean of bootstrap dist\nboot.dist.se &lt;- sd(boot.dist)  # SE of bootstrap dist\n\n\n# red line at the observed sample mean\nabline(v = mean(jaw.sample), col = \"firebrick2\", lwd = 2, lty = 1)\n# blue line at the center of bootstrap dist\nabline(v = boot.dist.mean, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 8\n\nReplace the ?? in the code cell below to calculate and store bootstrap estimate of bias to bias.jaw.\n\n# calculate bootstrap estimate of bias\nbias.jaw &lt;- ?? \nbias.jaw  # print to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-9",
    "href": "18-Bootstrap-Other-Stats.html#question-9",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 9",
    "text": "Question 9\n\nRecall the data set environmental from the lattice package we worked with earlier to estimate the proportion of all time in New York City when the ozone concentration exceeds 70 ppb. In the code cell below, we load the lattice package and store the sample of ozone concentrations to a vector named nyc.oz.\n\n\n\n\n\n\nNote\n\n\n\nIf you receive an error message when running the code cell below, then you do not have the lattice package installed.\n\nIn the R console, run the command install.packages(\"lattice\").\nThen rerun the code cell below.\n\n\n\n\n# be sure you have already installed the lattice package\nlibrary(lattice)  # loading lattice package\nnyc.oz &lt;- environmental$ozone  # store ozone data to a vector\n\nThe code cell below constructs a bootstrap distribution for the sample proportion of time the ozone concentration exceeds 70 ppb using the original sample nyc.oz. The corresponding bootstrap sample proportions are stored in the vector boot.prop.\n\nRead the code cell below.\nAfter interpreting the code, run the code cell. No edits are needed.\nIn the code cell provided in the solution space, calculate the bootstrap estimate of bias.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the case of bootstrap estimate for a proportion, we have \\({\\displaystyle\\mbox{Bias}_{\\rm{boot}} \\big( \\hat{p}_{\\rm{boot}} \\big) = {\\color{dodgerblue}{\\hat{p}_{\\rm{boot}}}} - {\\color{tomato}{\\hat{p}}}}\\).\n\n\n\n##############################\n# no edits needed\n# read and run the code\n#############################\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.prop &lt;- numeric(N)  # create vector to store bootstrap proportions\nn.oz &lt;- length(nyc.oz)\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(nyc.oz, size = n.oz, replace = TRUE)  # pick a bootstrap resample\n  boot.prop[i] &lt;- sum(x &gt; 70)/n.oz  # compute bootstrap sample proportion\n}\n\n# plot bootstrap distribution\nhist(boot.prop,  \n     breaks=20, \n     xlab = \"p-hat, sample proportion\",\n     main = \"Bootstrap Distribution for Sample Proportion\")\n\n# bootstrap mean and standard error\nboot.prop.mean &lt;- mean(boot.prop)  # mean of bootstrap dist\nboot.prop.se &lt;- sd(boot.prop)  # SE of bootstrap dist\n\n# red line at the observed sample proportion\nabline(v = sum(nyc.oz &gt; 70)/n.oz, col = \"firebrick2\", lwd = 2, lty = 1)\n# blue line at the center of bootstrap dist\nabline(v = boot.prop.mean, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 9\n\nReplace the ?? in the code cell below to calculate and store bootstrap estimate of bias to bias.ozone.\n\n# calculate bootstrap estimate of bias\nbias.ozone &lt;- ??\nbias.ozone  # print to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-10",
    "href": "18-Bootstrap-Other-Stats.html#question-10",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 10",
    "text": "Question 10\n\nRecall the data set birthwt from the MASS package we worked with earlier. In the code cell below, we load and clean the data in birthwt.\n\n\n\n\n\n\nNote\n\n\n\nIf you receive an error message when running the code cell below, then you do not have the MASS package installed.\n\nIn the R console, run the command install.packages(\"MASS\").\nThen rerun the code cell below.\n\n\n\n\nlibrary(MASS)  # load MASS package\nbirthwt$smoke[birthwt$smoke == 0]  &lt;- \"no\"  # non-smokers assigned \"no\"\nbirthwt$smoke[birthwt$smoke == 1]  &lt;- \"smoker\"  # smokers assigned \"smoker\"\nbirthwt$smoke &lt;- factor(birthwt$smoke)  # convert smoke variable to categorical factor\n\n# subset the sample into two independent samples\nnon &lt;- subset(birthwt, smoke == \"no\")\nsmoker &lt;- subset(birthwt, smoke == \"smoker\")\n\nThe code cell below constructs a bootstrap distribution for the difference in two sample means using the independent samples stored in the data frames non and smoker. The corresponding bootstrap difference in sample means are stored in the vector boot.dom.\n\nRead the code cell below.\nAfter interpreting the code, run the code cell. No edits are needed.\nIn the code cell provided in the solution space, calculate the bootstrap estimate of bias.\n\n\n##############################\n# no edits needed\n# read and run the code\n#############################\n\nN &lt;- 10^5  # Number of bootstrap samples\nboot.dom &lt;- numeric(N)  # create vector to store bootstrap proportions\n\nm.non &lt;- length(non$bwt)\nn.smoker &lt;- length(smoker$bwt)\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x.non &lt;- sample(non$bwt, size = m.non, replace = TRUE)  # pick a bootstrap resample\n  x.smoker &lt;- sample(smoker$bwt, size = n.smoker, replace = TRUE)  # pick a bootstrap resample\n  boot.dom[i] &lt;- mean(x.non) - mean(x.smoker)  # compute difference in sample means\n}\n\n# plot bootstrap distribution\nhist(boot.dom,  \n     breaks=20, \n     xlab = \"x.bar.non - x.bar.smoker (in grams)\",\n     main = \"Bootstrap Distribution for Difference in Means\")\n\n# bootstrap mean and standard error\nboot.dom.mean &lt;- mean(boot.dom)  # mean of bootstrap dist\nboot.dom.se &lt;- sd(boot.dom)  # SE of bootstrap dist\n\n# red line at the observed  difference in sample means\nabline(v = mean(non$bwt) - mean(smoker$bwt), col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue line at the center of bootstrap dist\nabline(v = boot.dom.mean, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\nSolution to Question 10\n\nReplace the ?? in the code cell below to calculate and store bootstrap estimate of bias to bias.smoke.\n\n# calculate bootstrap estimate of bias\nbias.smoke &lt;- ??\nbias.smoke  # print to screen"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-11",
    "href": "18-Bootstrap-Other-Stats.html#question-11",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 11",
    "text": "Question 11\n\nOut of the three bootstrap estimates of bias in Question 8, Question 9, and Question 10, which bias is the most extreme?\n\nSolution to Question 11"
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#rule-of-thumb-for-acceptable-bias",
    "href": "18-Bootstrap-Other-Stats.html#rule-of-thumb-for-acceptable-bias",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Rule of Thumb for Acceptable Bias",
    "text": "Rule of Thumb for Acceptable Bias\n\nWhat is a significant amount of bias? The answer to this question does not depend on the bootstrap estimate of bias alone. We need to consider how much bias is present relative to the overall variability in sample statistics. For example:\n\nIf the bootstrap estimate of bias is \\(5\\) and \\(\\mbox{SE}_{\\rm{boot}} = 5,\\!000\\), then relatively speaking the bias is quite small.\nIf the bootstrap estimate of bias is \\(-0.1\\) and \\(\\mbox{SE}_{\\rm{boot}} = 2\\), then relatively speaking the bias is much larger.\nWe compare the value of the bootstrap estimate of bias relative to the bootstrap standard error.\n\nIn the first example, we have the ratio \\(\\frac{5}{5000} = 0.001\\).\nIn the second example, we have a ratio \\(\\frac{-0.1}{2} = -0.05\\)\n\nIn general, we use the ratio of the bootstrap bias to the bootstrap standard error to measure how significant is the bias.\n\n\\[{\\color{dodgerblue}{\\boxed{ \\mbox{Relative Bootstrap Bias} = \\dfrac{\\mbox{Bootstrap Bias}}{\\mbox{Bootstrap SE}}}}}\\]\n\nRule of Thumb for Bootstrap Bias\n\nIf either \n\\[{\\color{dodgerblue}{\\frac{\\mbox{Bootstrap Bias}}{\\mbox{Bootstrap SE}}  &gt; 0.02 \\qquad \\mbox{or} \\qquad \\frac{\\mbox{Bootstrap Bias}}{\\mbox{Bootstrap SE}}  &lt; -0.02,}}\\]\nthen the bias is large enough to have a substantial effect on the accuracy of the estimate."
  },
  {
    "objectID": "18-Bootstrap-Other-Stats.html#question-12",
    "href": "18-Bootstrap-Other-Stats.html#question-12",
    "title": "5.3: Bootstrapping Comparitive Statistics",
    "section": "Question 12",
    "text": "Question 12\n\nDetermine whether the jackal mandible bootstrap distribution in Question 8, the ozone concentration bootstrap distribution for sample proportion in Question 9, or the bootstrap distribution for the difference in mean birth weights Question 10 exceed the 0.02 rule of thumb? Relatively speaking, which of the three biases is most extreme?\n\nSolution to Question 12\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "19-Parametric-CI-Means.html",
    "href": "19-Parametric-CI-Means.html",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "",
    "text": "Parametric and Non-Parametric Methods\nWe have been investigating resampling methods as a tool to estimate the variability in sample statistics. Statistics such as means and proportions are linear combinations of random variables, and using theory from probability, we are able the derive the Central Limit Theorem (CLT) formulas to model sampling distributions for means and proportions. In this section we will construct confidence intervals by applying a parametric approach that uses formulas from the CLT to measure the uncertainty (standard error) in our estimate.\nSuppose a climatologist has contacted us for help estimating the average wind speed of all storms in the North Atlantic. They have collected a random sample of \\(n=36\\) wind speeds from North Atlantic storms over the past 5 years.\nBefore introducing parametric methods for constructing a confidence interval estimate, we first review resampling methods to construct a bootstrap distribution from the sample my.sample that we can use to construct a bootstrap confidence interval. The code below uses the original sample my.sample to generate a bootstrap distribution for the sample mean wind speed (with \\(n=36\\)) that is stored in boot.wind for the next step.\nN &lt;- 10^5  # Number of bootstrap samples\nboot.wind &lt;- numeric(N)  # create vector to store bootstrap means\nn &lt;- 36  # sample size\n\n# for loop that creates bootstrap dist\nfor (i in 1:N)\n{\n  x &lt;- sample(my.sample, size = n, replace = TRUE)  # pick a bootstrap resample\n  boot.wind[i] &lt;- mean(x)  # compute bootstrap sample mean\n}\n\nboot.wind.est &lt;- mean(boot.wind)  # mean of bootstrap dist\nboot.wind.se &lt;- sd(boot.wind)  # bootstrap standard error\nmy.mean &lt;- mean(my.sample)  # original sample mean\nIn the code cell below, we use the bootstrap sample means (stored in boot.wind above) to construct a 95% bootstrap percentile confidence interval estimate for the mean wind speed of all North Atlantic storms.\nlower.boot.wind &lt;- quantile(boot.wind, 0.025)  # lower 95% cutoff\nupper.boot.wind &lt;- quantile(boot.wind, 0.975)  # upper 95% cutoff\n\n# print to screen\nlower.boot.wind   \n\n 2.5% \n51.25 \n\nupper.boot.wind\n\n   97.5% \n72.36111\nBased on the output from the previous code cell, we have a 95% bootstrap percentile confidence interval for the mean wind speed of all North Atlantic storms, namely from 51.25 knots to 72.36 knots. Note the interval estimates may vary, so you likely have a slightly different interval estimate if you are working with a different sample.\nIn the code cell below, we plot the bootstrap distribution with the original sample mean of my.sample marked with a red vertical line and the cutoffs for the confidence interval marked with blue vertical lines.\n#################################\n# code is ready to run!\n# no need to edit the code cell\n#################################\n\n# plot bootstrap distribution\nhist(boot.wind,  \n     breaks=20, \n     xlab = \"x.bar, sample mean wind speed (in knots)\",\n     main = \"Bootstrap Distribution for Sample Mean\")\n\n# red line at the observed sample proportion\nabline(v = my.mean, col = \"firebrick2\", lwd = 2, lty = 1)\n\n# blue lines marking cutoffs\nabline(v = lower.boot.wind, col = \"blue\", lwd = 2, lty = 2)\nabline(v = upper.boot.wind, col = \"blue\", lwd = 2, lty = 2)\nRecall the 68%-95%-99.7% empirical rule for normal distributions that states approximately:\nFor a sample of size \\(n\\) randomly picked from a normal distribution with unknown \\(\\mu\\) and known \\(\\mbox{Var}(X)=\\sigma^2\\), a 95% confidence interval for the mean is\n\\[{\\color{dodgerblue}{\\boxed{ \\large \\overline{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt;  \\overline{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}}}}.\\]\nIf we draw 1000’s of random samples each size \\(n\\) from a normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) and compute a 95% confidence interval from each sample, then about 95% of the intervals would successfully contain \\(\\mu\\). The 95% is the confidence level of the interval and gives the success rate of the interval estimate.\nIt can be easy to confuse the terms uncertain and unknown. In a less formal setting, people may use these terms interchangeably. In statistical inference, there is a clear distinction in their meaning that is important to clarify:\nFor a sample of size \\(n\\) randomly picked from a normal distribution with unknown \\(\\mu\\) and known \\(\\mbox{Var}(X)=\\sigma^2\\), then a confidence interval for the mean is given by\n\\[\\boxed{ \\large \\overline{X} - {\\color{tomato}{z_{\\alpha/2}}} \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + {\\color{tomato}{z_{\\alpha/2}}} \\cdot \\frac{\\sigma}{\\sqrt{n}}},\\]\nwhere the area under \\(N(0,1)\\) between \\(\\pm z_{\\alpha/2}\\) is equal to the confidence level of the estimate.\n\\[\\boxed{ \\large  \\mu \\approx  \\overline{X} \\pm {\\color{dodgerblue}{\\mbox{MoE}}}}\\]\nLet CL denote a selected confidence level that is the proportion of area in the middle.\nFigure 15.1: Identifying Z Values for Confidence Interval\nWhen estimating a population mean, in addition to \\(\\mu\\) being unknown, we often do not know the population variance, \\(\\sigma^2\\), either. We pick a random sample size \\(n\\), and we can plug-in the sample mean \\(\\bar{x}\\) as our point estimate for \\(\\mu\\). How can we find the margin of error if \\(\\sigma\\) is unknown?\nFrom the CLT for means, as long as the sample is normally distributed or if \\(n \\geq 30\\), then we know the distribution of sample means \\(\\overline{X}\\) is normally distributed with mean \\({\\color{mediumseagreen}{\\mu_{\\overline{X}} = \\mu_X}}\\) and standard error \\({\\color{dodgerblue}{\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}}}\\). Thus, the standardized distribution of \\(z\\)-scores corresponding the sampling distribution for the sample mean \\(\\overline{X}\\) is normal:\n\\[Z = \\frac{\\overline{X} - \\color{mediumseagreen}{\\mu_{\\overline{X}}}}{{\\color{dodgerblue}{\\sigma_{\\overline{X}}}}} = \\frac{\\overline{X} - \\color{mediumseagreen}{\\mu_{X}}}{{\\color{dodgerblue}{\\sigma_{X}/\\sqrt{n}}}} \\sim N(0,1).\\]\nLet’s apply our substitution of \\(s\\) in place of \\(\\sigma_{X}\\) and consider the resulting distribution of standardized statistics denoted \\(W\\):\n\\[Z = \\frac{\\overline{X} - \\mu_{X}}{{\\color{tomato}{\\sigma_{X}}}/\\sqrt{n}} \\quad \\xrightarrow{\\text{plug-in } {\\color{tomato}{s}} \\text{ for } {\\color{tomato}{\\sigma_{X}}}}  \\quad W = \\frac{\\overline{X}-\\mu_X}{{\\color{tomato}{s}}/\\sqrt{n}} \\sim \\mbox{ ?}\\]\n(a) Distribution of Standardized Statistics.\n\n\n\n\n\n\n\n(b) QQ-plot comparing W to N(0,1).\n\n\n\n\nFigure 16.1: Standardized Distribution When Variance Unknown.\nThe distribution \\(W\\) is symmetric and bell-shaped. From the QQ-plot, we see \\(W\\) is approximately normal in the middle of the distribution. However, the tails of distribution \\(W\\) are not similar to tails of a normal distribution. Distribution \\(W\\) has fatter tails than a normal distribution.\nFigure 16.2: Plots of t-distributions with different degrees of freedom\nIn our analysis of North Atlantic storm wind speeds, we used data stored in the storms data frame in the dplyr package. In our initial analysis, we assumed the data in storms represented the population, and we did not have access to the population data in storms. The data in storms is actually a sample of 19066 storm observations taken for North Atlantic storms that are measured every six hours during the lifetime of the storm. The population of interest is all storms that occur at all times in the North Atlantic.\nlibrary(dplyr)\nstorms$month &lt;- factor(storms$month)  # convert month to categorical factor\nboxplot(wind ~ month, \n        data = storms,\n        main = \"Distribution of Wind Speeds by Month\",\n        xlab = \"Month\",\n        ylab = \"Wind speed (in knots)\")\nThe t.test() function in R can used to construct a confidence interval for a difference in means from two independent populations using a \\(t\\)-distribution with Welch’s approximation for the degree’s for freedom. Let x and y denote vectors containing data from each of the two independent samples."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#loading-storms-data",
    "href": "19-Parametric-CI-Means.html#loading-storms-data",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Loading Storms Data",
    "text": "Loading Storms Data\n\nWhen practicing exploratory data analysis earlier, we accessed the dplyr package that contains a data set from the NOAA Hurricane Best Track Data, called storms, with data on many different storm attributes. We will begin analyzing the variable wind that gives the wind speed in knots. Run the code cell below to:\n\nLoad the dplyr package (which should already be installed), and\nStore the wind speeds to a new vector called population.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you receive an error message when running the code cell below, then you may not have the dplyr package installed.\n\nIn the R console, run the command install.packages(\"dplyr\").\nThen rerun the code cell below.\n\n\n\n\nlibrary(dplyr)\npopulation &lt;- storms$wind"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#picking-a-random-sample-of-storms",
    "href": "19-Parametric-CI-Means.html#picking-a-random-sample-of-storms",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Picking a Random Sample of Storms",
    "text": "Picking a Random Sample of Storms\n\nWhen performing statistical inference, we do not have complete data from the entire population. We typically have data from one sample picked randomly from the population. Based on the sample data, we try to estimate unknown parameters for the population. To begin our exploration of confidence intervals, suppose the 19066 wind speeds in the population data represents the full population of all storms at all times in the North Atlantic. We investigate the following statistical question:\n\nWhat is the average wind speed of all storms in the North Atlantic, \\(\\mu_{\\rm{wind}}\\)?\n\n\nAssume we do not have access to all the population data and cannot directly calculate \\(\\mu_{\\rm{wind}}\\).\nWe will pick one random sample of 36 wind speeds from the population.\nWe will use our sample to estimate the mean wind speed of all storms in the North Atlantic."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-1",
    "href": "19-Parametric-CI-Means.html#question-1",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 1",
    "text": "Question 1\n\nPick a random sample of \\(n=36\\) wind speeds from the population. In picking your original sample, sample without replacement. Store your sample to a vector named my.sample.\n\nSolution to Question 1\n\n\n# pick one random sample of 36 wind speeds without replacement\nmy.sample &lt;- sample(??, size = ??, replace = ??)"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-2",
    "href": "19-Parametric-CI-Means.html#question-2",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 2",
    "text": "Question 2\n\nUsing the random sample my.sample, give a possible point estimate for \\(\\mu_{\\rm{wind}}\\), the mean wind speed of all storms in the North Atlantic. The parameter \\(\\mu_{\\rm{wind}}\\) is unknown since we assume the full population data is not available.\n\nSolution to Question 2\n\n\n# use your sample to give a point estimate"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-3",
    "href": "19-Parametric-CI-Means.html#question-3",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 3",
    "text": "Question 3\n\nCheck the accuracy of the empirical rule for normal distributions.\n\nQuestion 3a\n\nUsing the R code cell below, check the accuracy of the approximation that 68% of the data is within \\(\\pm 1\\) standard deviation from the mean.\n\n\n\n\n\n\nTip\n\n\n\nUsing the standard normal distribution \\(Z \\sim N(0,1)\\), compute \\(P(-1 &lt; Z &lt; 1)\\).\n\n\n\nSolution to Question 3a\n\n\n# how much data is within +/- 1 standard deviation?\n\n\n\n\n\n\nQuestion 3b\n\nUsing the R code cell below, check the accuracy of the approximation that 95% of the data is within \\(\\pm 2\\) standard deviation from the mean.\n\nSolution to Question 3b\n\n\n# how much data is within +/- 2 standard deviations?\n\n\n\n\n\n\nQuestion 3c\n\nUsing the R code cell below, check the accuracy of the approximation that \\(99.7\\)% of the data is within \\(\\pm 3\\) standard deviation from the mean.\n\nSolution to Question 3c\n\n\n# how much data is within +/- 3 standard deviations?\n\n\n\n\n\n\nQuestion 3d\n\nLet’s improve the approximation that 95% of the data is withing \\(\\pm 2\\) standard deviations from the mean. Find a more accurate approximation for \\(z^*\\) such that \\(P( -z^* &lt; Z &lt; z^* ) = 0.95\\).\n\n\n\n\n\n\nTip\n\n\n\nSketch a picture to help determine how much area is in the left tail below \\(-z^*\\) if 95% of the area is between \\(-z^*\\) and \\(z^*\\). Then use the qnorm() command.\n\n\n\nSolution to Question 3d\n\n\n# find more accurate cutoffs for the middle 95%\nqnorm(??, 0, 1)"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-4",
    "href": "19-Parametric-CI-Means.html#question-4",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 4",
    "text": "Question 4\n\nLet \\(X\\) denote the wind speed of a randomly selected North Atlantic storm. We use \\(\\mu\\) and \\(\\sigma\\) to denote the mean and standard deviation, respectively, of the wind speed of the population of all storms in the North Atlantic. Answer the questions below to investigate a different approach to constructing an interval estimate for \\(\\mu\\).\n\nQuestion 4a\n\nLet \\(\\overline{X}\\) denote the distribution of sample means for samples size \\(n\\). Recall we denote the mean and standard error of the sampling distribution as \\(\\mu_{\\overline{X}}\\) and \\(\\sigma_{\\overline{X}}\\), respectively.\nJustify each of the steps below.\n\\[\\begin{aligned}\n0.95 &= P \\left( -1.96 &lt; Z &lt; 1.96 \\right) & \\mbox{from Question 3d}\\\\\n0.95   &= P \\left( -1.96 &lt; \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} &lt; 1.96 \\right) & \\mbox{Explanation 1}\\\\\n0.95   &=  P \\left( -1.96 &lt; \\frac{\\overline{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} &lt; 1.96 \\right) & \\mbox{Explanation 2}\\\\\n0.95   &= P \\left( -1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt;  1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right) & \\mbox{Explanation 3} \\\\\n0.95   &= P \\left( - \\overline{X} -1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt;  - \\overline{X} + 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right) & \\mbox{Explanation 4} \\\\\n0.95   &= P \\left( \\overline{X} - 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X}+1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right) & \\mbox{Explanation 5}\\\\\n\\end{aligned}\\]\n\nSolution to Question 4a\n\nExplanation 1:\n\nExplanation 2:\n\nExplanation 3:\n\nExplanation 4:\n\nExplanation 5:\n\n\n\n\n\nQuestion 4b\n\nLet \\(\\mu_{\\rm{wind}}\\) denote the mean wind speed of all North Atlantic storms which we assume is unknown. How can we use the result that \\(P \\left( \\overline{X} - 1.96\\cdot \\sigma_{\\overline{X}} &lt; \\mu_{\\rm{wind}} &lt; \\overline{X}+1.96 \\cdot \\sigma_{\\overline{X}} \\right)=0.95\\) to help construct a confidence interval for \\(\\mu_{\\rm{wind}}\\)?\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nYou will use the original random sample of \\(n=36\\) wind speeds stored in my.sample in Question 1 to construct an interval estimate using the result from Question 4b. If you have not already loaded the population data and created a sample size \\(n=36\\) stored in my.sample, be sure to answer Question 1 before continuing. Create a histogram to display the wind speeds in my.sample.\n\nSolution to Question 4c\n\n\n# Enter code to create a histogram to display your sample\n\n\n\n\n\n\nQuestion 4d\n\nAre the assumptions for the CLT for means satisfied by the data in my.sample? Explain why or why not.\n\nSolution to Question 4d\n\n\n\n\n\n\n\nQuestion 4e\n\nWe assume the population data is unknown and we do not know the actual value of the parameter \\(\\mu_{\\rm{wind}}\\). However, suppose we do know the population variance \\(\\mbox{Var}(X) = \\sigma^2_{\\rm{wind}} = 650\\) square knots. Using the CLT for means, what is the value of \\(\\sigma_{\\overline{X}}\\), the standard error of the sampling distribution for \\(\\overline{X}\\)? Show your work below either doing calculations by hand or in R. Tip: In R, the square root function is sqrt().\n\nSolution to Question 4e\n\n\n# compute standard error using CLT\n\n\n\n\n\n\n\nQuestion 4f\n\nGive an interval of values \\(?? &lt; \\mu_{\\rm{wind}} &lt; ??\\) that has a 95% chance of containing the actual value of \\(\\mu_{\\rm{wind}}\\). Include supporting work/code to explain how you determined your answer.\n\nSolution to Question 4f\n\n\n# compute upper and lower cutoffs for interval estimate"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-5",
    "href": "19-Parametric-CI-Means.html#question-5",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 5",
    "text": "Question 5\n\nA another climatologist collects their own random sample of \\(n=36\\) wind speeds of North Atlantic storms.\n\nQuestion 5a\n\nPick their random sample (without replacement) of \\(n=36\\) wind speeds from the population. Store the other sample to a vector named their.sample.\n\nSolution to Question 5a\n\n\n# pick another random sample of 36 wind speeds without replacement\ntheir.sample(??, size = ??, replace = ??)\n\n\n\n\n\n\nQuestion 5b\n\nUsing the random sample their.sample, give a possible point estimate for \\(\\mu_{\\rm{wind}}\\), the mean wind speed of all storms in population.\n\nSolution to Question 5b\n\n\n# use their sample to give another point estimate\n\n\n\n\n\n\nQuestion 5c\n\nAre the two point estimates in Question 2 and Question 5b equal?\n\nSolution to Question 5c\n\n\n\n\n\n\n\nQuestion 5d\n\nAre the true values of the population parameter different or the same for both climatologists?\n\nSolution to Question 5d\n\n\n\n\n\n\n\nQuestion 5e\n\nWithout checking, which point estimate (Question 2 or Question 5b) do you believe is “better”?\n\nSolution to Question 5e"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-6",
    "href": "19-Parametric-CI-Means.html#question-6",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 6",
    "text": "Question 6\n\nA researcher collects a random sample size \\(n=36\\) of wind speeds for storms in the North Atlantic and calculates a 95% confidence interval for the mean wind speed that is from \\(48.75\\) knots to \\(65.41\\) knots. For each statement, determine whether the interpretation is correct or not. If not, explain why not.\n\nQuestion 6a\n\nThere is a 95% chance that a randomly selected storm in the North Atlantic has a wind speed between \\(48.75\\) and \\(65.41\\) knots.\n\nSolution to Question 6a\n\n\n\n\n\n\n\nQuestion 6b\n\nThere is a 95% chance that the mean wind speed of all storms in the North Atlantic is between \\(48.75\\) and \\(65.41\\) knots.\n\nSolution to Question 6b\n\n\n\n\n\n\n\nQuestion 6c\n\nThere is a 95% chance the interval from \\(48.75\\) to \\(65.41\\) knots contains the mean wind speed of all storms in the North Atlantic.\n\nSolution to Question 6c\n\n\n\n\n\n\n\nQuestion 6d\n\n95% of all random samples of size \\(n=36\\) North Atlantic storms have a sample mean wind speed between \\(48.75\\) and \\(65.41\\) knots.\n\nSolution to Question 6d\n\n\n\n\n\n\n\nQuestion 6e\n\nThere is a 95% chance the interval from \\(48.75\\) to \\(65.41\\) knots contains the mean wind speed of a random sample of \\(n=36\\) storms.\n\nSolution to Question 6e"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#attaching-uncertainty-to-the-intervals",
    "href": "19-Parametric-CI-Means.html#attaching-uncertainty-to-the-intervals",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Attaching Uncertainty to the Intervals",
    "text": "Attaching Uncertainty to the Intervals\n\nThe plot shows 100 different confidence intervals based on 100 different random samples each size \\(n=36\\) storms selected from the population. For each sample mean \\(\\overline{X}\\), we follow the same process of constructing a 95% confidence interval using:\n\\[\\overline{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu_{\\rm{wind}} &lt;  \\overline{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}.\\]\n\nEach sample results is a different interval estimate (all with the same width).\nThe true population mean is marked with a blue vertical line at \\(\\mu_{\\rm{wind}} = 50.02\\) knots.\nThe goal of each interval is to contain the same, fixed value of the population parameter.\n\nThe confidence intervals that successfully contain \\(\\mu_{\\rm{wind}} = 50.02\\) are marked in green.\nThe unsuccessful confidence intervals that do not contain \\(\\mu_{\\rm{wind}} = 50.02\\) are marked in red.\n\n2 out of the 100 interval estimates are underestimates.\n2 out of the 100 interval estimates are overestimates.\n\nWe have a success rate of 96 out of 100 in this simulation.\n\nIf we ran this simulation many more times, the success rate would converge to 95%.\n\n\n\n\n\n\nFigure 14.1: Results of 100 randomly generated 95% confidence intervals."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-7",
    "href": "19-Parametric-CI-Means.html#question-7",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 7",
    "text": "Question 7\n\nWhat are some warnings to keep in mind when interpreting confidence intervals?\n\nSolution to Question 7\n\nWarning 1:\n\nWarning 2:\n\nWarning 3:"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-8",
    "href": "19-Parametric-CI-Means.html#question-8",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 8",
    "text": "Question 8\n\nYou will use the original random sample of \\(n=36\\) wind speeds stored in my.sample in Question 1 to construct a new confidence interval estimate for the mean wind speed of all North Atlantic storms. If you have not already loaded the population data and created a sample size \\(n=36\\) stored in my.sample, be sure to answer Question 1 before continuing.\n\nQuestion 8a\n\nBased on your sample data in my.sample, give a 90% confidence interval for the mean wind speed of all North Atlantic storms using the parametric method. As with earlier, suppose we know population variance is \\(\\sigma^2_X = 650\\). Interpret the practical meaning of your 90% confidence interval in the context of this example.\n\nSolution to Question 8a\n\n\nnew.z &lt;- ??  # Enter a command to calculate z value for 90% Conf Level\nwind.lowe90 &lt;- ??   # Enter a command to calculate the lower limit\nwind.upper90 &lt;- ??   # Enter a command to calculate the upper limit\n\n# Print your answers\nwind.lower90\nwind.upper90\n\n\n\nPractical interpretation of your 90% confidence interval:\n\n\n\n\n\n\nQuestion 8b\n\nAs we decrease the confidence level of the interval from 95% in Question 4f to 90% in Question 8a, what happened to the width of the interval estimate?\n\nSolution to Question 8b"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#the-t-distribution",
    "href": "19-Parametric-CI-Means.html#the-t-distribution",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "The \\(t\\)-Distribution",
    "text": "The \\(t\\)-Distribution\n\nWhen the population is known to be normally distributed, the distribution \\(W\\) is a \\(t\\)-distribution with \\(n-1\\) degrees of freedom, where \\(n\\) denotes the sample size. If the underlying population is known to be symmetric (but not necessarily normal), a \\(t\\)-distribution with \\(n-1\\) degrees of freedom is still a good estimate.\n\nThe larger our sample size \\(n\\), the better (less biased) the estimate \\(s\\) is for \\(\\sigma\\).\nThe larger the sample size \\(n\\), the closer the \\(t\\)-distribution is to \\(Z \\sim N(0,1)\\).\n\nIf a random sample of data is skewed, then the population is likely skewed. The more skewed the population is, the less accurate a \\(t\\)-distribution is to estimate the distribution of the standardized statistic \\(W\\).\n\n\n\n\n\n\nWarning\n\n\n\nIn cases where the sample is small (\\(n &lt; 30\\)) and skewed, a bootstrap confidence interval is typically preferred to using a \\(t\\)-distribution to approximate the margin of error."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#using-a-t-distribution-when-variance-is-unknown",
    "href": "19-Parametric-CI-Means.html#using-a-t-distribution-when-variance-is-unknown",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Using a \\(t\\)-Distribution When Variance is Unknown",
    "text": "Using a \\(t\\)-Distribution When Variance is Unknown\n\nFor a sample of size \\(n\\) randomly picked from a normal distribution with unknown \\(\\mu\\) and unknown \\(\\mbox{Var}(X)=\\sigma^2\\), we construct a confidence interval for \\(\\mu\\) using:\n\nThe sample standard deviation \\(s\\) in place of \\(\\sigma\\), and\n\nA \\(t\\)-distribution to find \\(t_{\\alpha/2}\\) instead of using \\(N(0,1)\\) to find \\(z_{\\alpha/2}\\)\n\nA corresponding \\(t\\)-distribution confidence interval is given by\n\\[{\\large \\boxed{ \\overline{X} - {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}}}},\\]\nwhere the area under the \\(t\\)-distribution with \\(n-1\\) degrees of freedom between \\({\\color{dodgerblue}{\\pm t_{\\alpha/2}}}\\) is equal to the confidence level."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-9",
    "href": "19-Parametric-CI-Means.html#question-9",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 9",
    "text": "Question 9\n\n\n\n\nCredit: Charles J. Sharp, CC BY-SA 4.0, via Wikimedia Commons\n\n\nResearchers want to estimate the average length of all adult female Komodo dragons. The pick a random sample of \\(n=8\\) adult female Komodo dragons with the following weights (in pounds). They believe the distribution in weights will by normally distributed, but otherwise, the population variance is unknown.\n\ndragon.wt &lt;- c(145, 178, 142, 139, 160, 190, 168, 122)  # load sample of weights  \ndragon.wt  # print to screen\n\n[1] 145 178 142 139 160 190 168 122\n\n\n\nQuestion 9a\n\nUse the R code cell below to construct a 95% confidence interval for mean weight of all female Komodo dragons using the sample weights in dragon.wt.\n\n\n\n\n\n\nTip\n\n\n\nUsing the data in dragon.wt, calculate the statistics \\(\\bar{x}\\) and \\(s\\). To find \\(t_{\\alpha/2}\\), use the qt() function similar to how you use the qnorm() to calculate \\(z_{\\alpha/2}\\). For more help using the qt() function, run ?qt().\n\n\n\nSolution to Question 9a\n\n\n# construct a 95% confidence interval\n\n\n\n\n\n\nQuestion 9b\n\nInterpret the practical meaning of the confidence interval in Question 9a in this context.\n\nSolution to Question 9b\n\n\n \n\n\n\nQuestion 9c\n\nUse the R code cell below to construct a 99% confidence interval for mean weight of all female Komodo dragons using the sample weights in dragon.wt.\n\nSolution to Question 9c\n\n\n# construct a 99% confidence interval"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#different-methods-for-t-distribution-confidence-intervals",
    "href": "19-Parametric-CI-Means.html#different-methods-for-t-distribution-confidence-intervals",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Different Methods for \\(t\\)-Distribution Confidence Intervals",
    "text": "Different Methods for \\(t\\)-Distribution Confidence Intervals\n\n\nUse the formula \\(\\displaystyle \\overline{X} - {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + {\\color{dodgerblue}{t_{\\alpha/2}}} \\cdot \\frac{{\\color{tomato}{s}}}{\\sqrt{n}}\\).\n\nUse the qt() function in R to find the value of \\(t_{\\alpha/2}\\), or\nUse a \\(t\\)-distribution table to estimate values for \\(t_{\\alpha/2}\\).\n\nUse the R function t.test(x, conf.level)$conf.int.\n\nx is the vector of sample data.\nSet the confidence level with the conf.level option.\nOpen the help manual with ?t.test for further information."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-10",
    "href": "19-Parametric-CI-Means.html#question-10",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 10",
    "text": "Question 10\n\nUsing the Komodo dragon sample data in dragon.wt and the t.test() function, find a 95% confidence interval. Compare the results with the 95% confidence interval from Question 9a.\n\nSolution to Question 10\n\n\nt.test(??, conf.level = ??)$conf.int"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#sec-clt-diff-means",
    "href": "19-Parametric-CI-Means.html#sec-clt-diff-means",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 11",
    "text": "Question 11\n\nLet \\(X\\) and \\(Y\\) be independent random variables with \\(X \\sim N(\\mu_1, \\sigma_1)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2)\\). Using properties of expected value and variance, show for sample sizes \\(n_1\\) and \\(n_2\\), respectively, that\n\\[E(\\overline{X}-\\overline{Y}) = \\mu_1-\\mu_2 \\qquad \\mbox{and} \\qquad \\mbox{Var}(\\overline{X}-\\overline{Y}) = \\frac{\\sigma_1^2}{n_1} +  \\frac{\\sigma_2^2}{n_2}.\\]\n\nSolution to Question 11"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#sec-informal",
    "href": "19-Parametric-CI-Means.html#sec-informal",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Confidence Intervals for a Difference in Means",
    "text": "Confidence Intervals for a Difference in Means\n\nLet \\(X_1, X_2, \\ldots , X_{n_1}\\) be independent and identically distributed (i.i.d.) random variables picked from \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and let \\(Y_1, Y_2, \\ldots , Y_{n_2}\\) be i.i.d. random variables picked from \\(Y \\sim N(\\mu_2, \\sigma_1^2)\\). An approximate confidence interval for the difference in means \\(\\mu_1 - \\mu_2\\) is given by:\n\\[{\\large \\boxed{ {\\color{mediumseagreen}{(\\overline{X} - \\overline{Y})}} -  {\\color{tomato}{t_{\\alpha/2}}} \\cdot  {\\color{dodgerblue}{\\sqrt{\\frac{s_1^2}{n_1} +  \\frac{s_2^2}{n_2}}}} &lt; \\mu_1 - \\mu_2 &lt; {\\color{mediumseagreen}{(\\overline{X} - \\overline{Y})}} +  {\\color{tomato}{t_{\\alpha/2}}} \\cdot  {\\color{dodgerblue}{\\sqrt{\\frac{s_1^2}{n_1} +  \\frac{s_2^2}{n_2}}}}}}. \\]\n\nUse \\(\\bar{x} - \\bar{y}\\) as the point estimate for \\(\\mu_1 - \\mu_2\\).\nApproximate the standard error using \\(s_1\\) and \\(s_2\\) in place of unknown standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\).\nThe area under the \\(t\\)-distribution with \\(df\\) degrees of freedom between \\(\\pm t_{\\alpha/2}\\) is equal to the confidence level.\n\nInformally, we can use the smaller of \\(n_1-1\\) and \\(n_2-1\\) as the degrees of freedom.\nMany R functions use the more accurate Welch’s approximation for the degrees of freedom,\n\n\\[ df = \\dfrac{\\left( \\dfrac{s_1^2}{n_1}+ \\dfrac{s_2^2}{n_2} \\right)^2}{ \\dfrac{(s_1^2/n_1)^2}{n_1-1} + \\dfrac{(s_2^2/n_2)^2}{n_2-1}}.\\]\n\nWe do not need to memorize Welch’s approximation since t.test() uses this method."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-12",
    "href": "19-Parametric-CI-Means.html#question-12",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 12",
    "text": "Question 12\n\nBased on the plots in the figure above, in which month do you suspect storms have the greatest wind speed? Explain how you determined your answer.\n\nSolution to Question 12"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#refining-our-question",
    "href": "19-Parametric-CI-Means.html#refining-our-question",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Refining Our Question",
    "text": "Refining Our Question\n\nThere are different statistics for assessing “greatest wind speed”. From the box plots, we have a good visual comparison of medians. With storms, outliers are typically the most important observations we do want to emphasize in our analysis. Thus, we use the mean as our measurement of center instead of a median or trimmed mean.\nSide-by-side box plots give a nice graphical summary of the distribution of winds speeds by month. As we might suspect, from this first plot we rule out some months from this analysis. Some months appear to have very similar distributions upon inspection of box plots. We dig a little into the sample data to determine which months have the greatest sample mean wind speeds.\n\nWe use the tapply() function below to calculate and compare sample mean wind speeds by month.\nHere is a nice introduction to the tapply() function.\n\n\ntapply(storms$wind, storms$month, mean)\n\n       1        4        5        6        7        8        9       10 \n49.85714 38.86364 35.17413 35.32092 41.39426 48.14414 54.56013 51.58109 \n      11       12 \n49.42741 45.54245 \n\n\nNotice September (9) and October (10) are the two months with the highest mean wind speeds of \\(54.56\\) knots and \\(51.58\\) knots, respectively. The two means are close, and we only have a subset of population data. Is the observed difference in sample means greater than the margin of error we can expect due to the uncertainty of sampling? We can use a confidence interval to answer this question!\n\nGive a 95% confidence interval to estimate the difference in the mean wind speed of storms in September compared to October?"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#subsetting-into-two-independent-samples",
    "href": "19-Parametric-CI-Means.html#subsetting-into-two-independent-samples",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Subsetting into Two Independent Samples",
    "text": "Subsetting into Two Independent Samples\n\nRun the code cell below to create two data frames, sep.wind and oct.wind, containing separate samples of wind speeds for North Atlantic storms in September and October, respectively. Both sep.wind and oct.wind are data frames with just one variable, wind.\n\n# create a vector of sep wind\nsep.wind &lt;- subset(storms, \n              month == \"9\", \n              select = wind)\n\n# create a vector of oct wind\noct.wind &lt;- subset(storms, \n              month == \"10\", \n              select = wind)"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-13",
    "href": "19-Parametric-CI-Means.html#question-13",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 13",
    "text": "Question 13\n\nAnswer the questions below to construct a 95% confidence interval for the difference in mean wind speeds of North Atlantic storms in September compared to October.\n\nQuestion 13a\n\nCalculate \\(n_s\\) and \\(n_o\\), the number of observations in September and October samples, respectively. Store the results in n.s and n.o.\n\nSolution to Question 13a\n\n\n# calculate n_s\nn.s &lt;- ??\n# calculate n_o\nn.o &lt;- ??\n\n# print to screen\nn.s\nn.o\n\n\n\n\n\n\nQuestion 13b\n\nUsing the sample data in sep.wind and oct.wind, give a point estimate for the difference in population means, \\(\\mu_s - \\mu_o\\). Store the result to point.est\n\nSolution to Question 13b\n\n\n# calculate point estimate\npoint.est &lt;- ??\npoint.est  # print to screen\n\n\n\n\n\n\nQuestion 13c\n\nUsing a \\(t\\)-distribution, calculate the margin of error of a 95% confidence interval for the difference in means.\n\n\n\n\n\n\nTip\n\n\n\nUse the sd() function to calculate each sample standard deviation. Use the qt() function to identify \\(t_{\\alpha/2}\\) using the informal count for the degrees of freedom.\n\n\n\nSolution to Question 13c\n\n\n# use code cell to construct a 95% confidence interval\n\n\nBased on the output above, a 95% confidence interval is from ?? to ??.\n\n\n\n\n\nQuestion 13d\n\nInterpret the meaning of your interval in Question 13c. In particular, can we conclude that wind speeds are on average greater in one month or the other?\n\nSolution to Question 13d"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-14",
    "href": "19-Parametric-CI-Means.html#question-14",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 14",
    "text": "Question 14\n\nUse the t.test() function with the samples sep.wind and oct.wind to construct a 95% confidence interval for the difference in mean wind speeds of North Atlantic storms in September compared to October. How does your answer compare to your confidence interval in Question 13c? If the two answers are different, which confidence interval do you believe is more accurate?\n\nSolution to Question 14\n\nComplete the command below and then compare result to answer in Question 13c.\n\nt.test(??, ??, conf.level = ??)$conf.int"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#subsetting-with-t.test",
    "href": "19-Parametric-CI-Means.html#subsetting-with-t.test",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Subsetting with t.test",
    "text": "Subsetting with t.test\n\nFrequently, we would like to compare the distributions of a quantitative variable, denoted quant, for two different groups based on a categorical variable in the data set, denoted categorical. If our sample data is stored in a data frame called data.name with this structure, we can use t.test() without having to first split the sample into independent samples according to category group. t.test() can subset the data into independent samples for us and construct a confidence interval for the difference in two means. Using the t.test() function\n\nt.test(quant ~ categorical, data = data.name, conf.level = 0.95)$conf.int\n\n will give a 95% confidence level for the difference in means of a specified quantitative variable between the two different groups of the categorical variable."
  },
  {
    "objectID": "19-Parametric-CI-Means.html#question-15",
    "href": "19-Parametric-CI-Means.html#question-15",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Question 15",
    "text": "Question 15\n\nThe code cell below subsets the sample data for all months in storms to a new data frame named pooled that:\n\nIncludes only storm observations from September or October.\nSelects only two variables, wind and month, to keep from storms.\nThe first six rows of the data frame pooled are printed to the screen with head(pooled).\nRun the code cell below and inspect the first six rows of pooled.\n\n\npooled &lt;- subset(storms,\n               month == \"9\" | month == \"10\",   # month is 9 OR month is 10\n               select = c(wind, month))  # select wind and month variables\n\nhead(pooled)  # print first 6 rows of pooled to screen\n\n# A tibble: 6 × 2\n   wind month\n  &lt;int&gt; &lt;fct&gt;\n1    30 9    \n2    20 9    \n3    20 9    \n4    75 9    \n5    75 9    \n6    75 9    \n\n\nUse the t.test() function with the pooled sample pooled to construct a 95% confidence interval for the difference in mean wind speeds of North Atlantic storms in September compared to October. How does your answer compare to your confidence intervals in Question 13c and Question 14? If the intervals are different, which confidence interval do you believe is most accurate?\n\nSolution to Question 15\n\nComplete the command below and then compare the result to your answers in Question 13c and Question 14.\n\nt.test(?? ~ ??, data = ??, conf.level = ??)$conf.int"
  },
  {
    "objectID": "19-Parametric-CI-Means.html#caution-using-t.test-for-a-difference-in-two-means",
    "href": "19-Parametric-CI-Means.html#caution-using-t.test-for-a-difference-in-two-means",
    "title": "5.4: Parametric Confidence Intervals for Means",
    "section": "Caution Using t.test() for a Difference in Two Means",
    "text": "Caution Using t.test() for a Difference in Two Means\n\nThe variable month in the original sample storms has observations from 10 different months. If you try running the command t.test(wind ~ month, data = storms, conf.level = 0.95)$conf.int you will receive an error since the categorical variable month has more than two classes. The categorical variable used to split the data must have exactly 2 different classes. In solving Question 15 we avoided this error by:\n\nFirst creating the pooled sample that included only two months, September and October.\nThen using t.test() with the pooled sample instead of the full storms data set.\n\nOtherwise, we can split the original sample into two independent samples x and y and use the t.test() function as we did in Question 14 with independent wind speed samples sep.wind and oct.wind.\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html",
    "href": "20-Parametric-CI-Proportions.html",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "",
    "text": "Public Opinion Polls\nConfidence intervals are frequently used when polling public opinion. Rather than give a point estimate alone, poll results are typically given along with a margin of error corresponding to a specified confidence level, which is typically 95%. For example, summarized in the bar plot and table below are the results of a PBS NewsHour/NPR/Marist poll1 that surveyed \\({\\color{tomato}{n=1,\\!227}}\\) randomly selected adults in the US and gauged their opinions on how the US is handling the COVID pandemic approximately one year after the initial outbreak in the United States.\nA confidence interval is an interval estimate for a population parameter with a rate of success given by the confidence level of the interval. We can construct confidence intervals for all sorts of statistics, and we can use confidence intervals as a tool for analyzing possible associations between two different variables. In general, a confidence interval has three components:\nIf we want to construct a confidence interval estimate for parameter \\(\\theta\\), then we have\n\\[({\\color{dodgerblue}{\\mbox{point estimate}}}) - {\\color{tomato}{\\mbox{MoE}}} &lt; \\theta &lt; ({\\color{dodgerblue}{\\mbox{point estimate}}}) + {\\color{tomato}{\\mbox{MoE}}}.\\]\nAll confidence intervals have this same general structure that we can construct using similar steps:\nThe Wald confidence interval for a proportion is given by\n\\[{\\large \\boxed{ \\mbox{Wald:} \\qquad {\\color{tomato}{\\hat{p}}} - z_{\\alpha/2} \\cdot \\sqrt{ \\frac{{\\color{tomato}{\\hat{p}}}(1-{\\color{tomato}{\\hat{p}}})}{n}}  &lt; p &lt;  {\\color{tomato}{\\hat{p}}} + z_{\\alpha/2} \\cdot \\sqrt{ \\frac{{\\color{tomato}{\\hat{p}}}(1-{\\color{tomato}{\\hat{p}}})}{n}}}}\\]\nWe use the plug-in principle and use \\(\\hat{p}\\) for the unknown value of \\(p\\) when calculating the standard error.\nWhen constructing a Wald confidence interval for a proportion, we use the sample proportion \\(\\hat{p}\\) as an estimator for \\(p\\). The sample proportion is a very reasonable and intuitive point estimate. In addition, \\(\\hat{p}\\) is an unbiased estimator for \\(p\\). However, there are many other estimators that could make sense to use in place of the parameter \\(p\\), and there are other properties of estimators we should take into account. In particular, using a less precise estimator such as \\(\\hat{p}\\) results in a larger margin of error in the confidence interval compared to some other estimators.\nWhen studying properties of estimators, we considered another estimator for the parameter \\(p\\), namely \\({\\color{tomato}{\\tilde{p} = \\frac{X+2}{n+4}}}\\), where \\(X\\) denotes the number of “successes”. Comparing estimators \\(\\hat{p}\\) and \\(\\tilde{p}\\), we discovered that:\nThe Agresti-Coull confidence interval for a proportion is\n\\[{\\large \\boxed{ \\mbox{Agresti-Coull:} \\qquad {\\color{tomato}{\\tilde{p}}} - z_{\\alpha/2} \\left( \\sqrt{ \\frac{{\\color{tomato}{\\tilde{p}}}(1-{\\color{tomato}{\\tilde{p}}})}{{\\color{dodgerblue}{n+4}}}} \\right) &lt; p &lt;  {\\color{tomato}{\\tilde{p}}} + z_{\\alpha/2} \\left( \\sqrt{ \\frac{{\\color{tomato}{\\tilde{p}}}(1-{\\color{tomato}{\\tilde{p}}})}{{\\color{dodgerblue}{n+4}}}} \\right)}}.\\]\nFrom the CLT for proportions, we have \\(\\widehat{P} \\sim N \\left( \\mu_{\\widehat{P}} , \\sigma_{\\widehat{P}} \\right) = N\\left( p, \\sqrt{\\frac{ p (1-p)}{n}} \\right)\\). A standardized sample proportion has \\(z\\)-score \\(z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{ p (1-p)}{n}}}\\). Thus, for confidence level \\(CL\\), we have\n\\[P \\left( -z_{\\alpha/2} &lt; \\frac{\\hat{p} -\\color{tomato}{p}}{\\sqrt{\\frac{\\color{tomato}{p}(1-\\color{tomato}{p})}{n}}} &lt; z_{\\alpha/2} \\right) =CL.\\]\nIn the equation above, the unknown population parameter \\(p\\) is in red. All the other values (\\(\\hat{p}\\), \\(n\\), and \\(z_{\\alpha/2}\\)) in the formula are known values. Given a confidence level, we can algebraically solve for the cutoff values for \\({\\color{tomato}{p}}\\) by solving the equations:\n\\[\\dfrac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} = z_{\\alpha/2} \\qquad \\mbox{and} \\qquad \\dfrac{\\hat{p} - {\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p})}}{n}}} = -z_{\\alpha/2}\\]"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-1",
    "href": "20-Parametric-CI-Proportions.html#question-1",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 1",
    "text": "Question 1\n\nBased on the poll summaries above, approximately what proportion of ALL adults in the US do NOT plan to get vaccinated?\n\nSolution to Question 1"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-2",
    "href": "20-Parametric-CI-Proportions.html#question-2",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 2",
    "text": "Question 2\n\nWe would like to estimate the parameter \\(p\\), the proportion of all adults in the US that do not plan on getting vaccinated. From the vaccination poll in Question 1, we have one random sample of 1,227 adults. From our sample, we observe that 30% said they do not intend to get vaccinated. Let’s apply the same general process summarized above to construct a 95% confidence interval to estimate the proportion of all adults in the US that do not plan on getting vaccinated.\n\nQuestion 2a\n\nBased on the sample of polled adults, what is a reasonable point estimate for \\(p\\), the proportion of all adults in the US that do not plan on getting vaccinated?\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nUsing the Central Limit Theorem for proportions, estimate the standard error for the sampling distribution of sample proportions.\n\n\n\n\n\n\nTip\n\n\n\nTo calculate the standard error, we need to know the population proportion \\(p\\). Plug an appropriate sample statistic in place of \\(p\\) to estimate the standard error.\n\n\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nNext, we identify the value (\\(z_{\\alpha/2}\\) or \\(t_{\\alpha/2}\\)) we multiply the standard error by to get the margin of error. For proportions, as long as the the sample is large enough, a normal distribution is an accurate model for the underlying sampling distribution.\nFrom the poll in Question 1, we have \\(n=1227\\). Since we do not know \\(p\\), we substitute \\(\\hat{p} = 0.3\\) instead. Since both \\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\), we can use a normal distribution to calculate the margin of error for the confidence interval.\nWhat is the value of \\(z_{\\alpha/2}\\) for a 95% confidence interval for a proportion?\n\nSolution to Question 2c\n\n\n\n\n\n\n\nQuestion 2d\n\nBased on your previous answers in Question 2, give a 95% confidence interval to estimate \\(p\\), the proportion of all adults in the US that do not plan on getting vaccinated.\n\nSolution to Question 2d\n\n\n\n\n\n\n\nQuestion 2e\n\nInterpret the practical meaning of your confidence interval in Question 2d in the context of COVID vaccinations in the US.\n\nSolution to Question 2e"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-3",
    "href": "20-Parametric-CI-Proportions.html#question-3",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 3",
    "text": "Question 3\n\nUsing the same poll from Question 1, find a 95% confidence interval for the proportion of all adults in the US that do not plan to get vaccinated using the Agresti-Coull confidence interval for a proportion.\n\nSolution to Question 3\n\n\n# use code to help with the calculations"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#score-confidence-interval-formulas",
    "href": "20-Parametric-CI-Proportions.html#score-confidence-interval-formulas",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Score Confidence Interval Formulas",
    "text": "Score Confidence Interval Formulas\n\nThe confidence interval estimate resulting from the algebraic solution is called the score confidence interval for a proportion. The algebraic work involved in solving the equations above is provided in the Appendix. The corresponding lower (\\(L\\)) and upper (\\(U\\)) cutoffs are:\n\\[\\begin{aligned}\n&L= \\dfrac{\\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}} \\\\\n\\\\\n&U= \\dfrac{\\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} + z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}}\n\\end{aligned}\\]\n\nPro: There is no additional uncertainty beyond the initial variability in sampling.\nCon: The formulas are quite complicated. Calculating by hand is not really practical.\nTypically we use technology to calculate score confidence intervals."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#score-confidence-intervals-in-r",
    "href": "20-Parametric-CI-Proportions.html#score-confidence-intervals-in-r",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Score Confidence Intervals in R",
    "text": "Score Confidence Intervals in R\n\nR has a built in function prop.test()$conf.int that calculates a score confidence interval for a proportion.\n\nIn R, use the command prop.test(X, n, conf.level = CL, correct = FALSE)$conf.int\n\n\\(X\\) denotes the number of “successes” observed in the sample.\n\\(n\\) denotes the total number of observations in the sample.\nCL is a chosen confidence level (as a proportion).\nThe option correct = FALSE means no continuity correction is applied."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-4",
    "href": "20-Parametric-CI-Proportions.html#question-4",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 4",
    "text": "Question 4\n\nUsing the poll data in Question 1, find a 95% score confidence interval for the proportion of all adults in the US that do not plan to get vaccinated by completing the prop.test() command in the code cell below.\n\nSolution to Question 4\n\n\nprop.test(??, ??, conf.level = ??, correct = FALSE)$conf.int\n\n\n\n\nChecking Your Solution to Question 4\n\nBased on the polling sample data, enter the values for X, n, and z.star in the first code cell below. Then run the code cell.\n\n##################################################\n# Replace the ?? in the three lines of code below\n# with appropriate values or commands\n##################################################\nX &lt;- ??  # number of successes in sample (do not plan to get vax)\nn &lt;- ??  # sample size\nz.star &lt;- ??  # find z_alpha/2 for 95% confidence level\n\nNext, run the code cell below to calculate the upper and lower cutoffs for a 95% score confidence interval for a proportion.\n\n#########################################\n# first run the code cell above\n# nothing to edit in this code cell\n# run as is\n#########################################\nphat &lt;- X/n  # Compute sample proportion\n\n# Computes Cutoffs for Score Confidence Interval\nlower.score95 &lt;- (phat+z.star^2/(2*n) - \n                z.star*sqrt( (phat*(1-phat))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\nupper.score95 &lt;- (phat+z.star^2/(2*n) + \n                z.star*sqrt( (phat*(1-phat))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\n\n# Print cutoffs to screen\nlower.score95\nupper.score95"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#applying-the-continuity-correction",
    "href": "20-Parametric-CI-Proportions.html#applying-the-continuity-correction",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Applying the Continuity Correction",
    "text": "Applying the Continuity Correction\n\nIn our construction of a score confidence interval, we have used a normal distribution to estimate a discrete (binomial) distribution. Recall when using a continuous, normal distribution to approximate a discrete, binomial distribution (as with the Central Limit Theorem for proportions), we miss some area under the curve resulting in an underestimate. We can improve estimates resulting from using a normal distribution instead of a binomial distribution by applying a continuity correction.\nSimilarly, we can obtain more a more accurate score confidence interval for a proportion by applying a continuity correction. The Appendix explains how the continuity correction is applied and provides the corresponding formulas. In practice, we can simply change the correct = FALSE option in prop.test()$conf.int to correct = TRUE.\n\nprop.test(X, n, conf.level = CL, correct = TRUE)$conf.int\nThe default for prop.test if no correct option is specified is correct = TRUE.\nApplying the continuity correction results in a more precise confidence interval.\n\n\nApplying the Continuity Correction in Code\n\nBelow we perform the direct calculations using the continuity correction formulas derived in the Appendix.\n\n##############################################\n# Be sure you have run previous code cells\n# And have already defined X, n, and z.star\n# Run this code cell without any edits needed\n##############################################\n\n# Continuity corrections applied to sample proportion\ncc.phat.L &lt;- (X - 0.5)/n\ncc.phat.U &lt;- (X + 0.5)/n\n\n# Plugged into formulas for Score Conf Interval\ncc.lower &lt;- (cc.phat.L + z.star^2/(2*n) - \n            z.star*sqrt( (cc.phat.L*(1-cc.phat.L))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\ncc.upper &lt;- (cc.phat.U + z.star^2/(2*n) + \n            z.star*sqrt( (cc.phat.U*(1-cc.phat.U))/n + z.star^2/(4*n^2) ) )/(1+z.star^2/n)\n\n# Print results to screen to check\ncc.lower\ncc.upper\n\nIn the code cell below, we apply the continuity correction using the correct = TRUE option in prop.test() to compare with the previous result.\n\nprop.test(368, 1227, conf.level = 0.95, correct = TRUE)$conf.int"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#central-limit-theorem-for-widehatp_1---widehatp_2",
    "href": "20-Parametric-CI-Proportions.html#central-limit-theorem-for-widehatp_1---widehatp_2",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Central Limit Theorem for \\(\\widehat{P}_1 - \\widehat{P}_2\\)",
    "text": "Central Limit Theorem for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nFor a difference in two proportions, we can derive a Central Limit Theorem to model the sampling distribution for the difference in two sample proportions, \\(\\widehat{P}_1 - \\widehat{P}_2\\). See the Appendix for a proof of the CLT for a difference in two proportions which is stated below:\n\\[\\widehat{P}_1 - \\widehat{P}_2  \\sim N \\left( \\mu_{\\widehat{P}_1 - \\widehat{P}_2} , \\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) \\right) = N \\left( p_1 - p_2  , \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\right).\\]"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#confidence-interval-for-widehatp_1---widehatp_2",
    "href": "20-Parametric-CI-Proportions.html#confidence-interval-for-widehatp_1---widehatp_2",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Confidence Interval for \\(\\widehat{P}_1 - \\widehat{P}_2\\)",
    "text": "Confidence Interval for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nWe can modify the Wald confidence interval to give an approximation for a confidence interval for a difference in two proportions\n\nThe point estimate is the difference in the two sample proportions, \\(\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}\\).\nThe standard error we estimate by plugging \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\) in place of \\(p_1\\) and \\(p_2\\) in the formula for the standard error from the Central Limit Theorem:\n\n\\[\\mbox{SE} \\left( \\widehat{P}_1 - \\widehat{P}_2 \\right) = \\sqrt{ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\approx \\sqrt{ \\frac{{\\color{mediumseagreen}{\\hat{p}_1}}(1-{\\color{mediumseagreen}{\\hat{p}_1}})}{n_1} + \\frac{{\\color{mediumseagreen}{\\hat{p}_2}}(1-{\\color{mediumseagreen}{\\hat{p}_2}})}{n_2}}\\]\n\nWe use the standard normal distribution to identify \\(z_{\\alpha/2}\\) to find the margin of error.\n\n\\[({\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}}) - {\\color{tomato}{z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\dfrac{\\hat{p}_2 (1-\\hat{p}_2) }{n_2}}}}  &lt; p_1-p_2 &lt; ({\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}}) + {\\color{tomato}{z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\dfrac{\\hat{p}_2 (1-\\hat{p}_2) }{n_2}}}}.\\]\nWe use Wald confidence intervals for a difference in two proportions, but be aware there are other variations of confidence intervals for a difference in two proportions, similar to the Agresti-Coull and score confidence intervals. In R, the command\n\nprop.test(c(x1, x2), c(n1, n2), conf.level = CL, correct = TRUE)$conf.int\n\n computes a Wald confidence interval for a difference in two proportions with a continuity correction applied.\n\n\n\n\n\n\nNote\n\n\n\nIn R, the prop.test() function uses different methods depending on whether the confidence interval is for a single proportion or a difference in two proportions.\n\nFor a single proportion, prop.test() gives a score confidence interval.\nFor a difference in two proportions, prop.test() gives a Wald confidence interval."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#question-5",
    "href": "20-Parametric-CI-Proportions.html#question-5",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Question 5",
    "text": "Question 5\n\nUsing the data below collected from the poll in Question 1, construct a 90% Wald confidence interval for the difference in the proportion of all Democrats and the proportion of all Republicans that do not plan to be vaccinated.\n\n\n\nParty\nYes, will\nYes, already\nNo\nUnsure\nTotal\n\n\n\n\nDemocrat\n213\n108\n40\n7\n368\n\n\nRepublican\n93\n70\n120\n9\n292\n\n\nTotal\n306\n178\n160\n16\n660\n\n\n\n\nSolution to Question 5\n\n\n# use code cell to help"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-samp-size",
    "href": "20-Parametric-CI-Proportions.html#sec-samp-size",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "A Note About Sample Sizes",
    "text": "A Note About Sample Sizes\n\n\nFor a single mean, we can use the CTL to construct a parametric confidence interval as long as:\n\nEither the population is symmetric or \\(n \\geq 30\\).\nIf the sample is symmetric, we can assume the population is symmetric.\n\nFor a difference in two means , we can use the CTL to construct a parametric confidence interval as long as:\n\nPopulation 1 is either symmetric or \\(n_1 \\geq 30\\), and\nPopulation 2 is either symmetric or \\(n_2 \\geq 30\\).\n\nFor a single proportion, we can use the CTL to construct a parametric confidence interval as long as:\n\nBoth \\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\).\n\nFor a difference in two proportions, we can use the CTL to construct a parametric confidence interval as long as:\n\nAll of \\(n_1\\hat{p}_1 \\geq 10\\), \\(n_1(1-\\hat{p}_1) \\geq 10\\), \\(n_2\\hat{p}_2 \\geq 10\\), and \\(n_2(1-\\hat{p}_2) \\geq 10\\) are satisfied."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#useful-r-functions",
    "href": "20-Parametric-CI-Proportions.html#useful-r-functions",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Useful R Functions",
    "text": "Useful R Functions\n\nIn R, we have the functions:\n\nt.test()$conf.int constructs a \\(t\\)-confidence interval for a single or difference in two means.\nprop.test()$conf.int constructs a score confidence interval for a single proportion.\nprop.test()$conf.int constructs a Wald confidence interval for a difference in two proportions."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-score",
    "href": "20-Parametric-CI-Proportions.html#sec-score",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Deriving the Score Confidence Interval Formulas",
    "text": "Deriving the Score Confidence Interval Formulas\n\nLet \\(X \\sim \\mbox{Binom}(n,p)\\) and consider the distribution of sample proportions, \\(\\widehat{P} = \\frac{X}{n}\\). From the CLT for proportions we know \\(\\widehat{P} \\sim N \\left( p, \\sqrt{\\frac{p(1-p)}{n}} \\right)\\). Thus, for confidence level CL, we have\n\\[P(-z_{\\alpha/2}&lt; Z &lt; z_{\\alpha/2}) = P \\left( -z_{\\alpha/2} &lt; \\frac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} &lt; z_{\\alpha/2} \\right) =CL.\\]\nThe upper cutoff, \\(U\\) is a value for \\({\\color{tomato}{p}}\\) such that\n\\[\\dfrac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} = z_{\\alpha/2}.\\]\nTo solve for \\({\\color{tomato}{p}}\\), we multiply both sides of the equation above by \\(\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}\\) and then square both sides giving\n\\[\\big( \\hat{p} - {\\color{tomato}{p}} \\big)^2 = (z_{\\alpha/2})^2 \\left( \\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n} \\right).\\]\nNext we distribute terms on both sides of the equation and have\n\\[\\hat{p}^2 - 2 {\\color{tomato}{p}}\\hat{p} + {\\color{tomato}{p}}^2 = {\\color{tomato}{p}} \\left( \\frac{z_{\\alpha/2}^2}{n} \\right) - {\\color{tomato}{p}}^2 \\left( \\frac{z_{\\alpha/2}^2}{n} \\right).\\]\nWe have a quadratic equation for the unknown \\({\\color{tomato}{p}}\\). We group all like terms together on one side of the equation,\n\\[{\\color{dodgerblue}{\\left( 1+ \\frac{z_{\\alpha/2}^2}{n} \\right)}} p^2 + {\\color{tomato}{\\left(-2\\hat{p}-\\frac{z_{\\alpha/2}^2}{n} \\right)}} p + {\\color{mediumseagreen}{\\hat{p}^2}} = {\\color{dodgerblue}{a}}p^2 + {\\color{tomato}{b}} p + {\\color{mediumseagreen}{c}} = 0.\\] We use the quadratic formula to solve for \\(p\\). The quadratic equation has two real solutions, the larger of the two solution is the upper limit for a 95% score confidence interval\n\\[{\\large \\boxed{ U = \\frac{ \\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} + z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+\\dfrac{z_{\\alpha/2}^2}{n}}}}\\]\nThe smaller of the two solutions is the lower cutoff, \\(L\\)\n\\[{\\large \\boxed{ L = \\frac{ \\hat{p} + \\dfrac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{\\hat{p}(1-\\hat{p})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+\\dfrac{z_{\\alpha/2}^2}{n}}}}\\]\nWe also consider the equation\n\\[\\dfrac{\\hat{p} -{\\color{tomato}{p}}}{\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}} = -z_{\\alpha/2}.\\]\nIf we multiply both sides of the equation above by \\(\\sqrt{\\dfrac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n}}\\) and then square both sides, we get\n\\[\\big( \\hat{p} - {\\color{tomato}{p}} \\big)^2 = (-z_{\\alpha/2})^2 \\left( \\frac{{\\color{tomato}{p}}(1-{\\color{tomato}{p}})}{n} \\right).\\]\nThe resulting equation is the same as with the first case we solved. Thus, solving the equation above gives the same expressions for \\(U\\) and \\(L\\)."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-score-corr",
    "href": "20-Parametric-CI-Proportions.html#sec-score-corr",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "Applying a Continuity Correction",
    "text": "Applying a Continuity Correction\n\nRecall, when using a normal distribution to approximate a discrete, binomial distribution \\(X \\sim \\mbox{Binom}(n,p)\\), we can improve the estimate by using a continuity correction.\nIn the case of a score confidence interval for a proportion, the continuity correction is applied as follows:\n\nIn the formula for the corrected lower cutoff \\(L^*\\), we use the corrected sample proportion \\({\\color{dodgerblue}{\\hat{p}_L^*= \\dfrac{X-0.5}{n}}}\\).\nIn the formula for the corrected upper cutoff \\(U^*\\), we use the corrected sample proportion \\({\\color{tomato}{\\hat{p}_U^* = \\dfrac{X+0.5}{n}}}\\).\n\n\\[\\begin{aligned}\n&L^* = \\dfrac{{\\color{dodgerblue}{\\hat{p}_L^*}} + \\dfrac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{{\\color{dodgerblue}{\\hat{p}_L^*}}(1-{\\color{dodgerblue}{\\hat{p}_L^*}})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}} \\\\\n\\\\\n&U^* = \\dfrac{{\\color{tomato}{\\hat{p}_U^*}} + \\dfrac{z_{\\alpha/2}^2}{2n} + z_{\\alpha/2} \\cdot \\sqrt{ \\dfrac{{\\color{tomato}{\\hat{p}_U^*}}(1-{\\color{tomato}{\\hat{p}_U^*}})}{n} + \\dfrac{z_{\\alpha/2}^2}{4n^2}}}{1+ \\dfrac{z_{\\alpha/2}^2}{n}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#sec-clt-diffprop",
    "href": "20-Parametric-CI-Proportions.html#sec-clt-diffprop",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "CLT: Difference in Two Proportions",
    "text": "CLT: Difference in Two Proportions\n\nLet \\(X_1 \\sim \\mbox{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mbox{Binom}(n_2,p_2)\\) be two independent binomial random variables with distribution of sample proportions \\(\\widehat{P}_1 = \\frac{X_1}{n_1}\\) and \\(\\widehat{P}_2 = \\frac{X_2}{n_2}\\), respectively. As long as both samples are large enough, the sampling distribution for the difference in sample proportions \\(\\widehat{P_1}-\\widehat{P_2}\\) will:\n\nBe approximately normally distributed.\nHave mean \\({\\color{dodgerblue}{E(\\widehat{P_1}-\\widehat{P_2}) = \\mu_{\\widehat{P}_1 - \\widehat{P}_2} =p_1 - p_2}}\\).\nHave standard error \\[{\\color{dodgerblue}{\\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) =  \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}}.\\]\n\nWe summarize the results of the Central Limit Theorem (CLT) for a Difference in Two Proportions more concisely below:\n\\[{\\color{dodgerblue}{\\boxed{ \\widehat{P}_1 - \\widehat{P}_2  \\sim N \\left( \\mu_{\\widehat{P}_1 - \\widehat{P}_2} , \\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) \\right) = N \\left( p_1 - p_2  , \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\right)}}}.\\]\n\nProof of CLT for Difference in Two Proportions\n\nBelow we prove both \\(E(\\widehat{P_1}-\\widehat{P_2}) = \\mu_{\\widehat{P}_1 - \\widehat{P}_2} =p_1 - p_2\\) and \\(\\mbox{SE}(\\widehat{P}_1 - \\widehat{P}_2) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\).\n\nMean of Sampling Distribution for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nWe have \\(E (\\widehat{P_1}-\\widehat{P_2}) = E \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right)\\). Using properties of expected value, we have\n\\[E (\\widehat{P_1}-\\widehat{P_2}) = E \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right) = \\frac{1}{n_1} E(X_1) - \\frac{1}{n_2} E(X_2).\\]\nSince \\(X_1 \\sim \\mbox{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mbox{Binom}(n_2,p_2)\\), we know \\({\\color{dodgerblue}{E(X_1) = n_1p_1}}\\) and \\({\\color{tomato}{E(X_2) = n_2p_2}}\\). Thus we have\n\\[E (\\widehat{P_1}-\\widehat{P_2}) = \\frac{1}{n_1} {\\color{dodgerblue}{n_1p_1}} - \\frac{1}{n_2} {\\color{tomato}{n_2p_2}}=p_1-p_2.\\]\nThis concludes the proof that \\(E (\\widehat{P_1}-\\widehat{P_2}) =p_1-p_2\\).\n\n\nStandard Error of Sampling Distribution for \\(\\widehat{P}_1 - \\widehat{P}_2\\)\n\nWe have \\(\\mbox{Var} (\\widehat{P_1}-\\widehat{P_2}) = \\mbox{Var} \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right)\\). \\(X_1\\) and \\(X_2\\) are independent random variables, so we can use properties of variance:\n\\[\\mbox{Var} \\left( \\frac{X_1}{n_1} -\\frac{X_2}{n_2} \\right) = \\left(\\frac{1}{n_1}\\right)^2 \\mbox{Var}(X_1) + \\left(- \\frac{1}{n_2} \\right)^2 \\mbox{Var}(X_2) = \\frac{1}{n_1^2}\\mbox{Var}(X_1) + \\frac{1}{n_2^2}\\mbox{Var}(X_2).\\]\nSince \\(X_1 \\sim \\mbox{Binom}(n_1,p_1)\\) and \\(X_2 \\sim \\mbox{Binom}(n_2,p_2)\\), we know \\({\\color{dodgerblue}{\\mbox{Var}(X_1) = n_1p_1(1-p_1)}}\\) and \\({\\color{tomato}{\\mbox{Var}(X_2) = n_2p_2(1-p_2)}}\\). Thus we have\n\\[\\mbox{Var} (\\widehat{P_1}-\\widehat{P_2}) = \\frac{1}{n_1^2} {\\color{dodgerblue}{n_1p_1(1-p_1)}} - \\frac{1}{n_2^2} {\\color{tomato}{n_2p_2(1-p_2)}}= \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}.\\]\nThe standard error of the sampling distribution is the square root of the variance, thus we have\n\\[\\mbox{SE} (\\widehat{P_1}-\\widehat{P_2}) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}.\\]\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "20-Parametric-CI-Proportions.html#footnotes",
    "href": "20-Parametric-CI-Proportions.html#footnotes",
    "title": "5.5: Parametric Confidence Intervals for Proportions",
    "section": "",
    "text": "“Politics still drives how Americans fell about COVID response, one year in”, PBS/News Hour, March 11, 2021↩︎"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html",
    "href": "21-Intro-Hypothesis-Tests.html",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "",
    "text": "An Experiment with Telepathy\nWe refer to the two competing hypotheses as the null hypothesis, denoted by \\(\\color{dodgerblue}{H_0}\\), and the alternative hypothesis, denoted by \\(\\color{tomato}{H_a}\\).\nThe general process form performing a hypothesis test is informally:\nWe will dive deeper into each step, but for now we take a tour through the process in the context of two different studies. Before considering our first example, we focus on setting up the hypotheses in Step 1.\nEarlier we informally discussed the concept of significance. Recall the logic of hypothesis tests, we assume \\(H_0\\) is true and consider whether the sample data supports or refutes the null claim. \\(p\\)-values are more formal statistics used to measure significance in hypothesis testing.\nThe \\(\\color{dodgerblue}{\\mathbf{p}}\\)-value is the probability that we would get a random sample with a test statistic as or more extreme as the observed test statistic if the null hypothesis were true.\n\\[{\\large {\\color{dodgerblue}{p\\mbox{-value} = P( \\mbox{test statistic as or more extreme than observed} \\ | \\ H_0 \\mbox{ is true} )}}}.\\]\nThe null distribution is the distribution of the test statistic if the null hypothesis is true. We use the null distribution to calculate the p-value."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-q1",
    "href": "21-Intro-Hypothesis-Tests.html#sec-q1",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 1",
    "text": "Question 1\n\nTelepathy is the ability of an individual to communicate thoughts and ideas by means other than the known senses. I claim that I do have telepathy. There are two possibilities: either I do or I do not. Which claim is the null hypothesis and which is the alternative?\n\nSolution to Question 1"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-q2",
    "href": "21-Intro-Hypothesis-Tests.html#sec-q2",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 2",
    "text": "Question 2\n\nThere are many different studies we could design to test these competing claims. I will collect data as as follows:\n\nI will think of a letter A, B, C, or D.\nI will telepathically communicate this letter to everyone.\nEach person will tell me what letter they believed I was thinking of.\n\nIf I could collect responses from everyone in the population, I could calculate the parameter \\(p\\), the proportion of all responses from everyone in the world that are the correct letter.\n\nIf \\(H_0\\) is true, what would you expect the value of \\(p\\) to be?\nIf \\(H_a\\) is true, what would you expect the value of \\(p\\) to be?\n\n\nSolution to Question 2"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-q3",
    "href": "21-Intro-Hypothesis-Tests.html#sec-q3",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 3",
    "text": "Question 3\n\nI cannot conduct my study on everyone in the world. Instead, I pick a sample of people from which to collect data. Suppose I am in a room with 15 other people who will participate in the study as designed in Question 2. Let \\(\\hat{p}\\) denote the proportion of the people in the room that correctly say the letter I was thinking of.\n\nWhat would be enough evidence to convince you that I do have telepathy?\n\nState your answer in terms of the value of the sample proportion \\(\\hat{p}\\).\n\nSolution to Question 3"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#stating-the-hypotheses",
    "href": "21-Intro-Hypothesis-Tests.html#stating-the-hypotheses",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Stating the Hypotheses",
    "text": "Stating the Hypotheses\n\nDeciding on a clear and pertinent set of hypotheses is perhaps the most important step in hypothesis testing. Carefully setting up hypotheses helps keep our analysis on track so we achieve our goal.\n\nThe Null and Alternative Hypotheses\n\n\nThe null hypothesis is a “boring” claim. If the null hypothesis is true, this would not really affect or change what is currently believed.\nThe alternative hypothesis is the new or exciting claim a researcher is hoping to establish that is in direct competition with the boring claim in the null hypothesis.\n\nFor example, we generally believe coins are “fair” in the sense that when you flip a coin it has a 50% chance of landing on heads and a 50% chance of landing on tails. It would be more interesting if somebody claimed that a coin was not fair.\n\nTwo-Tailed Tests\n\nLet \\(p\\) denote the proportion of all flips of a coin that land on heads. If our goal is to show a coin is not fair, without indicating a direction of the bias, we could set up hypotheses\n\n\\(H_0\\): \\(p=0.5\\). The coin is fair (boring).\n\\(H_a\\): \\({\\color{tomato}{p \\ne 0.5}}\\). The coin is not fair (what we want to prove).\n\nIf we claim a coin is not fair without specifying a direction of the bias, we use a \\({\\color{tomato}{\\ne}}\\) sign in \\(H_a\\), and we say the test is a two-tailed test. There are other ways we could claim a coin is biased.\n\n\nOne-Tailed Tests\n\nA coin might be more biased to land on heads (\\(p &gt; 0.5\\)). A coin might be more biased to land on tails (\\(p &lt; 0.5\\)).\n\nIf we want to test the claim that a coin is biased to land on heads:\n\n\\(H_0\\): \\(p=0.5\\). The coin is fair (boring).\n\\(H_a\\): \\({\\color{tomato}{p &gt; 0.5}}\\). The coin is biased to heads (what we want to prove).\n\nIf we want to test the claim that a coin is biased to land on tails:\n\n\\(H_0\\): \\(p=0.5\\). The coin is fair (boring).\n\\(H_a\\): \\({\\color{tomato}{p &lt; 0.5}}\\). The coin is biased to tails (what we want to prove).\n\n\nWhen we specify a direction of the inequality we use either \\(&gt;\\) or \\(&lt;\\) in \\(H_a\\), and we say the test is a one-tailed test.\n\n\n\nGuidelines for Stating Hypotheses\n\nFrom the examples above, we note some helpful guidelines for setting up hypotheses:\n\nWe state hypotheses both in words and using mathematical notation.\n\nClearly stating the hypothesis in words will help us interpret results.\nUsing appropriate mathematical notation will help guide our statistical analysis.\n\nThe hypotheses must be competing claims.\nBe sure to clearly indicate the population of interest.\nWhen using notation to state the hypotheses:\n\nUse population parameters such as \\(\\mu\\) and \\(p\\).\nDo not state hypotheses using statistics such as \\(\\bar{x}\\) or \\(\\hat{p}\\).\nWe always use an equality sign, \\(=\\), in \\(H_0\\).\nWe always use an inequality sign (\\(\\ne\\), \\(&gt;\\), or \\(&lt;\\)) in \\(H_a\\)."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#computing-a-test-statistic",
    "href": "21-Intro-Hypothesis-Tests.html#computing-a-test-statistic",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Computing a Test Statistic",
    "text": "Computing a Test Statistic\n\nStating our hypotheses in mathematical notation is extremely helpful in helping us decide which sample statistics will be most helpful in assessing our hypotheses. Essentially, based on the population parameters we use in \\(H_0\\) and \\(H_a\\), we use the corresponding statistic(s) from the sample(s). For example:\n\nIf we want to test claims about \\(p\\), calculating \\(\\hat{p}\\) will be useful.\nIf we want to test claims about \\(\\mu_1 - \\mu_2\\), calculating \\(\\bar{x}_1-\\bar{x}_2\\) will be useful.\n\nOften we standardize statistics, for example using \\(z\\)-scores. We will explore this idea more deeply later."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-informal-p",
    "href": "21-Intro-Hypothesis-Tests.html#sec-informal-p",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Assess Which Claim is More Likely",
    "text": "Assess Which Claim is More Likely\n\nWhen we set up hypotheses, we use a boring claim for the null hypothesis the that is generally assumed to be true. The alternative hypothesis is a new claim, if true, would cause us to doubt and possibly reject the null claim.\n\nOur goal is not to prove anything about the null claim \\(H_0\\).\nOur goal is to show \\(H_a\\) is very likely true by showing \\(H_0\\) is very unlikely to be true.\n\nThe logic in hypothesis testing is similar to a proof technique called proof by contradiction.\n\nWe assume the boring claim \\(H_0\\) is true.\nWe hope to prove our assumption is unlikely, and thus the opposite (\\(H_a)\\) is true.\n\nBased on our test statistic(s) and whether the alternative hypothesis is two-tailed or one-tailed, we assess statistical significance as follows:\n\nIf the test statistic is likely if \\(H_0\\) is true, then we do not have evidence to reject \\(H_0\\).\n\nThe test is not statistically significant.\nThe test is inconclusive.\nWe neither reject nor accept \\(H_0\\).\n\nIf the test statistic is unlikely if \\(H_0\\) is true in the direction of the claim in \\(H_a\\), then we have evidence that refutes \\(H_0\\) and supports \\(H_a\\).\n\nThe test is statistically significant.\nWe reject \\(H_0\\) and accept \\(H_a\\).\n\n\nWhat do we mean be “likely” and “unlikely”? We consider the concept informally at first, and we will explore this concept more deeply later using p-values."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#making-a-conclusion",
    "href": "21-Intro-Hypothesis-Tests.html#making-a-conclusion",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Making a Conclusion",
    "text": "Making a Conclusion\n\nIn the end, there are two possible results from a hypothesis test. Either:\n\nThe test is statistically significant.\n\nWe reject \\(H_0\\).\nWe accept the competing claim, \\(H_a\\).\n\nThe test is not statistically significant.\n\nWe fail to reject \\(H_0\\). We do NOT accept \\(H_0\\).\nThe test is inconclusive as far as \\(H_a\\) is considered,\n\n\nWe will more formally discuss just how unlikely or likely a sample must be in order to for the result to be statistically significant. Informally, we assume the null hypothesis is true and require evidence beyond a reasonable doubt that \\(\\mathbf{H_0}\\) is not true in order to be convinced we should reject \\(H_0\\).\nWe do not want different researchers subjectively choosing what “beyond a reasonable doubt” means to them. We hope to control for confounding variables in our studies, and making this decision as objective as possible helps account for potential bias by the researchers. In statistical hypothesis testing, a significance level is used to make this decision."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#sec-diner",
    "href": "21-Intro-Hypothesis-Tests.html#sec-diner",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 4: Unscrupulous Diner’s Dilemma",
    "text": "Question 4: Unscrupulous Diner’s Dilemma\n\n\n\n\nCredit: Benreis, CC BY 3.0, via Wikimedia Commons\n\n\nAn article1 from The Economic Journal studied the so called unscrupulous diner’s dilemma.\n\nThe unscrupulous diner’s dilemma is a problem faced frequently in social settings. When a group of diners jointly enjoys a meal at a restaurant, often an unspoken agreement exists to divide the check equally. A selfish diner could thereby enjoy exceptional dinners at bargain prices. This dilemma typifies a class of serious social problems2 from environmental protection and resource conservation to eliciting charity donations and slowing arms races.\n\nResearchers wanted to test whether people order more food and beverages when they know the bill is going to split evenly, or do they order the same amount regardless of whether they are splitting the bill or paying individually.\n\nQuestion 4a\n\nState the null and alternative hypotheses both in words and using mathematical notation.\n\nSolution to Question 4a\n\n\n\n\n\n\n\nQuestion 4b\n\nTo test the claims, participants were randomly assigned into two tables, each with four people. One table (even-split group) was randomly picked and told they were going to evenly-split the bill. The other table (control) was told each person was going to pay for what they ordered. The mean amount ordered by the control group was \\(\\$8.67\\). Which of following samples for the even-split group is the most statistically significant? Support your answer with an explanation.\n\n\n\nTest Stat 1\nTest Stat 2\nTest Stat 3\nTest Stat 4\n\n\n\n\n\\(\\$4.67\\)\n\\(\\$8.50\\)\n\\(\\$8.80\\)\n\\(\\$11.23\\)\n\n\n\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nIf the table below gives the amounts ordered by each of the four people in each group, what is the value of the test statistic? Complete the R code cell below to calculate a test statistic.\n\nEven-Split Group\n\n\n\nPerson 1\nPerson 2\nPerson 3\nPerson 4\n\n\n\n\n\\(\\$15.00\\)\n\\(\\$8.00\\)\n\\(\\$8.75\\)\n\\(\\$13.17\\)\n\n\n\n\n\nControl Group\n\n\n\nPerson 1\nPerson 2\nPerson 3\nPerson 4\n\n\n\n\n\\(\\$8.50\\)\n\\(\\$7.90\\)\n\\(\\$10.85\\)\n\\(\\$7.43\\)\n\n\n\n\n\nSolution to Question 4c\n\n\n# enter each sample as a vector below\neven &lt;- c(15, 8, 8.75, 13.17)   # even-split data\ncontrol &lt;- c(??)  # control data\n\n# calculate an appropriate test statistic\n\n\n\n\n\n\nQuestion 4d\n\nBased on the test statistic, what do you think we can conclude about the two competing claims?\n\nSolution to Question 4d"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#question-5-social-pressure-and-voter-turnout",
    "href": "21-Intro-Hypothesis-Tests.html#question-5-social-pressure-and-voter-turnout",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 5: Social Pressure and Voter Turnout",
    "text": "Question 5: Social Pressure and Voter Turnout\n\nA 2008 experiment3 at Yale aimed to determine whether positive or negative pressure is more effective at improving voter turnout.\n\nVoter turnout theories based on rational self-interested behavior generally fail to predict significant turnout unless they account for the utility that citizens receive from performing their civic duty. We distinguish between two aspects of this type of utility,\n\nOne group received a mailing emphasizing the intrinsic (internal) satisfaction for voting:\n\n\n\nIntrinsic Mailing\n\n\nAnother group received a mailing placing extrinsic (outside) pressure on people to vote:\n\n\n\nExtrinsic Mailing\n\n\n\nQuestion 5a\n\nState the null and alternative hypotheses the researches can use to test whether positive or negative pressure is more effective at improving voter turnout. State the hypotheses both in words and using mathematical notation.\n\nSolution to Question 5a\n\n\n\n\n\n\n\nQuestion 5b\n\nWhat is a possible test statistic the researchers could use to assess the competing claims in your previous answer? There is not specific value to give. Rather, explain how you could use sample data to compute a test statistic.\n\nSolution to Question 5b"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#question-6",
    "href": "21-Intro-Hypothesis-Tests.html#question-6",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 6",
    "text": "Question 6\n\nRecall the telepathy example in Questions 1, 2, and 3. Let \\(T\\) be the number of people (out of the possible 15 people in the room) that said the correct the letter I was thinking. Suppose we observed that 12 out of 15 people correctly said the letter I was thinking.\n\nQuestion 6a\n\nCalculate the \\(p\\)-value of the observed test statistic. Use the R code cell below to help with the calculation.\n\n\n\n\n\n\nTip\n\n\n\nIn 15 identical and independent trials under the assumption \\(H_0\\) is true, what is \\(P(T \\geq 12)\\)?\n\n\n\nSolution to Question 6a\n\n\n# p-value of the observed 12 out 15 successes\n\n\n\n\n\n\n\nQuestion 6b\n\nBased on the value of the \\(p\\)-value, what can we conclude about my telepathy ability?\n\nSolution to Question 6b"
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#question-7",
    "href": "21-Intro-Hypothesis-Tests.html#question-7",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "Question 7",
    "text": "Question 7\n\nWhat is the null distribution for the previous telepathy example?\n\nSolution to Question 7\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "21-Intro-Hypothesis-Tests.html#footnotes",
    "href": "21-Intro-Hypothesis-Tests.html#footnotes",
    "title": "6.1: Introduction to Hypothesis Tests",
    "section": "",
    "text": "Gneezy, U., E. Haruvy, and H. Yafe (2004), “The Inefficiency of Splitting the Bill”, The Economic Journal 114↩︎\nGlance, N., and B. Huberman, “The Dynamics of Social Dilemmas”, Scientific American↩︎\nGerber, A., D. Green, and C. Larimer, “Social Pressure and Voter Turnout: Evidence from a Largescale Field Experiment”, American Political Science Review, Feb 2008↩︎"
  },
  {
    "objectID": "22-Permutation-Tests.html",
    "href": "22-Permutation-Tests.html",
    "title": "6.2: Permutation Tests",
    "section": "",
    "text": "A Summary of Hypothesis Testing\nIn the section Introduction to Hypothesis Tests, we walked through the process of performing a statistical hypothesis test:\nSee Introduction to Hypothesis Tests for a refresher on Steps 1 and 2. We also informally explored the concept of statistical significance. Today, we focus on Step 3 and discuss a resampling method, called a permutation test, that we can use to calculate p-values to assess significance.\nWe previously considered the unscrupulous diner’s dilemma1.\nResearchers wanted to test whether people order more food and beverages when they know the bill is going to be split evenly compared to when each person only pays for what they ordered.\nTo perform a two-sample permutation test on data collected from two samples size \\(m\\) and \\(n\\):\nThe p-value is the proportion of times the randomized statistics are as or more extreme than the observed difference.\nSkin is the largest organ in the human body. In the United States, skin cancer is the most common cancer. Current estimates are that one in five Americans will develop skin cancer in their lifetime3. There are different types of skin cancers, and melanoma is one form of skin cancer that is particularly dangerous if not detected early. However, if a skin lesion is detected early, it can be surgically removed before it spreads, and a patient generally has good long-term outcomes.\nThe data set melanoma in the boot package contains measurements from a random sample of \\(205\\) patients with malignant melanoma at the University Hospital of Odense in Denmark. Each patient had a skin lesion (or tumor) surgically removed and various attributes of the patient and tumors are recorded. Run the code cell below to load the boot package and summarize the variables in the melanoma data set.\nlibrary(boot)\nsummary(melanoma)\n\n      time          status          sex              age             year     \n Min.   :  10   Min.   :1.00   Min.   :0.0000   Min.   : 4.00   Min.   :1962  \n 1st Qu.:1525   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1968  \n Median :2005   Median :2.00   Median :0.0000   Median :54.00   Median :1970  \n Mean   :2153   Mean   :1.79   Mean   :0.3854   Mean   :52.46   Mean   :1970  \n 3rd Qu.:3042   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:65.00   3rd Qu.:1972  \n Max.   :5565   Max.   :3.00   Max.   :1.0000   Max.   :95.00   Max.   :1977  \n   thickness         ulcer      \n Min.   : 0.10   Min.   :0.000  \n 1st Qu.: 0.97   1st Qu.:0.000  \n Median : 1.94   Median :0.000  \n Mean   : 2.92   Mean   :0.439  \n 3rd Qu.: 3.56   3rd Qu.:1.000  \n Max.   :17.42   Max.   :1.000\nWe use the permutation distribution as an estimate for the null distribution. The p-value is the proportion of all resampled test statistics that are as or more extreme than the observed test statistic. We can use a logical test to help compute this proportion."
  },
  {
    "objectID": "22-Permutation-Tests.html#the-null-distribution-and-p-values",
    "href": "22-Permutation-Tests.html#the-null-distribution-and-p-values",
    "title": "6.2: Permutation Tests",
    "section": "The Null Distribution and p-values",
    "text": "The Null Distribution and p-values\n\nRecall the logic of hypothesis test. We want to assess which of two competing claims, the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_a\\), are more likely to be true. We assume \\(H_0\\) is true and consider whether the sample data supports or refutes the null claim.\n\nThe null distribution is the distribution of the test statistic if the null hypothesis is true. We use the null distribution to calculate the p-value.\nThe p-value is the probability that we would get a random sample with a test statistic as or more extreme as the observed test statistic if the null hypothesis were true.\n\n\\[{\\large {\\color{dodgerblue}{p\\mbox{-value} = P( \\mbox{test statistic as or more extreme than observed} \\ | \\ H_0 \\mbox{ is true} )}}}.\\]\n\nThe smaller the \\(p\\)-value, the less likely the sample is assuming \\(H_0\\) is true.\nThere is evidence that contradicts \\(H_0\\) and supports \\(H_a\\).\nThe smaller the \\(p\\)-value, the more statistically significant the result."
  },
  {
    "objectID": "22-Permutation-Tests.html#step-1-state-the-hypotheses",
    "href": "22-Permutation-Tests.html#step-1-state-the-hypotheses",
    "title": "6.2: Permutation Tests",
    "section": "Step 1: State the Hypotheses",
    "text": "Step 1: State the Hypotheses\n\n\n\\(H_0\\): There is no difference in how much people order regardless of how the bill is split. \\({\\color{dodgerblue}{\\mu_{\\rm even} - \\mu_{\\rm control}=0}}.\\)\n\\(H_a\\): People order more when the bill is split evenly as opposed to when each person pays for what they order. \\({\\color{dodgerblue}{\\mu_{\\rm even} - \\mu_{\\rm control}&gt;0}}.\\)"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-2-collect-sample-data-and-define-a-test-statistic",
    "href": "22-Permutation-Tests.html#step-2-collect-sample-data-and-define-a-test-statistic",
    "title": "6.2: Permutation Tests",
    "section": "Step 2: Collect Sample Data and Define a Test Statistic",
    "text": "Step 2: Collect Sample Data and Define a Test Statistic\n\n8 people volunteered to take part in the study.\n\n4 people were randomly assigned to sit at a table where they were told the bill would be evenly split between the 4 people.\n4 people were randomly assigned to sit at a table where they were told each person would pay only for what they order themselves.\n\nThe results of the study are given in the tables below:\n\n\n\n\n\n\n\nEven Split Group\nPay for what you order (control) Group\n\n\n\n\n\\(\\$15.00\\), \\(\\$8.00\\), \\(\\$8.75\\), \\(\\$13.17\\)\n\\(\\$8.50\\) , \\(\\$7.90\\) , \\(\\$10.85\\), \\(\\$7.43\\)\n\n\n\\(\\bar{x}_{\\rm even} = \\$11.23\\)\n\\(\\bar{x}_{\\rm control} = \\$8.67\\)\n\n\n\n\nWe can use the difference in the two sample means as a test statistic:\n\n\\[{\\color{dodgerblue}{T= \\bar{x}_{\\rm even} - \\bar{x}_{\\rm control} = 2.56}}\\]"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-3-how-extreme-is-the-observed-difference",
    "href": "22-Permutation-Tests.html#step-3-how-extreme-is-the-observed-difference",
    "title": "6.2: Permutation Tests",
    "section": "Step 3: How Extreme Is the Observed Difference?",
    "text": "Step 3: How Extreme Is the Observed Difference?\n\nThe p-value is the probability of getting a difference in sample means between the even-split and self-pay groups as or more extreme than the observed test statistic, \\(\\bar{x}_{\\rm even} - \\bar{x}_{\\rm control} = 2.56\\). In this case (one-tailed test), more extreme implies a difference that is even bigger than \\(\\$2.56\\).\n\\[{\\color{dodgerblue}{\\mbox{p-value} = P(\\bar{x}_{\\rm even} - \\bar{x}_{\\rm control} \\geq 2.56 \\ | \\ H_0 \\mbox{ is true} )}}\\]\nIf \\(H_0\\) is true, then \\(\\mu_{\\rm even} - \\mu_{\\rm control} =0\\). Under this assumption, the center of the sampling distribution for the difference in sample means would be \\(0\\). However, we do not know how much variability there is due to the sampling. Is a difference in sample means equal to \\(\\$2.56\\) a “big” difference, or is the difference within the margins we would expect due to the uncertainty of sampling?\n\nHow can we calculate the p-value if we do not know the underlying probability distribution for the test statistic \\(T = \\mu_{\\rm{even}} - \\mu_{\\rm{control}}\\) if \\(H_0\\) is true?"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-1",
    "href": "22-Permutation-Tests.html#question-1",
    "title": "6.2: Permutation Tests",
    "section": "Question 1",
    "text": "Question 1\n\nIf people order the same amount of food no matter how the bill is split (assuming \\(H_0\\) is true), we assume each person would order the same amount of food regardless of the table they were seated at. Splitting by groups based on how the bill is paid is no different than if the eight values were just randomly divided into two groups of four people. If each person would have ordered the same regardless of which table they were seated at, then another possible sample could have been:\n\n\n\n\n\n\n\nEven Split Group\nPay for what you order (control) Group\n\n\n\n\n\\(\\mathbf{\\color{dodgerblue}{\\$7.43}}\\) , \\(\\$8.00\\), \\(\\$8.75\\), \\(\\$13.17\\)\n\\(\\$8.50\\) , \\(\\$7.90\\) , \\(\\$10.85\\), \\(\\mathbf{\\color{dodgerblue}{\\$15.00}}\\)\n\n\n\n\nQuestion 1a\n\nWhat would be the test statistic for the two samples above?\n\nSolution to Question 1a\n\n\n# find the test statistic for another possible sample\n\n\n\n\n\n\nQuestion 1b\n\nHow many different ways can we divide the eight participants into two groups of four?\n\nSolution to Question 1b\n\n\n# how many possible ways can we create two groups of 4"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-2",
    "href": "22-Permutation-Tests.html#question-2",
    "title": "6.2: Permutation Tests",
    "section": "Question 2",
    "text": "Question 2\n\nBased on the summary(melanoma) output, give a possible statistical question that could be analyzed using a hypothesis test. Which variable(s) in the melanoma data set would be involved in your analysis?\n\n\n\n\n\n\nTip\n\n\n\nRun ?melanoma to access the help documentation and learn more about the data.\n\n\n\nSolution to Question 2"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-3",
    "href": "22-Permutation-Tests.html#question-3",
    "title": "6.2: Permutation Tests",
    "section": "Question 3",
    "text": "Question 3\n\nWhich variables in melanoma are categorical? Are those variables being stored as categorical variables? If so, explain how you can tell. If not, in the code cell below, convert the categorical variables to a factor.\n\nSolution to Question 3\n\n\n# if needed, convert each categorical variable to a factor"
  },
  {
    "objectID": "22-Permutation-Tests.html#creating-a-pooled-sample",
    "href": "22-Permutation-Tests.html#creating-a-pooled-sample",
    "title": "6.2: Permutation Tests",
    "section": "Creating a Pooled Sample",
    "text": "Creating a Pooled Sample\n\nOne possible question we could ask: “Is the mean tumor thickness greater for all people with fatal melanoma compared to the mean tumor thickness for all people that survive melanoma?” The two variables of interest are status and thickness.\n\nstatus: The patients status at the end of the study. 1 indicates that they had died from melanoma, 2 indicates that they were still alive and 3 indicates that they had died from causes unrelated to their melanoma.\nthickness: Tumor thickness in millimeters (mm).\n\nFor our study, we are comparing two independent populations:\n\nPeople that have a melanoma tumor removed and survive. This is status group 2.\nPeople that have a melanoma tumor removed and died from melanoma. This is status group 1.\nThe people in status group 3 we exclude from this analysis."
  },
  {
    "objectID": "22-Permutation-Tests.html#question-3-1",
    "href": "22-Permutation-Tests.html#question-3-1",
    "title": "6.2: Permutation Tests",
    "section": "Question 3",
    "text": "Question 3\n\nAnswer the questions below to state our hypotheses, organize our data, and calculate a test statistic to help determine whether the mean tumor thickness is greater for all people with fatal melanoma compared to the mean tumor thickness for all people that survive melanoma?\n\nQuestion 3a\n\nCreate side-by-side box plots to display the distribution of tumor thickness for each of the three status groups 1 (died from melanoma), 2 (survived), and 3 (died from other causes).\n\nSolution to Question 3a\n\nFill in the boxplot() command to answer the question.\n\nboxplot(??)\n\n\n\n\n\n\nQuestion 3b\n\nThe function tapply(data, index, function) has three inputs:\n\nThe data is the data you want to summarize.\nThe index is a categorical feature that will split the data into two or more different classes or factors.\nThe function is some function that you want to apply to the data.\n\nInterpret the output from the code cell below.\n\n# run code and interpret output below\ntapply(melanoma$thickness, melanoma$status, length)  # nothing to edit\n\n  1   2   3 \n 57 134  14 \n\n\n\nSolution to Question 3b\n\n\n\n\n\n\n\nQuestion 3c\n\nWe would like to use the data in melanoma to test the if the mean tumor thickness is greater for all people with fatal melanoma compared to the mean tumor thickness for all people that survive melanoma? State the corresponding null and alternative hypotheses for this test. Be sure to state each hypothesis both in words and using mathematical notation.\n\nSolution to Question 3c\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 3d\n\nUsing the code cell below, subset the melanoma data to create three different vectors:\n\nThe vector died is the thickness values for status group 1.\nThe vector survived is the thickness values for status group 2.\nThe vector pooled is the thickness values for both status groups 1 and 2 (excluding group 3).\n\n\n\n\n\n\n\nNote\n\n\n\nThe logical operator == means “is equal to” while the logical operator != means “is not equal to”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe option drop = TRUE means the output will be a vector of numerical values as opposed to a data frame that has a variable with the name thickness. For example, if we want to calculate the sample mean for the pooled data:\n\nSince the data is stored as a vector, we use mean(pooled). There are no headers in vectors.\nIf the data is stored as a data frame, we do need to use headers and have mean(pooled$thickness).\n\nThe output of the code cell below are vectors, so we do not use the $var_name convention when referring to the sample data which helps simplify the code a little.\n\n\n\nSolution to Question 3d\n\nReplace each of the 9 ?? with an appropriate value or variable name.\n\ndied &lt;- subset(melanoma, select = ??, ?? == \"??\", drop = TRUE)\nsurvived &lt;- subset(melanoma, select = ??, ?? == \"??\", drop = TRUE)\npooled &lt;- subset(melanoma, select = ??, ?? != \"??\", drop = TRUE)\n\n\n\n\n\n\nQuestion 3e\n\nUsing the vectors died, survived, and pooled from Question 3d, calculate the sample size and the sample mean for each of the samples died, survived, and pooled.\n\nSolution to Question 3e\n\n\nn.died &lt;- ??  # size of sample of melanoma deaths\nn.survived &lt;- ??  # size of sample of survivors\nn.pooled &lt;- ??  # size of both samples pooled together\nxbar.died &lt;- ??  # mean thickness of sample that died\nxbar.survived &lt;- ??  # mean thickness of sample that survived\nxbar.pooled &lt;- ??  # mean thickness of pooled sample\n\n# print each result to screen\nn.died \nn.survived \nn.pooled \nxbar.died \nxbar.survived \nxbar.pooled   \n\n\n\n\n\n\nQuestion 3f\n\nBased on the output in Question 3e, calculate, store, and print the test statistic for this test to the screen.\n\nSolution to Question 3f\n\n\ntest.stat &lt;- ??  # compute test statistic\ntest.stat  # print test statistic to screen"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-1-create-a-vector-of-pooled-data",
    "href": "22-Permutation-Tests.html#step-1-create-a-vector-of-pooled-data",
    "title": "6.2: Permutation Tests",
    "section": "Step 1: Create a Vector of Pooled Data",
    "text": "Step 1: Create a Vector of Pooled Data\n\nSee the code used to create pooled in Question 3d."
  },
  {
    "objectID": "22-Permutation-Tests.html#step-2-create-resamples-for-each-treatment-group",
    "href": "22-Permutation-Tests.html#step-2-create-resamples-for-each-treatment-group",
    "title": "6.2: Permutation Tests",
    "section": "Step 2: Create Resamples for Each Treatment Group",
    "text": "Step 2: Create Resamples for Each Treatment Group\n\nFor this step, it is important to note the size of each original sample. From Question 3e we know\n\nThe sample died consists of 57 observations.\nThe sample survived consists of 134 observations.\nThe pooled sample consists of \\(57 + 134= 191\\) observations.\n\n\nCreate an Index Vector\n\nWe first create a vector called index that selects (without replacement) 57 random integers out of the integers \\(1, 2, \\ldots , 191\\).\n\nset.seed(3021)  # fix the randomization seeding\nindex &lt;- sample(191, size = 57, replace = FALSE)  # these are the 57 observations chosen for resample 1\n\nThe command head(index) shows the first 6 values in the vector index.\n\nhead(index)\n\n[1]  55 164  88 189 171  68\n\n\nThe first index value 55 means observation 55 from the pooled vector is the first thickness value randomly assigned to the died sample. From the code cell below, we see the 55th observation in pooled is a thickness of \\(0.81\\) mm.\n\npooled[55]\n\n[1] 0.81\n\n\nThe next index value 164 means observation 164 from the pooled vector is the second thickness value randomly assigned to the died sample. From the code cell below, we see the 164th observation in pooled is a thickness of \\(0.65\\) mm.\n\npooled[164]\n\n[1] 0.65\n\n\n\n\nUse index to Select Resample of Group 1\n\nThe index vector is a vector of integers that tells us which values in pooled are randomly assigned to the died resample. However, index does not contain the corresponding thicknesses of the selected observations. The vector pooled[index] will contain the tumor thicknesses for all 57 randomly selected observations picked in index.\n\ndied.resample &lt;- pooled[index]\n\n\n\nUse -index to Select the Remaining Values for Resample Group 2\n\n\nThe vector index consists of 57 randomly selected integers out of the integers \\(1, 2, \\ldots , 191\\).\nThe vector -index contains the remaining 134 integers that were not selected for index.\nThe vector pooled[-index] will contain thicknesses for the remaining 134 observations in resample group 2.\n\n\nsurvived.resample &lt;- pooled[-index]"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-3-calculate-the-test-statistic-for-the-resamples",
    "href": "22-Permutation-Tests.html#step-3-calculate-the-test-statistic-for-the-resamples",
    "title": "6.2: Permutation Tests",
    "section": "Step 3: Calculate the Test Statistic for the Resamples",
    "text": "Step 3: Calculate the Test Statistic for the Resamples\n\nCalculate the difference in the sample means between the two resamples.\n\nperm.stat &lt;- mean(died.resample) - mean(survived.resample)\nperm.stat\n\n[1] -0.1487287"
  },
  {
    "objectID": "22-Permutation-Tests.html#step-4-repeat-this-many-times-to-construct-a-permutation-distribution",
    "href": "22-Permutation-Tests.html#step-4-repeat-this-many-times-to-construct-a-permutation-distribution",
    "title": "6.2: Permutation Tests",
    "section": "Step 4: Repeat this Many Times to Construct a Permutation Distribution",
    "text": "Step 4: Repeat this Many Times to Construct a Permutation Distribution\n\n\nIn practice, it takes a lot of time and energy (and money) to generate all possible resamples (without any duplicate resamples).\n\nInstead we’ll generate a lot of resamples rather than all possible resamples.\n\nWe’ll use \\(N=10^5-1=99,\\!999\\) as the default number of resamples. Why use 99,999 resamples?\n\nWe may not generate the original sample as one of the resamples.\nWe want to be sure that we do include the original sample when we calculate the p-value.\nWe will add the original sample back in with the \\(99,\\!999\\) resamples giving \\(100,\\!000\\) samples.\n\nThe resulting distribution of test statistics of the permutation resamples is called a permutation distribution.\nWe use the permutation distribution as an estimate for the null distribution to compute the p-value."
  },
  {
    "objectID": "22-Permutation-Tests.html#question-4",
    "href": "22-Permutation-Tests.html#question-4",
    "title": "6.2: Permutation Tests",
    "section": "Question 4",
    "text": "Question 4\n\nComplete the first code cell below to create a permutation distribution for the difference in sample mean tumor thickness. Be sure you have already created the vector pooled in Question 3d and stored test.stat in Question 3f.\nAfter generating a permutation distribution, run the second code cell below to create a histogram to display the distribution of resample statistics along with a red vertical line through the observed test statistic. There is nothing to edit in the second code cell.\n\nSolution to Question 4\n\n\n##########################################\n# save all 99,999 permutation resample \n# statistics to the vector perm.stat\n##########################################\nN &lt;- 10^5 - 1\nperm.stat &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  index &lt;- sample(??, size = ??, replace = ??)  # create index vector\n  x.died &lt;- ??  # use index to select died resample\n  x.survived &lt;- ??  # the rest of the values go to survived resample\n  perm.stat[i] &lt;- ??  # calc difference in sample means\n}\n\n\n##################################################\n# plot permutation distribution as a histogram\n# mark observed test stat with red vertical line\n##################################################\nhist(perm.stat, xlab = \"xbar.died - xbar.survived\",\n     main = \"Permutation Distribution\")\nabline(v = test.stat, col = \"red\")"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-5",
    "href": "22-Permutation-Tests.html#question-5",
    "title": "6.2: Permutation Tests",
    "section": "Question 5",
    "text": "Question 5\n\nHow likely is it to get resamples with a difference in means as or more extreme than the observed test statistic? Explain what the code below is doing in practical terms.\n\np.value &lt;- sum(perm.stat &gt;= test.stat)/N\np.value\n\n\nSolution to Question 5\n\nInterpret the code cell above."
  },
  {
    "objectID": "22-Permutation-Tests.html#question-6",
    "href": "22-Permutation-Tests.html#question-6",
    "title": "6.2: Permutation Tests",
    "section": "Question 6",
    "text": "Question 6\n\nThe calculation from Question 5 used the \\(N=10^5-1=99,\\!999\\) permutation resamples, but recall we want to be sure to include the original, observed sample when computing the p-value. Explain how the code cell below accomplishes this goal.\n\np.value &lt;- (sum(perm.stat &gt;= test.stat) + 1) / (N + 1)\np.value\n\n\nSolution to Question 6"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-7",
    "href": "22-Permutation-Tests.html#question-7",
    "title": "6.2: Permutation Tests",
    "section": "Question 7",
    "text": "Question 7\n\nInterpret the practical meaning of the p-value from Question 6 to a person who is not very familiar with statistics.\n\nSolution to Question 7"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-8",
    "href": "22-Permutation-Tests.html#question-8",
    "title": "6.2: Permutation Tests",
    "section": "Question 8",
    "text": "Question 8\n\nUlceration is a breakdown of the skin over the melanoma tumor. Using the data set melanoma from the boot package, perform a permutation test to see if the variance of the tumor thickness for ulcerated tumors is different from the variance of the tumor thickness for non-ulcerated tumors.\n\n\n\n\n\n\nTip\n\n\n\nThe variable ulcer indicates whether the removed tumor was ulcerated (ulcer group 1) or not ulcerated (ulcer group 0).\n\n\n\nQuestion 8a\n\nWrite out the null and alternative hypotheses using appropriate notation.\n\nSolution to Question 8a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\n\nQuestion 8b\n\nWhat can we use as the test statistic? What is the value of the observed test statistic?\n\nSolution to Question 8b\n\n\ntest.stat2 &lt;- ??  # compute observed test statistic\ntest.stat2  # print output to screen\n\n\n\n\n\n\nQuestion 8c\n\nCreate a permutation distribution for the difference in sample variances.\n\nSolution to Question 8c\n\n\n# nothing to edit in this cell\npooled2 &lt;- melanoma$thickness  # create pooled vector of all tumor thicknesses\n\n\n# Save resamples to vector called result\nN &lt;- 10^5 - 1\nresult &lt;- numeric(N)\n\n# Create permutation distribution\nfor (i in 1:N)\n{\n  index &lt;- sample(??, size = ??, replace = ??)\n  result[i] &lt;- ??\n}\n\n# Display permutation distribution and observed sample diff\nhist(result, xlab = \"diff in sample variances\",\n     main = \"Permutation Distribution\")\nabline(v = c(-test.stat2, test.stat2), col = c(\"blue\", \"red\"))\n\n\n\n\n\n\nQuestion 8d\n\nCalculate the p-value of the observed test statistic and interpret its meaning in practical terms.\n\nSolution to Question 8d\n\n\n# compute the p-value"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-9",
    "href": "22-Permutation-Tests.html#question-9",
    "title": "6.2: Permutation Tests",
    "section": "Question 9",
    "text": "Question 9\n\nIs the proportion of females with ulcerated tumors less than the proportion of males with ulcerated tumors?\n\nQuestion 9a\n\nWrite out the null and alternative hypotheses using appropriate notation.\n\nSolution to Question 9a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\n\nQuestion 9b\n\nWhat can we use as the test statistic? What is the value of the observed test statistic?\n\nSolution to Question 9b\n\n\n# original ulceration data for female sample\nfemale &lt;- subset(melanoma, select = \"ulcer\", sex == \"0\", drop = TRUE)\n\n# original ulceration data for male sample\nmale &lt;- subset(melanoma, select = \"ulcer\", sex == \"1\", drop = TRUE)\n\n# original ulceration data for both samples pooled together\npooled.sex &lt;- melanoma$ulcer \n\n\n# enter a formula to compute the test statistic\ntest.diff.prop &lt;- ??\n\n\n\n\n\n\nQuestion 9c\n\nCreate a permutation distribution for the difference in sample proportions.\n\nSolution to Question 9c\n\nComplete the code cell below.\n\nN &lt;- 10^5 - 1\nresult.prop &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  index &lt;- sample(??, size = ??, replace = ??)\n  result.prop[i] &lt;- ??\n}\n\nhist(result.prop, xlab = \"phat1-phat2\",\n     main = \"Permutation Distribution\")\nabline(v = test.diff.prop, col = \"red\")\n\n\n\n\n\n\nQuestion 9d\n\nCalculate the p-value of the observed test statistic and interpret its meaning in practical terms.\n\nSolution to Question 9d\n\n\n# compute the p-value"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-10",
    "href": "22-Permutation-Tests.html#question-10",
    "title": "6.2: Permutation Tests",
    "section": "Question 10",
    "text": "Question 10\n\nIn this example, we use data collected from a matched pair designed study to determine whether smoking during pregnancy is associated with lower birth weight. In our study, we solicit volunteers that have already given birth to two babies. During one of the pregnancies, the parent smoked. During the other pregnancy, they did not smoke. Below is hypothetical data from such a study. A sample of \\(n=10\\) people volunteer to share their data with the researchers from which we have 10 different pairs of birth weights (in grams) summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225\n\n\n\n\nQuestion 10a\n\nResearchers are testing to see if the birth weight of babies born to a parent that smoked while pregnant is less, on average, compared to a babies whose parent did not smoke while they were pregnant. Write out the null and alternative hypotheses using appropriate notation.\n\nSolution to Question 10a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\n\nQuestion 10b\n\nWhat can we use as the test statistic? What is the value of the observed test statistic?\n\nSolution to Question 10b\n\n\n# data from study\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking births weights\n\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight\n\ndiff &lt;- no - smoker  # differences between matched pairs\n\n# Calculate the observed test statistic\n\n\n\n\n\n\nQuestion 10c\n\nWhen we construct a permutation distribution for the sample mean difference between matched pairs, we want to be sure the resampling we use preserves each pairing.\n\nWe do not randomize how the pairs are formed.\nEach pair of values should remain paired after resampling.\nInstead, we randomly assign values in each pair to the smoker and non-smoker positions.\n\nFor example, our original sample of matched pair differences is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225\n\n\nDifference\n960\n539\n-80\n85\n157\n1125\n14\n331\n199\n610\n\n\n\nOne possible resample is given below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking Resample\n1790\n2920\n3860\n3317\n2282\n2665\n3586\n3487\n2920\n2835\n\n\nSmoked Resample\n2750\n2381\n3940\n3402\n2125\n3790\n3572\n3156\n2721\n2225\n\n\nDifference Resample\n-960\n539\n-80\n-85\n157\n-1125\n14\n331\n199\n610\n\n\n\nTo create a permutation distribution for the sample mean difference between matched pairs, we randomly choose a sign (positive or negative) for each observed matched-pair difference. Complete the code cell below to generate a permutation distribution for the sample mean difference between matched pairs.\n\nSolution to Question 10c\n\nComplete and run the code cell below to create a permutation distribution.\n\nN &lt;- 10^5-1\nperm.match &lt;-numeric(N)\n\n# for each pair, randomly assign the difference to be positive or negative.\n# then calculate the new mean of the paired differences\nfor (i in 1:N)\n{\n  sign &lt;-sample(c(-1,1), size = ??, replace = ??) # random choose a sign -1 or 1\n  diff.resample &lt;- sign * diff\n  perm.match[i] &lt;- ??\n}\n\nThere is nothing to edit in the code cell below. Run the code cell below to plot the permutation distribution and test statistic.\n\n# create a histogram of the permutation distribution\n# and add a vertical line at the observed test statistic\nhist(perm.match,  xlab = \"xbar-diff\",\n     main = \"Permutation Distribution\")\nabline(v = test.match, col =\"red\")\n\n\n\n\n\n\nQuestion 10d\n\nCalculate the p-value of the observed test statistic and interpret its meaning in practical terms.\n\nSolution to Question 10d\n\n\n# compute the p-value"
  },
  {
    "objectID": "22-Permutation-Tests.html#question-11",
    "href": "22-Permutation-Tests.html#question-11",
    "title": "6.2: Permutation Tests",
    "section": "Question 11",
    "text": "Question 11\n\nIs there a difference in the price of groceries sold by Target and Walmart? The data set Groceries in the resampledata package contains a sample of \\(n=24\\) different grocery items and a pair prices (price at Target and price at Walmart) advertised on their respective websites on a specific day.\n\nFirst we load the resampledata package.\n\n\nlibrary(resampledata)  # load resampledata package\n\n\nThen we print the first six rows of the Groceries data.\nNotice this is matched pairs data!\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you received an error when running the code cell below, it is possible you do not have the resampledata package installed. From the R console, run the command install.packages(\"resampledata\") to first install the resampledata packaged. Run the library(resampledata) command in the code cell above again. Then try running the code cell below again.\n\n\n\nhead(Groceries)\n\n                             Product    Size Target Walmart Units UnitType\n1            Kellogg NutriGrain Bars  8 bars   2.50    2.78     8     bars\n2 Quaker Oats Life Cereal  Original     18oz   3.19    6.01    18       oz\n3         General Mills Lucky Charms 11.50oz   3.19    2.98    11       oz\n4          Quaker Oats Old Fashioned    18oz   2.82    2.68    18       oz\n5               Nabisco Oreo Cookies  14.3oz   2.99    2.98    14       oz\n6                 Nabisco Chips Ahoy    13oz   2.64    1.98    13       oz\n\n\n\nNext, we save corresponding Target and Walmart prices to separate vectors target and walmart.\nThe first six values in each vector are printed to the screen.\nNotice the ordering of the values in each vector is very important to preserve.\n\n\ntarget &lt;- Groceries$Target\nwalmart &lt;- Groceries$Walmart\nhead(target)\n\n[1] 2.50 3.19 3.19 2.82 2.99 2.64\n\nhead(walmart)\n\n[1] 2.78 6.01 2.98 2.68 2.98 1.98\n\n\nUsing the sample data stored in target and walmart, answer the questions below to perform a permutation test.\n\nSet up hypotheses to test whether there a difference in the price of groceries sold by Target and Walmart?\nWhat is the observed test statistic?\nCreate a permutation distribution for the sample mean difference between matched pairs.\nCalculate the p-value.\nInterpret the meaning of the p-value.\n\n\nSolution to Question 11\n\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "22-Permutation-Tests.html#footnotes",
    "href": "22-Permutation-Tests.html#footnotes",
    "title": "6.2: Permutation Tests",
    "section": "",
    "text": "Gneezy, U., E. Haruvy, and H. Yafe (2004), “The Inefficiency of Splitting the Bill”, The Economic Journal 114.↩︎\nGlance, N., and B. Huberman, “The Dynamics of Social Dilemmas”, Scientific American↩︎\nAmerican Academy of Dermatology, https://www.aad.org/media/stats-skin-cancer.↩︎"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html",
    "href": "23-Hypothesis-Single-Population.html",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "",
    "text": "What is Significant Enough?\nThe general process form performing a hypothesis test is informally:\nWe have discussed Steps 1 and 2 and used resampling methods as one method to calculate p-values in Step 3. Refer to Appendix A for a summary of the steps outlined above. Before investigating parametric methods for computing p-values, let’s discuss steps and 4 and 5:\nAs with confidence intervals, when estimating an unknown population mean, we often do not know the population variance. Nevertheless, we can still conduct a hypothesis test on a single mean, but there will be some additional uncertainty due to our need to estimate \\(\\sigma^2\\). Below we work through an example using the storms data frame in the dplyr package to devise a method for computing p-values under these circumstances.\nIf we are performing a two-tailed hypothesis test using a significance level \\(\\alpha=0.05\\), then we can reject the null hypothesis if either:\nWe can adjust the statements above for a hypothesis test performed at other significance levels. For example, if we are conducting a two-tailed test at a 1% significance level, we can use 99% confidence interval instead of calculating a p-value.\nThe plots below require the package ggplot2 that is loaded in the code cell below. Be sure to first run the code cell below to load ggplot2 before running any of the code cells that follow.\nlibrary(ggplot2)"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#the-significance-level",
    "href": "23-Hypothesis-Single-Population.html#the-significance-level",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "The Significance Level",
    "text": "The Significance Level\n\nThe significance level of a test, denoted \\({\\color{dodgerblue}{\\alpha}}\\), is the value we choose that is used to determine whether the p-value is small enough to claim the result is statistically significant and reject \\(H_0\\).\n\nIf p-value \\(\\leq \\alpha\\), we reject \\(H_0\\).\nIf p-value \\(&gt; \\alpha\\), we do not reject \\(H_0\\).\n\nGenerally speaking, \\(H_0\\) is a claim we currently accept as true. \\(H_a\\) is some new and interesting result that if true would contradict the currently accepted belief in \\(H_0\\). We typically require compelling evidence, beyond a “reasonable doubt”, to reject the currently accepted claim in \\(H_0\\) in favor of a new and competing claim in \\(H_a\\).\n\nThe default significance level is typically 5% (or \\(\\alpha = 0.05\\)).\nSome other (less) common significance levels are \\(\\alpha = 0.1\\), \\(0.01\\) or \\(0.001\\).\nThe smaller we set \\(\\alpha\\), the more certainty we require to reject \\(H_0\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe significance level is not something we compute. We choose the significance level for the test, and the significance level should be determined prior to our analysis. Do not first calculate the p-value, and then retroactively choose the significance level to ensure the result is significant."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#summarizing-the-results",
    "href": "23-Hypothesis-Single-Population.html#summarizing-the-results",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Summarizing the Results",
    "text": "Summarizing the Results\n\nThere are two possible results with hypothesis tests:\n\nIf p-value \\(\\leq \\alpha\\), the test is statistically significant.\n\nThere is strong enough evidence to reject \\(H_0\\).\nAnd thus, we accept the competing claim in \\(H_a\\).\n\nIf p-value \\(&gt; \\alpha\\), the test is not statistically significant and there is not sufficient evidence to reject \\(H_0\\):\n\nWe fail to reject \\(H_0\\) (which is different from accepting \\(H_0\\)).\nThe test is inconclusive regarding the claims in \\(H_0\\) and \\(H_a\\).\n\n\nIn the end, we want to be sure we communicate the results clearly, in proper context, to a more general audience that may not have an advanced background in statistics and mathematics."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-1",
    "href": "23-Hypothesis-Single-Population.html#question-1",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 1",
    "text": "Question 1\n\n\n\n\nCredit: Kelvinsong, CC BY 3.0, via Wikimedia Commons\n\n\nPressure is a common measurement used to characterize the strength of a storm. The lower the storm pressure, the higher the wind speeds, and the more dangerous the storm. Let \\(\\mu\\) denote the mean pressure (in millibars) of all storms in the North Atlantic. Suppose we set up the following hypothesis to test claims about the value of \\(\\mu\\).\n\n\\(H_0\\): \\(\\mu = 950\\). The mean pressure of all storms in the North Atlantic is 950 millibars.\n\\(H_a\\): \\(\\mu \\ne 950\\). The mean pressure of all storms in the North Atlantic is not 950 millibars.\n\nWe collect a random sample of \\(n\\) storm pressure observation. We find the sample mean pressure \\(\\bar{x} = 992\\) millibars has a p-value \\(= 0.012\\).\n\nQuestion 1a\n\nSummarize the results if we perform the hypothesis test using a 5% significance level. Be sure to explain in the context of the example using terminology a more general audience would understand.\n\nSolution to Question 1a\n\n\n \n\n\n\nQuestion 1b\n\nSummarize the results if we perform the hypothesis test using a 10% significance level. Be sure to explain in the context of the example using terminology a more general audience would understand.\n\nSolution to Question 1b\n\n\n \n\n\n\nQuestion 1c\n\nSummarize the results if we perform the hypothesis test using a 1% significance level. Be sure to explain in the context of the example using terminology a more general audience would understand.\n\nSolution to Question 1c\n\n\n \n\n\n\nQuestion 1d\n\nSuppose we instead we want to show the mean storm pressure is greater than 950 millibars.\n\n\\(H_0\\): \\(\\mu = 950\\). The mean pressure of all storms in the North Atlantic is 950 millibars.\n\\(H_a\\): \\(\\mu &gt; 950\\). The mean pressure of all storms in the North Atlantic is greater than 950 millibars.\n\nWe still have the sample of the same size \\(n\\) and sample mean \\(\\bar{x} = 992\\) millibars as in Question 1. For the two-tailed test, this sample has a p-value \\(= 0.012\\).\n\nWhat would be the p-value for this same sample be if we use this one-tailed test instead?\nSummarize the result of the one-tailed test in practical terms if we use a significance level of 5%.\nSummarize the result of the one-tailed test in practical terms if we use a significance level of 1%.\n\n\nSolution to Question 1d"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-2",
    "href": "23-Hypothesis-Single-Population.html#question-2",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n\nCredit: Mbz1, CC BY-SA 3.0, via Wikimedia Commons\n\n\nIn the 2010 World Cup in Germany, Paul the octopus (aka the Oracle Octopus) correctly predicted the correct outcome in all 8 of the matches he predicted. Paul beat out his rival Mani, Singapore’s psychic parakeet, who predicted six matches in a row before missing a prediction. Below is an excerpt from a recent article1 about animal clairvoyance and the World Cup.\n\nNo FIFA World Cup would be complete without “psychic” animals predicting the winners, and Qatar 2022 has been no exception. From “clairvoyant” camels to “mystic” elephants and “cryptic” rats, a range of animals – big and small – have tried their paws, hooves and tentacles at predicting the score line.   It all started with Paul, the “psychic” octopus. The eight-tentacled icon put TV pundits to shame with an incredible string of correct World Cup winner predictions from his glass tank at the Aquarium Sea Life Centre in Oberhausen, Germany. The tentacled tipster had an incredible success rate: he correctly predicted eight world cup matches at South Africa’s tournament in 2010, including Spain beating the Netherlands in the World Cup final.\n\nIn 2010, Paul the Octopus “predicted” 8 matches and made 8 correct predictions. Is this evidence that Paul actually has psychic powers?\n\nQuestion 2a\n\nSet up the null and alternative hypotheses in terms of the proportion of all World Cup matches ever played that Paul correctly predicts. State hypotheses both in words and using appropriate notation.\n\nSolution to Question 2a\n\n\n\n\n\n\n\nQuestion 2b\n\nHow unusual would this be if Paul was just randomly guessing? Compute the p-value corresponding to this sample of \\(n=8\\) predictions all of which are correct.\n\nSolution to Question 2b\n\n\n\n\n\n\n\nQuestion 2c\n\nSummarize the result of the test to a general audience if a 5% significance level is chosen.\n\nSolution to Question 2c"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#p-value-for-a-single-proportion",
    "href": "23-Hypothesis-Single-Population.html#p-value-for-a-single-proportion",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "p-value for a Single Proportion",
    "text": "p-value for a Single Proportion\n\nLet \\(X\\) be a binomial random variable and let \\(p_0\\) denote the value of \\(p\\) claimed in \\(H_0\\). If we observe \\(X=x\\) successes out of a sample of \\(n\\) independent and identical trials, then we can find the p-value using the binomial null distribution \\({\\color{dodgerblue}{X \\sim \\mbox{Binom}(n, {\\color{tomato}{p_0}})}}\\)."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-3",
    "href": "23-Hypothesis-Single-Population.html#question-3",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 3",
    "text": "Question 3\n\nThe mean height of all adult males in the United Kingdom is claimed2 to be the \\(68.5\\) inches (\\(5\\) foot \\(8.5\\) inches or \\(173.9\\) cm) with a standard deviation of \\(2.5\\) inches (or \\(6.35\\) cm). A physician suspects males in her town seem to be taller than average when compared to the population of all adult males in the UK. She collects data from a random sample of \\(n=25\\) adult males from the town and calculates the mean height of the sample is \\(69.25\\) inches.\n\nQuestion 3a\n\nSet up the null and alternative hypotheses (both in words and using appropriate notation) to test the physician’s claim that adult males in the town are taller than the national average height for all adult males in the UK.\n\nSolution to Question 3a\n\n\n\\(H_0\\): ??\n\\(H_a\\): ??\n\n\n\n\n\n\nQuestion 3b\n\nCompute the test statistic.\n\nSolution to Question 3b\n\n\n\n\n\n\n\nQuestion 3c\n\nWhat is a reasonable null distribution to use to perform this test? Standardize the test statistic (give the \\(z\\)-score) from Question 3b. Interpret the meaning of the standardized test statistic.\n\nSolution to Question 3c\n\n\n\n\n\n\n\nQuestion 3d\n\nCompute the p-value and interpret the meaning in practical terms.\n\nSolution to Question 3d\n\n\n\n\n\n\n\nQuestion 3e\n\nShade area(s) under the graph of a null distribution corresponding to the p-value. Either make an informal sketch on paper or see Appendix B to plot in R.\n\nSolution to Question 3e\n\n\n\n\n\n\n\nQuestion 3f\n\n\nIf a 5% significance level is chosen, summarize the result in practical terms.\nIf a 10% significance level is chosen, summarize the result in practical terms.\n\n\nSolution to Question 3f"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-known-sigma2",
    "href": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-known-sigma2",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "p-value for a Single Mean: Known \\(\\sigma^2\\)",
    "text": "p-value for a Single Mean: Known \\(\\sigma^2\\)\n\nSuppose a random sample size \\(n\\) is picked from a population with known population variance \\(\\sigma^2\\) but unknown mean \\(\\mu\\). If we are doing a hypothesis test on a single mean with null claim \\({\\color{tomato}{H_0: \\mu = \\mu_0}}\\), then as long as the population is symmetric or the sample size is large enough \\((n \\geq 30)\\), we can use the Central Limit Theorem for means to:\n\nModel the null distribution with the sampling distribution \\({\\color{dodgerblue}{\\overline{X} \\sim N \\left( {\\color{tomato}{\\mu_0}}, \\frac{\\sigma}{\\sqrt{n}} \\right)}}\\).\nCalculate the standardized test statistic which is the \\(z\\)-score of the sample mean:\n\n\\[{\\color{dodgerblue}{z = \\frac{\\mbox{sample stat}- {\\color{tomato}{\\mbox{null claim}}}}{\\mbox{SE}(\\overline{X})} = \\dfrac{\\bar{x} - {\\color{tomato}{\\mu_0}}}{\\frac{\\sigma}{\\sqrt{n}}}}}.\\]"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#picking-a-random-sample-of-storm-pressures",
    "href": "23-Hypothesis-Single-Population.html#picking-a-random-sample-of-storm-pressures",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Picking a Random Sample of Storm Pressures",
    "text": "Picking a Random Sample of Storm Pressures\n\nThe storms data set is from the NOAA Hurricane Best Track Data. We will perform a hypothesis test to test claims about the mean storm pressure, so we will need to analyze the variable pressure.\n\nRun the code cell below to load the dplyr package (which should already be installed).\n\n\nlibrary(dplyr)  # load dplyr package\n\n\nRun the code cell below to pick a random sample of \\(n=32\\) storm pressures from storms.\n\n\nmy.sample &lt;- sample(storms$pressure, size=32, replace=FALSE)"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-4",
    "href": "23-Hypothesis-Single-Population.html#question-4",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 4",
    "text": "Question 4\n\nIt is claimed3 that the average pressure of all North Atlantic storms is 950 millibars. You believe this claim is inaccurate and would like to show the average pressure of all storms is not 950 millibars.\n\nQuestion 4a\n\nSet up the null and alternative hypotheses both in words and using appropriate notation.\n\nSolution to Question 4a\n\n\n\\(H_0\\): ??\n\\(H_a\\): ??\n\n\n\n\n\n\nQuestion 4b\n\nCompute the test statistic.\n\nSolution to Question 4b\n\n\n\n\n\n\n\nQuestion 4c\n\nWhat is a reasonable standardized null distribution to use to perform this test? Standardize the test statistic from Question 4b and interpret its meaning.\n\nSolution to Question 4c\n\n\n\n\n\n\n\nQuestion 4d\n\nCompute the p-value and interpret the meaning in practical terms.\n\nSolution to Question 4d\n\n\n\n\n\n\n\nQuestion 4e\n\nShade area(s) under the graph of a null distribution corresponding to the p-value in Question 4d. Either make an informal sketch on paper or see Appendix B to plot in R.\n\nSolution to Question 4e\n\n\n\n\n\n\n\nQuestion 4f\n\nIf a 5% significance level is chosen, summarize the result in practical terms.\n\nSolution to Question 4f"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-unknown-sigma2",
    "href": "23-Hypothesis-Single-Population.html#p-value-for-a-single-mean-unknown-sigma2",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "p-value for a Single Mean: Unknown \\(\\sigma^2\\)",
    "text": "p-value for a Single Mean: Unknown \\(\\sigma^2\\)\n\nSuppose a random sample size \\(n\\) is picked from a population with unknown population mean and variance. If we are doing a hypothesis test on a single mean with null claim \\(H_0: \\mu = \\mu_0\\), then as long as the population is symmetric or the sample size is large enough \\((n \\geq 30)\\):\n\nThe standardized test statistic is called the t-test statistic:\n\n\\[{\\large \\boxed{{\\color{dodgerblue}{{\\color{tomato}{t}} = \\frac{\\mbox{sample stat}-\\mbox{null claim}}{\\mbox{SE}(\\overline{X})} = \\dfrac{\\bar{x} - \\mu_0}{\\frac{{\\color{tomato}{s}}}{\\sqrt{n}}}}}}}.\\]\n\nThe null distribution is the distribution of t-test statistics that we model using a \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\n\nIn R, we can use the command t.test(x, mu = [null], alt = [direction]).\n\nSample data is stored in the vector x.\nSet the option mu equal to the value, \\(\\mu_0\\), claimed in \\(H_0\\).\nSet the option alt based on the inequality used in \\(H_a\\).\n\nUse \"greater\" for right-tail test.\nUse \"less\" for left-tail test.\nUse \"two.sided\" for a two-tail test.\nIf you do not indicate any alt option, the default is a two-tailed test."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-5",
    "href": "23-Hypothesis-Single-Population.html#question-5",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 5",
    "text": "Question 5\n\nCheck your results for the hypothesis test in Question 4 using the t.test() function.\n\nSolution to Question 5\n\nFill in the options for the t.test() function in the code cell below.\n\nt.test(??)"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#question-6",
    "href": "23-Hypothesis-Single-Population.html#question-6",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Question 6",
    "text": "Question 6\n\nThe output of t.test() gives both a p-value and a 95% confidence interval (by default). Let’s interpret the confidence interval and see if we obtain a result that is consistent with our summary in Question 4f.\n\nQuestion 6a\n\nBased on the output of your code in Question 5, what is a 95% confidence interval for the mean pressure of all North Atlantic storms?\n\nSolution to Question 6a\n\nA 95% confidence interval for the mean pressure of all storms is from ?? millibars to ?? millibars.\n \n\n\n\nQuestion 6b\n\nBased on the 95% confidence interval Question 6a, is 950 millibars (the null claim for \\(\\mu\\) in \\(H_0\\)) a plausible estimate for \\(\\mu\\)? Is this consistent with your answer in Question 4f? Explain why or why not.\n\nSolution to Question 6b"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#sec-appendb-known",
    "href": "23-Hypothesis-Single-Population.html#sec-appendb-known",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Illustrating p-values: Known Population Variance",
    "text": "Illustrating p-values: Known Population Variance\n\nIf we are performing a hypothesis test on a single mean for a population whose variance is known, then we can either use:\n\nThe null distribution is \\({\\color{dodgerblue}{\\overline{X} \\sim N \\left( \\mu_0, \\frac{\\sigma}{\\sqrt{n}} \\right)}}\\) with test statistic is \\(\\bar{x}\\), or\nThe standardized normal distribution \\(Z \\sim N(0,1)\\) with standardized test statistic\n\n\\[{\\color{dodgerblue}{z =  \\dfrac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}}}.\\]\n\nIn the code cell below, enter values for the mean and standard error of the null distribution as well as the test statistic.\n\n\nnull.mean &lt;- ??  # mean of the null distribution\nnull.se &lt;- ??  # standard error of the null distribution\ntest.stat &lt;- ??  # test statistic\n\n\nTwo-Tailed Test: Known Variance\n\nTo illustrate the p-value for a two-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined null.mean, null.se and test.stat.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a two-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(null.mean + 4*null.se, x.test)\nxmin &lt;- -1*xmax\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, x.test)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = c(-x.test, x.test), linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(-x.test,  x.test)) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nLeft-Tailed Test: Known Variance\n\nTo illustrate the p-value for a left-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined null.mean, null.se and test.stat.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a left-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(null.mean + 4*null.se, x.test)\nxmin &lt;- -1*xmax\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, xmax)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_vline(xintercept = -x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=-x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nRight-Tailed Test: Known Variance\n\nTo illustrate the p-value for a right-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined null.mean, null.se and test.stat.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a right-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(null.mean + 4*null.se, x.test)\nxmin &lt;- -1*xmax\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = NA, \n            xlim = c(xmin, x.test)) +\n  geom_area(stat = \"function\",   fun = dnorm, \n            args = list(mean = null.mean, sd = null.se),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))"
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#sec-appendb-unknown",
    "href": "23-Hypothesis-Single-Population.html#sec-appendb-unknown",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "Illustrating p-values: Unknown Population Variance",
    "text": "Illustrating p-values: Unknown Population Variance\n\nIf we are performing a hypothesis test on a single mean for a population whose variance is unknown, then we use \\(\\mathbf{t_{n-1}}\\), a \\(t\\)-distribution with \\(n-1\\) degrees of freedoms, for the null distribution and have t-test statistic\n\\[{\\color{dodgerblue}{{\\color{tomato}{t}} = \\dfrac{\\bar{x} - \\mu_0}{\\frac{{\\color{tomato}{s}}}{\\sqrt{n}}}}}.\\]\n\nIn the code cell below, enter the value of the t-test statistic and the degrees of freedom.\n\n\ntest.stat &lt;- ??  # t-test statistic\ndeg.free &lt;- ??  # degrees of freedom  \n\n\nTwo-Tailed Test: Unknown Variance\n\nTo illustrate the p-value for a two-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined test.stat and deg.free.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a two-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(4, x.test)\nxmin &lt;- -1*xmax\nv &lt;- deg.free\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, x.test)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = c(-x.test, x.test), linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(-x.test,  x.test)) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nLeft-Tailed Test: Unknown Variance\n\nTo illustrate the p-value for a left-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined test.stat and deg.free.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a left-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(4, x.test)\nxmin &lt;- -1*xmax\nv &lt;- deg.free\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = NA, \n            xlim = c(-x.test, xmax)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(xmin, -x.test)) +\n  geom_vline(xintercept = -x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=-x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n\nRight-Tailed Test: Unknown Variance\n\nTo illustrate the p-value for a right-tailed test:\n\nBe sure you have already loaded the ggplot2 package and defined test.stat and deg.free.\nRun the code cell below. No edits are needed.\n\n\n################################################\n# for a right-tailed test run the code cell below\n################################################\nx.test &lt;- abs(test.stat)\nxmax &lt;- max(4, x.test)\nxmin &lt;- -1*xmax\nv &lt;- deg.free\n\n\nggplot(NULL, aes(c(-xmin, xmax))) + \n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = NA, \n            xlim = c(xmin, x.test)) +\n  geom_area(stat = \"function\",   fun = dt, \n            args = list(df = v),\n            color = \"black\", fill = \"firebrick2\", \n            xlim = c(x.test, xmax)) +\n  geom_vline(xintercept = x.test, linetype=\"dashed\", \n                color = \"firebrick2\", linewidth = 1) + \n  labs(x = \"test statistic\", y = \"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=x.test) + \n  geom_hline(yintercept=0) + \n  theme_bw() + \n  theme(axis.text.x=element_text(size=15, color = \"firebrick2\"))\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "23-Hypothesis-Single-Population.html#footnotes",
    "href": "23-Hypothesis-Single-Population.html#footnotes",
    "title": "6.3: Parametric Hypothesis Tests",
    "section": "",
    "text": "“The ‘psychic’ animals predicting who will win the World Cup”, ABC News, accessed July 13, 2023.↩︎\n“Height, Weight, and Body Mass of the British Population Since 1820” by Roderick Floud, National Bureau of of Economic Research, October 1998.↩︎\nThe University of Arizona Hydrology and Atmospheric Sciences, accessed July 13, 2023.↩︎"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html",
    "href": "24-Hypothesis-Comparing-Two.html",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "",
    "text": "Questions that Compare Two Populations\nIn many cases, the explanatory variable of interest in our question has more than just two levels. We can apply hypothesis tests on two populations to compare two levels of the explanatory variable. Often the researchers have a goal in mind, and they choose which two levels they would like to compare and/or how the levels are defined. Another group of researchers might approach the same statistical question from a different perspective and choose a different set up.\nWhen doing a hypothesis test for a difference between population means or two population proportions, we apply the same steps as with previous tests. See Appendix A for a summary of the steps below.\nIn our previous work with hypothesis tests on two populations, we used two-sample permutations tests to find standardized test statistics and p-values in steps 2 and 3. We now devise parametric methods for hypothesis tests that compare two populations.\nIf we are doing a test on a difference of two proportions, then the null claim is \\(H_0\\): \\(p_1 - p_2=0\\). We use the difference in sample proportions, \\({\\color{dodgerblue}{\\hat{p}_1 - \\hat{p}_2}}\\), as the test statistic:\n\\[{\\color{dodgerblue}{\\mbox{stand test stat} = \\frac{ (\\mbox{diff in sample stats}) - ({\\color{tomato}{\\mbox{diff in null claim}}})}{\\mbox{SE of Null Dist}} = \\frac{ (\\hat{p}_1 - \\hat{p}_2) - {\\color{tomato}{0}}}{\\mbox{SE of Null Dist}}}}.\\]\nFrom the CLT for a difference in proportions we have\n\\[\\widehat{P}_1 - \\widehat{P}_2  \\sim N \\left( {\\color{tomato}{p_1 - p_2}} , {\\color{dodgerblue}{\\mbox{SE}}} \\right) = N \\left( {\\color{tomato}{0}} , {\\color{dodgerblue}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}} } \\right).\\]"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-1",
    "href": "24-Hypothesis-Comparing-Two.html#question-1",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 1",
    "text": "Question 1\n\nSuppose the explanatory variable for statistical questions Q1, Q2, and Q3 are:\n\nQ1: Medication Type\nQ2: Gender\nQ3: Latitude\n\nIn the questions below, set up null and alternative hypothesis (in words and symbolically) to perform a hypothesis test to answer the statistical question.\n\nQuestion 1a\n\nQ1: Is a new medication more effective at preventing disease than current treatments?\nIn general, we might be interested in comparing several treatments. For example, the variable Medication Type could have four different levels: Treatment A, Treatment B, Treatment C, and No Treatment. Within our hypothesis test framework, we can compare two different treatments types. Set up hypotheses for the following question:\n\nDoes xylitol (a sweetener) reduce ear infections in children compared to no treatment (control)?\n\n\nSolution to Question 1a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 1b\n\nQ2: Is there a gender pay gap in the US?\nThe explanatory variable Gender may be gathered using different methods: free response question, multiple choice question (with various ways of defining the choices), or pulled directly from available records. Suppose we would like to focus on identifying if there is a pay gap between men and women that have just completed a bachelor’s degree in the US. Set up hypothesis for the following question:\n\nDo women that have recently completed a bachelor’s degree in the US get paid less, on average, compared to men that have recently completed a bachelor’s degree?\n\n\n\n\n\n\n\nNote\n\n\n\nThe median is often used as the measure of center to summarize income since the distribution of income is typically heavily skewed. Outliers will have a bigger influence in the analysis if we compare means. With the median, outliers have as much influence as any other observation. However, there is no parametric method for testing claims about medians.\n\n\n\nSolution to Question 1b\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 1c\n\nQ3: Is there a region in the North Atlantic where storm strength is greatest?\nThe explanatory variable Latitude is typically a quantitative variable given in degrees north or south of the equator. There are many different ways a researcher can define different regions, and how they define those regions depends on the data available and their research goals. Set up hypothesis for the following question:\n\nIs the storm pressure different, on average, for storms in the North Atlantic above the Tropic of Cancer compared to those below the Tropic of Cancer?\n\n\nSolution to Question 1c\n\n\n\\(H_0\\):\n\\(H_a\\):"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#a-general-test-statistic",
    "href": "24-Hypothesis-Comparing-Two.html#a-general-test-statistic",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "A General Test Statistic",
    "text": "A General Test Statistic\n\nWe can apply the Central Limit Theorem, CLT, to standardize the test statistic and calculate p-values using the same strategy as the parametric approach for testing claims about a single population. We begin by assuming the claim in the null hypothesis is true:\n\n\\(H_0\\): There is no difference. For example, \\(\\mu_1 - \\mu_2 =0\\) or \\(p_1 -p_2 = 0\\).\n\nThe standardized test statistic is the \\(z\\)-score or t-test statistic corresponding to the observed difference in sample statistics:\n\\[{\\color{dodgerblue}{\\mbox{stand test stat} = \\frac{ (\\mbox{diff in sample stats}) - ({\\color{tomato}{\\mbox{diff in null claim}}})}{\\mbox{SE of Null Dist}} = \\frac{ (\\mbox{diff in sample stats}) - {\\color{tomato}{0}}}{\\mbox{SE of Null Dist}}}}.\\]\nTo calculate the standardized test statistic, we consider the following questions:\n\nDo the samples satisfies conditions for CLT?\nWhat is the correct probability distribution to use, a t-distribution or a normal distribution?\nWhat is the difference in observed sample statistics?\nWhat is an estimate for the standard error of the null distribution?"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-2",
    "href": "24-Hypothesis-Comparing-Two.html#question-2",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 2",
    "text": "Question 2\n\nIf we are doing a test on a difference of means from two independent populations, then the null claim is \\(H_0\\): \\(\\mu_1 - \\mu_2=0\\) and recall the CLT for a difference in means is\n\\[\\overline{X}_1 - \\overline{X}_2  \\sim N \\left( {\\color{tomato}{\\mu_1 - \\mu_2}} , {\\color{dodgerblue}{\\mbox{SE}}} \\right) = N \\left( {\\color{tomato}{0}} , {\\color{dodgerblue}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} } \\right).\\] Suppose we are performing a test on the difference of two independent means, \\(\\mu_1 - \\mu_2\\), and the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown.\n\nWhat are reasonable estimates to use in place of \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)?\nWhat is a formula for estimating the standard error of the null distribution?\nShould we use a t-distribution or normal distribution to approximate the null distribution?\n\nBased on your answers to the previous questions, fill in the blanks label (i), (ii), (iii), and (iv) to give a formula for standardized test statistic. In blank (i), enter either \\(z\\) or \\(t\\) depending on whether a normal or t-distribution is appropriate.\n\\[{\\color{dodgerblue}{\\mbox{(i)} = \\frac{ (\\mbox{diff in sample stats}) - ({\\color{tomato}{\\mbox{diff in null claim}}})}{\\mbox{SE of Null Dist}} = \\frac{ \\mbox{(ii)} - {\\color{tomato}{\\mbox{(iii)}}}}{\\mbox{(iv)}}}}.\\]\n\nSolution to Question 2\n\nComplete the formula for the standardized test statistic:\n\nChoose either \\(z\\) or \\(t\\) for blank (i).\nBlank (ii) is ??\nBlank (iii) is ??\nBlank (iv) is ??"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-3",
    "href": "24-Hypothesis-Comparing-Two.html#question-3",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 3",
    "text": "Question 3\n\nWe return to our statistical question and hypothesis from Question 1b regarding a possible gender pay gap in the US. We collect data from a random sample of people that completed their bachelor’s degree in 2021 and compare the starting salaries of men and women in the sample. A sample of 154 women have a mean income of \\(\\$52,\\!266\\) with standard deviation \\(\\$15,\\!510\\) A sample of 122 men have a mean income of \\(\\$64,\\!022\\) with a standard deviation of \\(\\$16,\\!890\\).\nIs there enough evidence to support the claim that women who have recently completed a bachelor’s degree in the US get paid less, on average, compared to men who have recently completed a bachelor’s degree? Test using a significance level of \\(\\alpha=0.05\\).\n\n\n\n\n\n\nNote\n\n\n\nThe statistical question in this example as well as the data are motivated by an analysis of students that earned a bachelor’s degree in the US in 2021 by the National Association of Colleges and Employers (NACE).\n\n\n\nSolution to Question 3\n\nUsing the hypotheses you set up in Question 1b, answer the following:\n\nBased on the sample data, compute the standardized test statistic.\n\n\n# calculate standardized test statistic\n\n\n\n\nBased on your standardized test statistic, compute the p-value.\n\n\n# compute p-value\n\n\n\n\nMake a decision and summarize the results in the context of this example."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#using-t.test-to-compare-two-populations",
    "href": "24-Hypothesis-Comparing-Two.html#using-t.test-to-compare-two-populations",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Using t.test() to Compare Two Populations",
    "text": "Using t.test() to Compare Two Populations\n\nTo calculate a p-value for a test on the difference of means from two independent populations with unknown variances, we can calculate the t-test statistic\n\\[{\\boxed{\\color{dodgerblue}{{\\color{tomato}{t}} = \\frac{ \\mbox{diff in sample stats} - 0}{\\mbox{SE of Null Dist}} = \\dfrac{(\\bar{x}_1 - \\bar{x}_2) - 0 }{\\sqrt{\\frac{{\\color{tomato}{s_1}}^2}{n_1} + \\frac{{\\color{tomato}{s_2}}^2}{n_2}}}}}}.\\]\nIn Question 3, you were conveniently provided numerical summaries of each sample that we substituted into the formula for the t-test statistic. Then we can err on the side caution and choose the degrees of freedom, \\(df\\), to be the smaller of either \\(\\mathbf{n_1-1}\\) or \\(\\mathbf{n_2-1}\\).\nIf we have access to all the sample data (not just summaries), then we can load the sample data into R and use the t.test() function to calculates both the t-test statistic and p-value using a more accurate approximation for \\(df\\) obtained using Welch’s approximation.\n\nSamples Stored as Separate Vectors\n\nIn R, we can use the command t.test(x1, x2, alt = [direction]).\n\nSamples are stored in independent vectors x1 and x2.\nSet the option alt based on the inequality used in \\(H_a\\).\n\nUse \"greater\" for right-tail test.\nUse \"less\" for left-tail test.\nUse \"two.sided\" for a two-tail test.\nIf you do not indicate any alt option, the default is a two-tailed test.\n\nThe default is mu=0 which is exactly what we want, so we do not specify this option.\n\n\n\nSubsetting Samples Inside t.test()\n\nFrequently, we would like to compare the means of a quantitative response variable, denoted quant, for two different levels of a categorical explanatory variable, denoted categorical, in the data frame named data.name. The t.test() function can simultaneously subset the quant data into two independent samples (one for each level of category) and then perform a hypothesis test for the difference in two means:\n\nt.test(quant ~ categorical, data = data.name, alt = [direction])\n\n\n\n\n\n\n\nCaution\n\n\n\nIf the categorical variable has exactly two levels, then the command t.test(quant ~ categorical, data = data.name, alt = [direction]) works as desired. However, if categorical has more than two levels, this results in an error. If there are more than two levels, you can first subset or filter the data and then use t.test()."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-4",
    "href": "24-Hypothesis-Comparing-Two.html#question-4",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 4",
    "text": "Question 4\n\nWe use the storms data set from the NOAA Hurricane Best Track Data in the dplyr package to test if the storm pressure is different, on average, for storms in the North Atlantic above the Tropic of Cancer compared to those below the Tropic of Cancer.\n\nRun the code cell below to load the dplyr package (which should already be installed).\n\n\nlibrary(dplyr)  # load dplyr package\n\n\nQuestion 4a\n\nRun the code cell below to calculate the p-value of the sample data in storms. Interpret the meaning of the output.\n\nt.test(pressure ~ lat, data = storms, alt = \"two.tail\")\n\n\nSolution to Question 4a\n\n\n\n\n\n\n\nQuestion 4b\n\nThe Tropic of Cancer is the most northerly circle of latitude on Earth at which the Sun can be directly overhead. The position of this circle is constantly wobbling just slightly. The Tropic of Cancer is currently located at \\(23.43623^{\\circ}\\) north of the Equator.\n\nCreating Levels for the Explanatory Variable with mutate()\n\nThe code cell use the mutate() function that is in the dplyr package to create and add a new categorical variable, we name tropic, consisting of two levels (above and below) depending on whether the storm’s location is above or below the Tropic of Cancer. Then the tapply() function is used to calculate separate means for the sample of storms above and below the Tropic of Cancer.\n\n\n\n\n\n\nNote\n\n\n\nThe data frame storms and mutate() script are both in the dplyr package. Be sure you have already run library(dplyr) before running the code cell below.\n\n\n\n# create and add new variable tropic to storms\nstorms &lt;- mutate(storms, \n                 tropic = case_when(lat &lt; 23.43623 ~ 'below',  # below is assigned if lat &lt; 23.43623\n                                    lat &gt; 23.43623 ~ 'above'))  # above is assigned if lat &gt; 23.43623\n\nstorms$tropic &lt;- factor(storms$tropic)  # ensure tropic stored as categorical factor\ntapply(storms$pressure, storms$tropic, mean)  # compare mean pressures\n\nAfter running the code cell above, complete the code cell below to create side-by-side box plots of the distributions of storm pressures above and below the Tropic of Cancer.\n\n\nSolution to Question 4b\n\nReplace each ?? with an appropriate variable or data frame name.\n\nboxplot(?? ~ ??, data = ??)\n\n\n\n\n\n\nQuestion 4c\n\nFill in the t.test() function below to compute the test statistic and corresponding p-value based on the sample data in storms to test the hypothesis in Question 1c\n\nSolution to Question 4c\n\nComplete and run the code cell below.\n\nt.test(?? ~ ??, data = ??, alt = ??)\n\n\n\n\n\n\nQuestion 4d\n\nBased on the output in Question 4c, summarize the result of this test in practical terms. Use a significance level of 5%.\n\nSolution to Question 4d"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-5",
    "href": "24-Hypothesis-Comparing-Two.html#question-5",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 5",
    "text": "Question 5\n\nWhen doing a test on the difference of two proportions, the null hypothesis is the claim \\(p_1 - p_2 = 0\\) or \\(p_1 = p_2\\).\n\nWe have not claimed anything about the value of \\(p_1\\) on its own.\nWe have not claimed anything about the value of \\(p_2\\) on its own.\n\nWhat is a reasonable estimate to plug in place of \\(p_1\\) and \\(p_2\\) in the formula for the standard error?\n\nSolution ot Question 5"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#defining-a-test-statistic",
    "href": "24-Hypothesis-Comparing-Two.html#defining-a-test-statistic",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Defining a Test Statistic",
    "text": "Defining a Test Statistic\n\nLet \\(X_1\\) and \\(X_2\\) denote the number of successes in samples 1 and 2 with sizes \\(n_1\\) and \\(n_2\\), respectively.\n\nWe use a normal distribution to model the null distribution (as long as both samples are large enough).\nIn \\(H_0\\), we assume there is no difference in the two groups, so we pool the samples together and calculate the sample pooled proportion:\n\n\\[{\\color{tomato}{\\mbox{sample pooled proportion} = \\hat{p}_p = \\frac{X_1 + X_2}{n_1 + n_2}}}.\\]\n\nTo estimate the standard error, we use \\({\\color{tomato}{\\hat{p}_p}}\\) in place of both \\(p_1\\) and \\(p_2\\).\nThe standardized test statistic is the z-score of the difference in sample proportions:\n\n\\[\\boxed{{\\color{dodgerblue}{z = \\dfrac{ (\\mbox{diff in sample stats}) - (\\mbox{diff in null claim})}{\\mbox{SE of Null Dist}}= \\dfrac{(\\hat{p}_1 - \\hat{p}_2) - 0 }{\\sqrt{{\\color{tomato}{\\hat{p}_p}}(1 -{\\color{tomato}{\\hat{p}_p}})\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}}}}.\\]"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#calculating-the-p-value",
    "href": "24-Hypothesis-Comparing-Two.html#calculating-the-p-value",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Calculating the p-Value",
    "text": "Calculating the p-Value\n\nWhen doing a hypothesis test on a difference of two proportions, we use a normal distribution to estimate the null distribution and calculate p-values. If we denote the z-score of the observed difference in sample means as z, then:\n\nFor a left-tailed test, p-value = pnorm(z, 0, 1)\nFor a right-tailed test, p-value = 1 - pnorm(z, 0, 1)\nFor a two-tailed test, p-value = 2 * pnorm(-1*abs(z), 0 , 1)"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-6",
    "href": "24-Hypothesis-Comparing-Two.html#question-6",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 6",
    "text": "Question 6\n\n\n\n\nCredit: Nick Youngson CC BY-SA 3.0 via Pix4free.org\n\n\nIt is believed that a sweetener called xylitol helps prevent ear infections. In a randomized experiment1 \\(n_1 = 165\\) children took a placebo and \\(68\\) of them got ear infections. Another \\(n_2 = 159\\) children took xylitol and \\(46\\) of them got ear infections. We believe that the proportion of ear infections in the placebo group will be greater than the xylitol group. Test this claim using \\(\\alpha=0.01\\).\n\nSolution to Question 6\n\nUse hypotheses from Question 1a and answer the questions below.\n\n\nEstimate the standard error of the null distribution.\n\n\n# estimate of SE\n\n\n\n\nBased on your answer to (a), use the parametric formula to find the z-score of the observed difference in sample proportions.\n\n\n# calculate standardized test statistic\n\n\n\n\nBased on your answer in (b), compute the p-value.\n\n\n# calculate p-value\n\n\n\n\nMake a decision and summarize the results in the context of this example."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#applying-a-continuity-correction",
    "href": "24-Hypothesis-Comparing-Two.html#applying-a-continuity-correction",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Applying a Continuity Correction",
    "text": "Applying a Continuity Correction\n\n\nThe CLT approximation uses a continuous normal distribution to estimate a discrete distribution.\nWe can improve the approximation using a continuity correction that we saw earlier.\nAdd or subtract \\(0.5\\) from each success count so the difference in proportions becomes closer to 0, and thus gives a larger p-value."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-7",
    "href": "24-Hypothesis-Comparing-Two.html#question-7",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 7",
    "text": "Question 7\n\nRedo the solution to Question 6 by applying a continuity correction.\n\nSolution to Question 7\n\n\nStandard error is the same as in Question 6\n\n\n\nBased on the SE in Question 6, use the parametric formula along with a continuity correction to find the z-score of the observed difference in sample proportions.\n\n\n# calculate standardized test statistic\n\n\n\n\nBased on your answer in (b), compute the p-value.\n\n\n# calculate p-value\n\n\n\n\nDoes the p-value in (c) lead to a different result than in Question 6? Explain."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#using-the-prop.test-function",
    "href": "24-Hypothesis-Comparing-Two.html#using-the-prop.test-function",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Using the prop.test() Function",
    "text": "Using the prop.test() Function\n\nIf one sample has size \\(n_1\\) with \\(X_1\\) successes and the other sample has size \\(n_2\\) with \\(X_2\\) successes, then we can use the prop.test() function in R:\n\nprop.test(c(X1, X2), c(n1, n2), alt = [direction], correct = [option])\n\n\n\nThe alt option works the same as with t.test().\nWe can turn the continuity correction on or off with the correct option.\n\nUse correct = TRUE to apply a continuity correction.\nUse corret = FALSE if you do not want to apply a correction.\nIf you do not indicate any correct option, the default is TRUE."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-8",
    "href": "24-Hypothesis-Comparing-Two.html#question-8",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 8",
    "text": "Question 8\n\nUse the prop.test() function to verify your work in Question 6 and Question 7.\n\nSolution to Question 8\n\n\n# verify answers to question 6\nprop.test(??, ??, alt = ??, correct = ??)\n\n\n# verify answers to question 7\nprop.test(??, ??, alt = ??, correct = ??)"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-9",
    "href": "24-Hypothesis-Comparing-Two.html#question-9",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 9",
    "text": "Question 9\n\nIn this example, we use data collected from a matched pair designed study to determine whether smoking during pregnancy is associated with lower birth weight. In our study, we solicit volunteers that have already given birth to two babies. During one of the pregnancies, the parent smoked. During the other pregnancy, they did not smoke. Below is hypothetical data from such a study. A sample of \\(n=10\\) people volunteer to share their data with the researchers from which we have 10 different pairs of birth weights (in grams) summarized in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nNo Smoking\n2750\n2920\n3860\n3402\n2282\n3790\n3586\n3487\n2920\n2835\n\n\nSmoked\n1790\n2381\n3940\n3317\n2125\n2665\n3572\n3156\n2721\n2225\n\n\n\n\n# data from study\nno &lt;- c(2750, 2920, 3860, 3402, 2282, \n        3790, 3586, 3487, 2920, 2835)  # non-smoking births weights\n\nsmoker &lt;- c(1790, 2381, 3940, 3317, 2125, \n            2665, 3572, 3156, 2721, 2225)  # matching smoking birth weight\n\ndiff &lt;- no - smoker  # vector of matched pair differences\n\nResearchers will perform a hypothesis test to determine if the birth weight of babies born to a parent that smoked while pregnant is less, on average, compared to a babies whose parent did not smoke while they were pregnant. Test the claim using a 5% significance level.\n\nSolution to Question 9\n\n\nSet up the hypothesis for the test both in words and using appropriate notation.\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\nUse the data stored in no, smoker, and/or diff to calculate a reasonable test statistic.\n\n\n# compute a test statistic\n\n\n\n\n\nCalculate the p-value of the observed sample.\n\n\n# compute p-value\n\n\nWhat is the conclusion? Summarize the result in the context of this example."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#question-10",
    "href": "24-Hypothesis-Comparing-Two.html#question-10",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Question 10",
    "text": "Question 10\n\nThe data set storms in the dplyr package is summarized below.\n\nlibrary(dplyr)  # package may already be loaded\nsummary(storms)\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512            \n\n\n\nQuestion 10a\n\nWrite one possible claim that could be tested by a hypothesis test for a difference in two means. Then write the corresponding hypotheses and calculate the p-value of the observed samples using the t.test() function. What is the result of your test?\n\nSolution to Question 10a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \nt.test(??)\n\n\nSummarize Result:\n\n\n\n\n\nQuestion 10b\n\nWrite one possible claim that could be tested by a hypothesis test for a difference in two proportions. Then write the corresponding hypotheses and calculate the p-value of the observed samples using the prop.test() function. What is the result of your test?\n\nSolution to Question 10b\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \nprop.test(??)\n\n\nSummarize Result:\n\n\n\n\n\nQuestion 10c\n\nWrite one possible claim that could be tested by a hypothesis test for a single mean. Then write the corresponding hypotheses and calculate the p-value of the observed samples using the t.test() function. What is the result of your test?\n\nSolution to Question 10c\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \nt.test(??)\n\n\nSummarize Result:\n\n\n\n\n\nQuestion 10d\n\nWrite one possible claim that could be tested by a hypothesis test for a single proportion. Then write the corresponding hypotheses and calculate the p-value of the observed sample using a binomial distribution. What is the result of your test?\n\nSolution to Question 10d\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n# find p-value \n\n\nSummarize Result:"
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#sec-appenda",
    "href": "24-Hypothesis-Comparing-Two.html#sec-appenda",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Appendix A: Summary of Hypothesis Testing",
    "text": "Appendix A: Summary of Hypothesis Testing\n\n\nState the hypotheses and identify (from the alternative claim in \\(H_a\\)) if it is a one or two-tailed test.\n\n\\(H_0\\) is the “boring” claim. Express using an equal sign \\(=\\).\n\\(H_a\\) is the claim we want to show is likely true. Use inequality sign (\\(&gt;\\), \\(&lt;\\), or \\(\\ne\\)).\nState both \\(H_0\\) and \\(H_a\\) in terms of population parameters such as \\(\\mu_1-\\mu_2\\) and \\(p_1-p_2\\).\n\nCompute the test statistic.\n\nIf the observed sample contradicts the null claim, the result is significant.\nA standardized test statistic measures how many SE’s the observed stat is from the null claim.\nA standardized test statistic with a large absolute value is supporting evidence to reject \\(H_0\\).\n\nUsing the null distribution, compute the p-value. The p-value is the probability of getting a sample with a test statistic as or more extreme than the observed sample assuming \\(H_0\\) is true.\n\nThe p-value is the area in one or both tails beyond the test statistic.\nThe p-value is a probability, so we have \\(0 &lt; \\mbox{p-value} &lt; 1\\).\nThe smaller the p-value, the stronger the evidence to reject \\(H_0\\).\n\nBased on the significance level, \\(\\alpha\\), make a decision to reject or not reject the null hypothesis\n\nIf p-value \\(\\leq \\alpha\\), we reject \\(H_0\\).\nIf p-value \\(&gt; \\alpha\\), we do not reject \\(H_0\\).\n\nSummarize the results in practical terms, in the context of the example.\n\nIf we reject \\(H_0\\), this means there is enough evidence to support the claim in \\(H_a\\).\nIf we do not reject \\(H_0\\), this means there is not evidence to reject \\(H_0\\) nor support \\(H_a\\). The test is inconclusive."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#sec-samp-size",
    "href": "24-Hypothesis-Comparing-Two.html#sec-samp-size",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Appendix B: Sample Sizes and the CLT",
    "text": "Appendix B: Sample Sizes and the CLT\n\n\nFor a single mean, we can use the CTL to perform a hypothesis test as long as:\n\nEither the population is symmetric or \\(n \\geq 30\\).\nIf the sample is symmetric, we can assume the population is symmetric.\n\nFor a difference in two means , we can use the CTL to perform a hypothesis test as long as:\n\nPopulation 1 is either symmetric or \\(n_1 \\geq 30\\), and\nPopulation 2 is either symmetric or \\(n_2 \\geq 30\\).\n\nFor a single proportion, we can use the CTL to perform a hypothesis test as long as:\n\nBoth \\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\).\n\nFor a difference in two proportions, we can use the CTL to perform a hypothesis test as long as:\n\nAll of \\(n_1\\hat{p}_1 \\geq 10\\), \\(n_1(1-\\hat{p}_1) \\geq 10\\), \\(n_2\\hat{p}_2 \\geq 10\\), and \\(n_2(1-\\hat{p}_2) \\geq 10\\) are satisfied."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#appendix-c-parametric-formulas",
    "href": "24-Hypothesis-Comparing-Two.html#appendix-c-parametric-formulas",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "Appendix C: Parametric Formulas",
    "text": "Appendix C: Parametric Formulas\n\n\n\n\n\n\n\n\n\n\nParameter(s)\nSample Stat\nStandard Error\nDistribution\n\n\n\n\nA single mean  (\\(\\sigma^2\\) known)\n\\(\\bar{x}\\)\n\\(\\dfrac{\\sigma}{\\sqrt{n}}\\)\n\\(N(0,1)\\)\n\n\nA single mean  (\\(\\sigma^2\\) unknown)\n\\(\\bar{x}\\)\n\\(\\dfrac{{\\color{tomato}{s}}}{\\sqrt{n}}\\)\n\\({\\color{tomato}{t_{n-1}}}\\)\n\n\nA single proportion\n\\(\\hat{p}\\)\nNot Applicable\n\\(\\mbox{Binom}(n, p_0)\\)\n\n\nA difference in two means (unknown \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\))\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\dfrac{{\\color{tomato}{s_1}}^2}{n_1} + \\dfrac{{\\color{tomato}{s_2}}^2}{n_2}}\\)\n\\({\\color{tomato}{t_{n_{\\rm min}-1}}}\\)\n\n\nA difference in two proportions\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\\(\\sqrt{{\\color{tomato}{\\hat{p}_p}}(1 -{\\color{tomato}{\\hat{p}_p}})\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\\)\n\\(N(0,1)\\)\n\n\nMean difference between matched-pairs\n\\(\\bar{x}_{\\rm diff}\\)\n\\(\\dfrac{{\\color{tomato}{s_{\\rm diff}}}}{\\sqrt{n}}\\)\n\\({\\color{tomato}{t_{n-1}}}\\)\n\n\n\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "24-Hypothesis-Comparing-Two.html#footnotes",
    "href": "24-Hypothesis-Comparing-Two.html#footnotes",
    "title": "6.4: Parametric Tests for Two Populations",
    "section": "",
    "text": "This example is based on research by University of Toronto Faculty of Dentistry. The data provided in this question has been modified from the original research.↩︎"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html",
    "href": "25-Error-Types-and-Power-of-Tests.html",
    "title": "6.5: Error Types and Power of Tests",
    "section": "",
    "text": "Potential Errors with Hypothesis Tests\nThe result of a hypothesis test has one of two possibilities:\nAs with confidence intervals, it is possible we do all of our analysis perfectly without any mistakes, but our conclusion is incorrect due to the randomness in sampling. Rarely, we are unlucky and dealt a biased sample, in which case we arrive at an incorrect conclusion.\nIn this section, we explore the following questions:\nThere are two possible errors in a hypothesis test:\nFor example, when a jury is deciding a case in court, the hypotheses would be:\nA jury can make two possible errors:\nThe significance level of a hypothesis test is the largest value of \\(\\mathbf{\\alpha}\\) we find acceptable for the probability for a type I error.\nWhen performing a hypothesis test at a significance level of \\(\\alpha\\), the rejection or critical region, denoted \\(\\mathcal{R}\\), is the set of all values of the test statistic for which we reject \\(H_0\\). The endpoint(s) of the region are called critical values.\nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-1",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-1",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 1",
    "text": "Question 1\n\nA hospital is testing to see whether a donated organ is a match for a recipient in need of an organ transplant.\n\n\\(H_0\\): The organ is not a match (boring).\n\\(H_a\\): The organ is a match (interesting).\n\nDescribe the type I and type II errors in this context. What are the practical consequences of making these errors?\n\nSolution to Question 1"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-2",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-2",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 2",
    "text": "Question 2\n\nA lab runs viral tests to see whether a person is currently infected with COVID-19.\n\n\\(H_0\\): The person is not currently infected with COVID-19 (boring).\n\\(H_a\\): The person is currently infected with COVID-19 (interesting).\n\nDescribe the type I and type II errors in this context. What are the practical consequences of making these errors?\n\nSolution to Question 2"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-3",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-3",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 3",
    "text": "Question 3\n\nThe cholesterol level of healthy men is normally distributed with a mean of 180 mg/dL and a standard deviation of 20 mg/dL, whereas men predisposed to heart disease have a mean cholesterol level of 300 mg/dL with a standard deviation of 30 mg/dL. The cholesterol level 225 mg/dL is used to demarcate healthy from predisposed men.\n\nQuestion 3a\n\nGiven that a man is healthy, what is the probability they are diagnosed as predisposed?\n\nSolution to Question 3a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 3b\n\nGiven that a man is not healthy, what is the probability they are not diagnosed as predisposed?\n\nSolution to Question 3b\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 3c\n\nWhich of the previous answers gives the probability of a type I error and which is for a type II error? Explain.\n\nSolution to Question 3c"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-4",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-4",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 4",
    "text": "Question 4\n\nSuppose we want to test whether a ten-sided die is fair (with sides numbered 0 to 9). Let \\(p\\) be the proportion of all rolls that land on an even number.\n\nQuestion 4a\n\nSet up the hypotheses to test our claim.\n\nSolution to Question 4a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 4b\n\nRoll the die 20 times, and record how many times it lands on an even number (0, 2, 4, 6, or 8). If you do not have a ten-sided die, use the code cell below to simulate rolling a fair, ten-sided die \\(n=20\\) times.\n\nSolution to Question 4b\n\n\n# run code cell if you do not have a ten-sided die\nsample(0:9, 20, replace = TRUE)\n\n\n\n\n\n\nQuestion 4c\n\nCalculate the p-value of your sample.\n\nSolution to Question 4c\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 4d\n\nWhat (if anything) can you conclude about the hypothesis at 10% significance level?\n\nSolution to Question 4d"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-5",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-5",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 5",
    "text": "Question 5\n\n\n\n\nCredit: Seobility CC BY-SA 4.0\n\n\nA company claims that only 3% of people who use their facial lotion develop an allergic reaction (a rash). You are suspicious of their claim based on hearing some of your friends had an allergic reaction, and you believe it is more than 3%. You pick a random sample of 50 people and have them try the lotion. If more than 3 out of the 50 people develop the rash, you will blow up social media with posts about the dishonesty of the company’s claim.\n\nQuestion 5a\n\nSet up hypotheses for this test.\n\nSolution to Question 5a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 5b\n\nExplain what type I and type II errors are in this case. Make sure you explain in the context of this example.\n\nSolution to Question 5b\n\n\n\n\n\n\n\nQuestion 5c\n\nWhat is the probability of making a type I error?\n\nSolution to Question 5c\n\n\n# code cell to help with calculations\n\n\n\n\n\n\n\nQuestion 5d\n\nIf you were to perform the hypothesis test at a 5% significance level, and you observe \\(X=4\\), what would be the result of the test?\n\nSolution to Question 5d\n\n\n\n\n\n\n\nQuestion 5e\n\nFor what values of \\(X\\) would you reject \\(H_0\\) at a 5% significance level?\n\nSolution to Question 5e"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-6",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-6",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 6",
    "text": "Question 6\n\nIn Question 4 we tested whether or not a ten-sided die is fair by rolling it 20 times and counting the number of rolls that land on an even number. If \\(p\\) is the proportion of all rolls that land on an even number, then we have\n\\[H_0: p = 0.5 \\qquad \\mbox{vs.} \\qquad H_a: p \\ne 0.5.\\]\n\nQuestion 6a\n\nIf you found only \\(X=7\\) rolls landed on an even number, what is the p-value?\n\nSolution to Question 6a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 6b\n\nFind the critical values and rejection region if we use a significance level of 10%.\n\nSolution to Question 6b\n\n\n# code cell to help with calculations"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-7",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-7",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 7",
    "text": "Question 7\n\nSuppose you are interested in the lengths of a certain species of snake in an ecosystem. Assume the lengths (in cm.) are normally distributed with unknown mean \\(\\mu\\), but the standard deviation of the population is known to be \\(\\sigma = 4\\) cm. It has been claimed that the mean length of this species is 25 cm. You believe the actual mean length is greater than 25 cm. You collect a random sample of 30 snakes. You will test using a significance level of \\(\\alpha = 0.05\\).\n\nQuestion 7a\n\nSet up hypotheses for the test.\n\nSolution to Question 7a\n\n\n\\(H_0\\):\n\\(H_a\\):\n\n\n\n\n\n\nQuestion 7b\n\nFind the critical value, and give the rejection region.\n\nSolution to Question 7b\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 7c\n\nIf in fact \\(\\mu = 27\\) cm, what is the probability of making a type II error?\n\nSolution to Question 7c\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 7d\n\nWhat is the probability of correctly rejecting \\(H_0\\) when \\(H_a\\) is true?\n\nSolution to Question 7d"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#definition-of-the-power-of-a-test",
    "href": "25-Error-Types-and-Power-of-Tests.html#definition-of-the-power-of-a-test",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Definition of the Power of a Test",
    "text": "Definition of the Power of a Test\n\nThe power of a test is the probability of correctly rejecting \\(H_0\\).\n\\[{\\color{dodgerblue}{\\mbox{power} = P(\\mbox{Reject } H_0 \\  | \\  H_a \\mbox{ is true}) = 1 - {\\color{tomato}{\\beta}}}},\\]\nwhere \\(\\beta\\) denotes the probability of a type II error."
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-8",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-8",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 8",
    "text": "Question 8\n\nLet \\(X_1\\), \\(X_2\\), \\(\\ldots\\) , \\(X_{12}\\) be a random variable from a Bernoulli distribution with unknown probability \\(p\\). We test\n\\[H_0: p=0.3 \\qquad \\mbox{versus} \\qquad H_a: p &lt; 0.3.\\]\nWe will reject the null if the number of success \\(Y= X_1 + X_2 + \\ldots + X_{12} \\leq 1\\).\n\nQuestion 8a\n\nFind the probability of a type I error.\n\nSolution to Question 8a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 8b\n\nIf the alternative hypothesis is true, find an expression for the power, \\(1-\\beta\\), as a function of \\(p\\).\n\nSolution to Question 8b\n\n\n# code cell to help with calculations"
  },
  {
    "objectID": "25-Error-Types-and-Power-of-Tests.html#question-9",
    "href": "25-Error-Types-and-Power-of-Tests.html#question-9",
    "title": "6.5: Error Types and Power of Tests",
    "section": "Question 9",
    "text": "Question 9\n\nYou draw a random sample \\(X_1, X_2, \\ldots , X_{10}\\) from an exponential distribution \\(f(x; \\lambda) = \\lambda e^{-\\lambda x}\\) (recall \\(\\mu = 1/\\lambda\\)). You will test\n\\[H_0: \\lambda = 0.25 \\qquad \\mbox{versus} \\qquad H_a: \\lambda &lt; 0.25.\\]\nYou decide you will reject the null hypothesis if at least 3 of the values of \\(X_i\\) are greater than 9.\n\nQuestion 9a\n\nCompute the probability of a type I error.\n\nSolution to Question 9a\n\n\n# code cell to help with calculations\n\n\n\n\n\n\nQuestion 9b\n\nIf actually \\(\\lambda = 0.15\\), what is the power of this test?\n\nSolution to Question 9b\n\n\n# code cell to help with calculations"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html",
    "href": "Intro-to-Vectors-Dataframes.html",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "",
    "text": "Introduction\nUnderstanding the data types of the variables in our data set, and the structure of our data is crucial before we can identify what aspects might need to be cleaned and transformed so we can perform statistical analysis more efficiently.\nThis notebook is intended to be a brief overview of some fundamentals of working with data in R.\nThese topics are important. This notebook just scratches the surface on many concepts. If you do not find a complete answer here, there are free resources online that dig deeper and more completely. Below are two such recommended references.\nThe package dplyr contains a data set called storms. Let’s find some useful information about this data.\n# get a numerical summary of all variables\nsummary(storms)\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512\nTo store a data structure in the computer’s memory we must assign it a name.\nData structures can be stored using the assignment operator &lt;- or =.\nSome comments:\nIn the following code, we compute the mean of a vector. Why can’t we see the result after running it?\nw &lt;- storms$wind  # wind is now stored in w\nxbar.w &lt;- mean(w)  # compute mean wind speed and assign to xbar.w\nxbar.w  # print the mean wind speed to screen\n\n[1] 50.01741\nprint(xbar.w)  # print the mean with print() command\n\n[1] 50.01741\n# calculate, assign, and print standard deviation\n(s &lt;- sd(w))  # note ( ) around the entire command\n\n[1] 25.50103\nR has 6 basic data types:\nR operates on data structures. A data structure is simply some sort of “container” that holds certain kinds of information\nR has 5 basic data structures:\nSee R documentation for more info.\nThe read.table function imports data from file into R as a data frame.\nUsage: read.table(file, header = TRUE, sep = \",\")\nHere is an example reading a csv (comma separated file) with a header:\n# import data as data frame\nbike.store &lt;- read.table(file=\"https://raw.githubusercontent.com/CU-Denver-MathStats-OER/Statistical-Theory/main/Data/Transactions.csv\",\n                         header = TRUE,  # Keep column headers as names\n                         sep = \",\")  # comma as separator of columns\n\nglimpse(bike.store)\n\nRows: 20,000\nColumns: 13\n$ transaction_id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ product_id              &lt;int&gt; 2, 3, 37, 88, 78, 25, 22, 15, 67, 12, 5, 61, 3…\n$ customer_id             &lt;int&gt; 2950, 3120, 402, 3135, 787, 2339, 1542, 2459, …\n$ transaction_date        &lt;chr&gt; \"25-02-2017\", \"21-05-2017\", \"16-10-2017\", \"31-…\n$ online_order            &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, F…\n$ order_status            &lt;chr&gt; \"Approved\", \"Approved\", \"Approved\", \"Approved\"…\n$ brand                   &lt;chr&gt; \"Solex\", \"Trek Bicycles\", \"OHM Cycles\", \"Norco…\n$ product_line            &lt;chr&gt; \"Standard\", \"Standard\", \"Standard\", \"Standard\"…\n$ product_class           &lt;chr&gt; \"medium\", \"medium\", \"low\", \"medium\", \"medium\",…\n$ product_size            &lt;chr&gt; \"medium\", \"large\", \"medium\", \"medium\", \"large\"…\n$ list_price              &lt;dbl&gt; 71.49, 2091.47, 1793.43, 1198.46, 1765.30, 153…\n$ standard_cost           &lt;dbl&gt; 53.62, 388.92, 248.82, 381.10, 709.48, 829.65,…\n$ product_first_sold_date &lt;int&gt; 41245, 41701, 36361, 36145, 42226, 39031, 3416…\n?read.table\nSometimes we need to know if the elements of an object satisfy certain conditions. This can be determined using the logical operators &lt;, &lt;=, &gt;, &gt;=, ==, !=.\nExecute the following commands in R and see what you get.\na &lt;- seq(2, 16, by = 2) # creating the vector a\na\n\n[1]  2  4  6  8 10 12 14 16\n\na &gt; 10\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\na &lt;= 4\n\n[1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\na == 10\n\n[1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\na != 10\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-load",
    "href": "Intro-to-Vectors-Dataframes.html#sec-load",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Loading Packages with the library() Command",
    "text": "Loading Packages with the library() Command\n\nTo explore some fundamentals of working with data in R, we will use the storms data set which is located in the package dplyr.\n\nThe dplyr package is already installed in Google Colaboratory\nWe still need to use a library command to load the package.\nRun the code cell below to load the dplyr package.\n\n\n# load the library of functions and data in dplyr\nlibrary(dplyr)\n\n\n\n\n\n\n\nCaution\n\n\n\nEach time you connect or restart a session, you will need to run a library() command in order to access data and scripts in a package."
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-help",
    "href": "Intro-to-Vectors-Dataframes.html#sec-help",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Help Documentation",
    "text": "Help Documentation\n\nThe functions introduced in this document have robust help documentation with lots of options to customize. If you want to view help documentation for any of the functions used in this document, run commands such?typeof, ?is.numeric, ?read.table, and so on.\n\n# access help documentation for storms\n?storms  # side panel should open with help manual for storms\n\n\n# access help documentation for typeof\n?typeof"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-na",
    "href": "Intro-to-Vectors-Dataframes.html#sec-na",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Missing Data",
    "text": "Missing Data\n\nA missing value occurs when the value of something isn’t known. R uses the special object NA to represent missing value. If you have a missing value, you should represent that value as NA. Note: The character string \"NA\" is not the same thing as NA.\n\nThe storms data has properly coded 14,382 missing values for category since storms that are not hurricanes do not have a category.\nThe storms data has properly coded 9,512 missing values for each of tropicalstorm_force_diameter and hurricane_force_diameter since these value only began being recorded in 2004."
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-typeof",
    "href": "Intro-to-Vectors-Dataframes.html#sec-typeof",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Checking Data Type Using typeof()",
    "text": "Checking Data Type Using typeof()\n\n\nThe typeof() function returns the R internal type or storage mode of any object.\n\n\ntypeof(1.0)\n\n[1] \"double\"\n\ntypeof(2)\n\n[1] \"double\"\n\ntypeof(3L)\n\n[1] \"integer\"\n\ntypeof(\"hello\")\n\n[1] \"character\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\ntypeof(storms$status)\n\n[1] \"integer\"\n\ntypeof(storms$year)\n\n[1] \"double\"\n\ntypeof(storms$name)\n\n[1] \"character\""
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-isdatatype",
    "href": "Intro-to-Vectors-Dataframes.html#sec-isdatatype",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Investigating Data Types with is.numeric()",
    "text": "Investigating Data Types with is.numeric()\n\n\nThe is.numeric(x) function tests whether or not an object x is numeric.\nThe is.character(x) function tests whether x is a character or not.\nThe is.factor(x) function tests whether x is a factor or not.\n\n\n\n\n\n\n\nNote\n\n\n\nCategorical data is typically stored as a factor in R.\n\n\n\nis.numeric(storms$year)  # year is numeric\n\n[1] TRUE\n\nis.numeric(storms$category)  # category is also numeric\n\n[1] TRUE\n\nis.numeric(storms$name)  # name is not numeric\n\n[1] FALSE\n\nis.character(storms$name)  # name is character string\n\n[1] TRUE\n\n\n\nis.numeric(storms$status)  # status is not numeric\n\n[1] FALSE\n\nis.character(storms$status)  # status is not a character\n\n[1] FALSE\n\nis.factor(storms$status)  # status is a factor which is categorical\n\n[1] TRUE\n\n\n\nThe function str(x) provides information about the levels or classes of x.\n\n\nstr(storms$status)\n\n Factor w/ 9 levels \"disturbance\",..: 7 7 7 7 7 7 7 7 8 8 ..."
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-changetype",
    "href": "Intro-to-Vectors-Dataframes.html#sec-changetype",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Changing Data Types",
    "text": "Changing Data Types\n\n\nConverting to Categorical Data with factor()\n\n\nSometimes we think a variable is one data type, but it is actually being stored (and thus interpreted by R) as a different data type.\nOne common issue is categorical data is stored as characters. We would like observations with the same values to be group together.\nThe status variable in storms is being properly stored as a factor!\nThe category variable in storms is being stored as a numeric since it is ordinal.\nWith ordinal categories, we may choose to keep it stored as numeric, or we may prefer to treat them as factors.\n\n\nsummary(storms$category)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   1.000   1.898   3.000   5.000   14382 \n\n\n\nThe summary of category computes statistics such as mean and median.\nTypically with categorical data, we prefer to count how many observations are in each class of the variable.\nIn the code cell below, we convert category to a factor, and then observe the resulting summary.\n\n\nstorms$category &lt;- factor(storms$category)\nsummary(storms$category)\n\n    1     2     3     4     5  NA's \n 2478   973   579   539   115 14382 \n\n\n\n\nConverting Data Types with as.numeric(), as.integer(), etc.\n\nFrom the summary of the storms data set we first found above, we see that the variables year and month are being stored as double. These variables actually are integer values.\nWe can convert another variable of one format into another format using as.[new_datatype]()\n\nFor example, to convert to year to integer, we use as.integer(storms$year).\nTo convert a data type to character, we can use as.character(x).\nTo convert to a decimal (double), we can use as.numeric(x)\n\n\ntypeof(storms$year)\n\n[1] \"double\"\n\ntypeof(storms$month)\n\n[1] \"double\"\n\nstorms$year &lt;- as.integer(storms$year)\nstorms$month &lt;- as.integer(storms$month)\ntypeof(storms$year)\n\n[1] \"integer\"\n\ntypeof(storms$month)\n\n[1] \"integer\""
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-vectors",
    "href": "Intro-to-Vectors-Dataframes.html#sec-vectors",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Vectors",
    "text": "Vectors\n\nA vector is a single-dimensional set of data of the same type.\n\nCreating Vectors from Scratch\n\nThe most basic way to create a vector is the combine function c. The following commands create vectors of type numeric, character, and logical, respectively.\n\nx1 &lt;- c(1, 2, 5.3, 6, -2, 4)\nx2 &lt;- c(\"one\", \"two\", \"three\")\nx3 &lt;- c(TRUE, TRUE, FALSE, TRUE)\nx4 &lt;- c(TRUE, 3.4, \"hello\")\ntypeof(x1)\n\n[1] \"double\"\n\ntypeof(x2)\n\n[1] \"character\"\n\ntypeof(x3)\n\n[1] \"logical\"\n\ntypeof(x4)\n\n[1] \"character\"\n\n\n\nWe can check the data structure of an object using commands such as is.vector(), is.list(), is.matrix(), and so on.\n\n\nis.list(x1)\n\n[1] FALSE\n\nis.vector(x1)\n\n[1] TRUE\n\nis.list(x4)\n\n[1] FALSE\n\nis.vector(x4)\n\n[1] TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-dataframe",
    "href": "Intro-to-Vectors-Dataframes.html#sec-dataframe",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Data Frames",
    "text": "Data Frames\n\nData frames are two-dimensional data objects and are the fundamental data structure used by most of R’s libraries of functions and data sets.\n\nTabular data is tidy if each row corresponds to a different observation and column corresponds to a different variable.\n\nEach column of a data frame is a variable (stored as a vector). If the variable:\n\nIs measured or counted by a number, it is a quantitative or numerical variable.\nGroups observations into different categories or rankings, it is a qualitative or categorical variable.\n\n\nCreating Data Frames from Scratch\n\nData frames are created by passing vectors into the data.frame() function.\nThe names of the columns in the data frame are the names of the vectors you give the data.frame function.\nConsider the following simple example.\n\n# create basic data frame\nd &lt;- c(1, 2, 3, 4)\ne &lt;- c(\"red\", \"white\", \"blue\", NA)\nf &lt;- c(TRUE, TRUE, TRUE, FALSE)\ndf &lt;- data.frame(d,e,f)\ndf\n\n  d     e     f\n1 1   red  TRUE\n2 2 white  TRUE\n3 3  blue  TRUE\n4 4  &lt;NA&gt; FALSE\n\n\n\n\nNaming Column Headers\n\nThe columns of a data frame can be renamed using the names() function on the data frame.\n\n# name columns of data frame\nnames(df) &lt;- c(\"ID\", \"Color\", \"Passed\")\ndf\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE\n\n\nThe columns of a data frame can be named when you are first creating the data frame by using [new_name] = [orig_vec_name] for each vector of data.\n\n# create data frame with better column names\ndf2 &lt;- data.frame(ID = d, Color = e, Passed = f)\ndf2\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-checkstructure",
    "href": "Intro-to-Vectors-Dataframes.html#sec-checkstructure",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Checking Data Structure",
    "text": "Checking Data Structure\n\n\nThe is.matrix(x) function tests whether or not an object x is a matrix.\nThe is.vector(x) function test whether x is a vector.\nThe is.data.frame(x) function test whether x is a data frame.\n\n\nis.matrix(df)\n\n[1] FALSE\n\nis.vector(df)\n\n[1] FALSE\n\nis.data.frame(df)\n\n[1] TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-extractname",
    "href": "Intro-to-Vectors-Dataframes.html#sec-extractname",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Extracting a Column By Name",
    "text": "Extracting a Column By Name\n\nThe column vectors of a data frame may be extracted using $ and specifying the name of the desired vector.\n\ndf$Color would access the Color column of data frame df.\n\n\ndf$Color  # prints column of data frame df named Color\n\n[1] \"red\"   \"white\" \"blue\"  NA"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-indexing",
    "href": "Intro-to-Vectors-Dataframes.html#sec-indexing",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Slicing Rows and Columns By Indexing",
    "text": "Slicing Rows and Columns By Indexing\n\nPart of a data frame can also be extracted by thinking of at as a general matrix and specifying the desired rows or columns in square brackets after the object name.\n\nNote R starts with index 1 which is different from Python which indexes starting from 0.\n\nFor example, if we had a data frame named df:\n\ndf[1,] would access the first row of df.\ndf[1:2,] would access the first two rows of df.\ndf[,2] would access the second column of df.\ndf[1:2, 2:3] would access the information in rows 1 and 2 of columns 2 and 3 of df.\n\n\ndf[,2]  # second column is Color\n\n[1] \"red\"   \"white\" \"blue\"  NA     \n\n\n\ndf[2,]  # second row of df\n\n  ID Color Passed\n2  2 white   TRUE\n\n\n\ndf[1:2,2:3]  # first and second rows of columns 2 and 3\n\nIf you need to select multiple columns of a data frame by name, you can pass a character vector with column names in the column position of [].\n\ndf[, c(\"ID\", \"Passed\")] would extract the ID and Passed columns of df.\n\n\ndf[, c(\"ID\", \"Passed\")]\n\n  ID Passed\n1  1   TRUE\n2  2   TRUE\n3  3   TRUE\n4  4  FALSE\n\n\n\ndf[, c(1, 3)]  # another we to pick columns 1 and 3\n\n  ID Passed\n1  1   TRUE\n2  2   TRUE\n3  3   TRUE\n4  4  FALSE\n\n\n\n# another we to pick columns 1 and 3\ndf[, -2]  # exclude column 2\n\n  ID Passed\n1  1   TRUE\n2  2   TRUE\n3  3   TRUE\n4  4  FALSE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-andor",
    "href": "Intro-to-Vectors-Dataframes.html#sec-andor",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "And and Or Statements",
    "text": "And and Or Statements\n\nMore complicated logical statements can be made using & and |.\n\n& means “and”\n\nBoth statements must be true for state1 & state2 to return TRUE.\n\n| means “or”\n\nOnly one of the the two statements must be true for state1 | state2 to return TRUE.\nIf both statements are true in an “or” statement, the statement is also TRUE.\n\n\nBelow is a summary of “and” and “or” logic:\n\nTRUE & TRUE returns TRUE\nFALSE & TRUE returns FALSE\nFALSE & FALSE returns FALSE\nTRUE | TRUE returns TRUE\nFALSE | TRUE returns TRUE\nFALSE | FALSE returns FALSE\n\n\n# relationship between logicals & (and), | (or)\nTRUE & TRUE\n\n[1] TRUE\n\nFALSE & TRUE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\nTRUE | TRUE\n\n[1] TRUE\n\nFALSE | TRUE\n\n[1] TRUE\n\nFALSE | FALSE\n\n[1] FALSE\n\n\nExecute the following commands in R and see what you get.\n\nb &lt;- 3  # b is equal to the number 3\n\n# complex logical statements\n(b &gt; 6) & (b &lt;= 10)  # FALSE and TRUE\n\n[1] FALSE\n\n(b &lt;= 4) | (b &gt;= 12)  # TRUE or FALSE\n\n[1] TRUE"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-logicindex",
    "href": "Intro-to-Vectors-Dataframes.html#sec-logicindex",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Logical Indexing",
    "text": "Logical Indexing\n\nWe can use a logical statement as an index to extract certain entries from a vector or data frame. For example, if we want to to know the product_id (column 2), brand (column 7), product_line (column 8), and list_price (column 11) of all transactions that have a list_price greater than $2,090, we can run the code cell below.\n\nWe use a logical index for the row to extract just the rows that have a list_price value strictly greater than 2090.\nWe indicate we want to keep just columns 2, 7 through 8, and 11 with the column index c(2, 7:8, 11).\nWe store the results to a new data frame named expensive.\nFinally, we print the first 6 rows of our new data frame with the head() function to check the results.\n\n\nexpensive &lt;- bike.store[bike.store$list_price &gt; 2090, c(2, 7:8, 11)]\nhead(expensive)\n\n    product_id         brand product_line list_price\n2            3 Trek Bicycles     Standard    2091.47\n16           3 Trek Bicycles     Standard    2091.47\n69          38 Trek Bicycles     Standard    2091.47\n154          3 Trek Bicycles     Standard    2091.47\n165          3 Trek Bicycles     Standard    2091.47\n188          3 Trek Bicycles     Standard    2091.47"
  },
  {
    "objectID": "Intro-to-Vectors-Dataframes.html#sec-cc",
    "href": "Intro-to-Vectors-Dataframes.html#sec-cc",
    "title": "Appendix A — Fundamentals of Working with Data",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "Overview-of-Plots.html",
    "href": "Overview-of-Plots.html",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "",
    "text": "Introduction\nPlots can provide a useful visual summary of the data. Sometimes, a nice plot or two is all that is need for statistical analysis. In this document, we cover a basic overview of creating some plots in R.\nHere’s a link to a more thorough coverage of plotting in R: https://r-graph-gallery.com/index.html.\nR packages are a collection functions, sample data, and/or other code scripts. R installs a set of default packages during installation.\nRun the code cell below to get a list of all default R packages that are already installed.\n# See a list of installed default packages\nallpack &lt;- installed.packages()\nrownames(allpack)\nThe package dplyr contains a data set called storms. Let’s find some useful information about this data.\n# be sure to run the code cell above first\n# so you have loaded the dplyr package\n?storms\n# See a summary of all variables\nsummary(storms)\n\n     name                year          month             day       \n Length:19066       Min.   :1975   Min.   : 1.000   Min.   : 1.00  \n Class :character   1st Qu.:1993   1st Qu.: 8.000   1st Qu.: 8.00  \n Mode  :character   Median :2004   Median : 9.000   Median :16.00  \n                    Mean   :2002   Mean   : 8.699   Mean   :15.78  \n                    3rd Qu.:2012   3rd Qu.: 9.000   3rd Qu.:24.00  \n                    Max.   :2021   Max.   :12.000   Max.   :31.00  \n                                                                   \n      hour             lat             long                         status    \n Min.   : 0.000   Min.   : 7.00   Min.   :-109.30   tropical storm     :6684  \n 1st Qu.: 5.000   1st Qu.:18.40   1st Qu.: -78.70   hurricane          :4684  \n Median :12.000   Median :26.60   Median : -62.25   tropical depression:3525  \n Mean   : 9.094   Mean   :26.99   Mean   : -61.52   extratropical      :2068  \n 3rd Qu.:18.000   3rd Qu.:33.70   3rd Qu.: -45.60   other low          :1405  \n Max.   :23.000   Max.   :70.70   Max.   :  13.50   subtropical storm  : 292  \n                                                    (Other)            : 408  \n    category          wind           pressure      tropicalstorm_force_diameter\n Min.   :1.000   Min.   : 10.00   Min.   : 882.0   Min.   :   0.0              \n 1st Qu.:1.000   1st Qu.: 30.00   1st Qu.: 987.0   1st Qu.:   0.0              \n Median :1.000   Median : 45.00   Median :1000.0   Median : 110.0              \n Mean   :1.898   Mean   : 50.02   Mean   : 993.6   Mean   : 146.3              \n 3rd Qu.:3.000   3rd Qu.: 65.00   3rd Qu.:1007.0   3rd Qu.: 220.0              \n Max.   :5.000   Max.   :165.00   Max.   :1024.0   Max.   :1440.0              \n NA's   :14382                                     NA's   :9512                \n hurricane_force_diameter\n Min.   :  0.00          \n 1st Qu.:  0.00          \n Median :  0.00          \n Mean   : 14.81          \n 3rd Qu.:  0.00          \n Max.   :300.00          \n NA's   :9512\nOften a graph or plot is a more preferred format to summarize a variable than a summary statistics. The documentation below explains we could graphically summarize the quantitative variable pressure.\nQualitative (also called categorical) variables required other types of plots. For example, we cannot create a density or boxplot for a qualitative variable. Qualitative variables may be stored as characters (such as the status variable) or values (such as the category variable). This brings up a good question:\nImagine we would like to compare the wind speeds of storms by status. In this case, we would like to compare a quantitative variable (wind) for different classes of a qualitative variable (status).\nImagine we would like to compare the number of different category hurricanes that occurred in each month. In this case, we would like to compare two qualitative variables, namely category and month.\nImagine we would like to compare the wind speeds (wind) to the pressure (pressure). In this case, we would like to compare two quantitative variables.\nA scatter plot can be used to identify the relationship between two quantitative variables.\n# create a scatter plot\n# first variable wind is response (y-axis)\n# second variable pressure is predictor (x-axis)\n\nplot(wind ~ pressure,  # response ~ predictor(s)\n     data = storms,  # data frame name\n     main = \"Relation of Pressure and Wind Speed of Storms\",  # main title\n     xlab = \"Pressure (in millibars)\",  # horizontal axis label\n     ylab = \"Wind Speed (in knots)\")  # vertical axis label\npar(mfrow = c(2, 2))  # create a 2 x 2 array of plots\n\n# the next 5 plots created will be arranged in the array\nboxplot(storms$wind)  # create boxplot of wind speed\n\n# code below creates a histogram of wind speed\n# we can add many options to customize\nhist(storms$wind, xlab = \"wind speed (in knots)\",   # x-axis label\n     ylab = \"Frequency\",  # y-axis label\n     main = \"Distribution of Storm Wind Speed 1975-2020\",  # main label\n     col = \"steelblue\")  # change color of bars\n\nplot(storms$status, col = \"gold\")  # plots status, which is categorical\n\nplot(wind ~ pressure, data = storms)  # plots two numerical variables\n# create a table of status counts\n# we will pull of the row names of the table\n# as the labels in the legend\nstatus.table &lt;- table(storms$status)\n\nplot(wind ~ status,  # quantitative first ~ categorical second\n     data = storms,  # name of data frame\n     col = my.colors,  # fill colors colors\n     ylab = \"Wind speed in knots\",  # vertical axis label\n     main = \"Wind Speeds of Storms by Status\")  # main title\n\n# we can add a legend to identify which plot is which storm status\nlegend(x = \"topright\",  # place legend in top right corner\n       legend=rownames(status.table),  # each row of table is label in legend\n       fill = my.colors)  # fill colors\nThe previous plots were created using R’s base graphics system.\nA fancier alternative is to construct plots using the ggplot2 package.\nIn its simplest form, to construct a (useful) plot in ggplot2, you need to provide:"
  },
  {
    "objectID": "Overview-of-Plots.html#help-documentation",
    "href": "Overview-of-Plots.html#help-documentation",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Help Documentation",
    "text": "Help Documentation\n\nThe plotting functions introduced in this document have robust help documentation with lots of options to customize your plots. If you want to view help documentation for any of the functions used in this document, run commands such?hist, ?plot, ?table, and so on.\n\n# access help documentation for hist\n?hist  #Side panel should open with help doc"
  },
  {
    "objectID": "Overview-of-Plots.html#loading-packages-with-the-library-command",
    "href": "Overview-of-Plots.html#loading-packages-with-the-library-command",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Loading Packages with the library() Command",
    "text": "Loading Packages with the library() Command\n\nEach time we start or restart a new session and want to access the library of functions and data in the package, we need to load the library of files in the package with the library() command.\nTo demonstrate how to create common statistical plots in R, we will use the storms data set which is located in the package dplyr.\n\nThe dplyr package is already installed in Google Colaboratory\nWe still need to use a library command to load the package.\nRun the code cell below to load the dplyr package.\n\n\n# load the library of functions and data in dplyr\nlibrary(dplyr)"
  },
  {
    "objectID": "Overview-of-Plots.html#reloading-packages-when-restarting-a-session",
    "href": "Overview-of-Plots.html#reloading-packages-when-restarting-a-session",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Reloading Packages When Restarting a Session",
    "text": "Reloading Packages When Restarting a Session\n\nIf we take a break in our work, it is possible our R session will time out and close. Each time we restart an R session, we will need to rerun library() commands in order reload any packages we plan to use.\nThe same caution applies to any objects, vectors, or data frames we create or edit in an R session. If a session times out, and we want to use an object x that we previously created, we will need to run the code cell(s) where object x is created again before we can refer back to x in the current session.\nBE SURE YOU RUN THE COMMAND library(dplyr) BEFORE ATTEMPTING TO RUN ANY OF THE CODE CELLS BELOW!"
  },
  {
    "objectID": "Overview-of-Plots.html#histograms",
    "href": "Overview-of-Plots.html#histograms",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Histograms",
    "text": "Histograms\n\nThe hist function can be used create a histogram of a numerical vector.\n\nSee histogram documentation: https://r-graph-gallery.com/histogram.html\nLike making colorful plots? Here’s a guide to colors in R.\nWe use a $ symbol to indicate the name of the variable in storms we will access in the plot.\n\n\nhist(storms$pressure,  # plot pressure variable in storms data\n     xlab = \"storm pressure (in millibars)\",  # x-axis label\n     main = \"Distribution of Storm Pressure\",  # main title\n     breaks = 10,  # number of breaks or bins\n     col = \"aquamarine4\")  # color of bars"
  },
  {
    "objectID": "Overview-of-Plots.html#density-plots",
    "href": "Overview-of-Plots.html#density-plots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Density plots",
    "text": "Density plots\n\nA histogram is more sensitive to its options. For example, a histogram with 3 breaks may tell a different story than plotting the same data with 20 breaks.\nThus, we may prefer to use a density plot.\n\nFirst compute density of pressure.\n\n\nFor more information, see density help documentation.\n\n\nThe plot() function will then create a density plot.\n\n\nFor more advanced density plots see https://r-graph-gallery.com/density-plot.html.\nIf a variable is categorical, plot() will create a different plot, namely a bar chart.\nplot() can also be used to generate a plot to compare two different variables.\nThe output of plot() depends on the type and number of variables that we input in the function.\n\n\n# approximate densities and then plot\nplot(density(storms$pressure),\n     xlab = \"storm pressure (in millibars)\",  # horizontal axis label\n     main = \"Distribution of Storm Pressure\")  # main title"
  },
  {
    "objectID": "Overview-of-Plots.html#boxplots",
    "href": "Overview-of-Plots.html#boxplots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Boxplots",
    "text": "Boxplots\n\nBoxplots are another useful plot for presenting the distribution of a quantitative variable using quartiles and the five number summary.\n\nSee boxplot documentation at https://r-graph-gallery.com/boxplot.html.\nRun the command ?boxplot to see more options.\n\n\n# create boxplot of quantitative variable\nboxplot(storms$pressure,\n        ylab = \"storm pressure (in millibars)\",  # horizontal axis label\n        col = \"gold\",  # color of box\n        main = \"Distribution of Storm Pressure\")  # main title\n\n\n\n\n\nChanging the Layout of Boxplots\n\n\n# horizontally aligned boxplot\nboxplot(storms$pressure,\n        horizontal = TRUE,  # display horizontally\n        xlab = \"storm pressure (in millibars)\",  # horizontal axis label\n        main = \"Distribution of Storm Pressure\",  # main title\n        col = \"azure3\")  # color"
  },
  {
    "objectID": "Overview-of-Plots.html#checking-the-data-type",
    "href": "Overview-of-Plots.html#checking-the-data-type",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Checking the Data Type",
    "text": "Checking the Data Type\n\nThe typeof() command can help identify what is the type of a variable.\n\ntypeof(storms$status)\n\n[1] \"integer\"\n\ntypeof(storms$category)\n\n[1] \"double\"\n\n\n\nData Types\n\nFrom the output above, we see:\n\nThe variable status is initially read as an integer.\nThe individual values are strings of characters such as “tropical storm” or “hurricane”.\nThe summary statistics of status are counts that are stored as integers.\nThe variable category is initially read as double or decimal values.\nThe individual values are ordinal integers “1”, “2”, “3”, “4”, and “5” for category of hurricane.\nThere are 14,2328 NA (or missing) values corresponding to the observations that are not hurricanes.\nThe summary statistics of category (such as the mean) are stored decimals.\nHowever, we would like to treat category as a qualitative variable and plot how many storms fall into each category."
  },
  {
    "objectID": "Overview-of-Plots.html#caution-with-data-types-and-using-plot",
    "href": "Overview-of-Plots.html#caution-with-data-types-and-using-plot",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Caution with Data Types and Using plot()",
    "text": "Caution with Data Types and Using plot()\n\nIf we try to use the general plot() function, R will give its best guess at which plot makes the most sense to display the data. If the data is stored as the wrong data type, plot() will not work as we might expect.\n\nRun the two code cells below, and notice the following:\n\nThe output of the plot(storms$status) looks like a reasonable bar chart.\nThe output of plot(storms$category) does not nicely summarize the counts of how many storms are in each category.\n\n\n\nplot(storms$status)  # plot of status\n\n\n\n\n\nplot(storms$category)  # plot of category"
  },
  {
    "objectID": "Overview-of-Plots.html#creating-bar-charts-from-tables",
    "href": "Overview-of-Plots.html#creating-bar-charts-from-tables",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creating Bar Charts From Tables",
    "text": "Creating Bar Charts From Tables\n\nThe table() function will count the number of times a value (or string of characters) occurs in a vector or variable.\n\nOfficial Documentation Page for table()\nAnother nice resource with examples\n\nOne way to improve the initial plot of categories above is as follows:\n\nFirst use the table() command to count how many storms are in each category.\nThen create a bar chart using the barplot() function.\n\n\ncat.table &lt;- table(storms$category)  # create table of counts\ncat.table  # print table to screen\n\n\n   1    2    3    4    5 \n2478  973  579  539  115 \n\n\n\n# create bar chart from table counts\nbarplot(cat.table,  # input table from previous code cell\n        main = \"Distribution of Hurricane Categories\",  # main title\n        xlab = \"Hurricane Category\",  # horizontal axis label\n        ylab = \"Frequency\",  # vertical axis label\n        col = \"steelblue\")  # fill color of bars"
  },
  {
    "objectID": "Overview-of-Plots.html#relative-frequency-tables-and-bar-charts",
    "href": "Overview-of-Plots.html#relative-frequency-tables-and-bar-charts",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Relative Frequency Tables and Bar Charts",
    "text": "Relative Frequency Tables and Bar Charts\n\nIf instead of plotting the number of hurricanes in each category we wish to plot the proportion of all hurricanes in each category, we can use the prop.table() function to convert the table counts to proportions relative to the grand total.\nRun the two code cells below to create a relative frequency bar chart.\n\nWe input our previous table of counts, cat.table, into the function prop.table() to convert counts to proportions.\nThen we create a bar chart of the resulting proportions.\n\n\ncat.prop &lt;- prop.table(cat.table)  # create table of proportions\nbarplot(cat.prop,  # input table of proportions\n        main = \"Relative Frequency of Hurricane Categories\",  # main title\n        xlab = \"Hurricane Category\",  # horizontal axis label\n        ylab = \"Proportion\",  # vertical axis label\n        col = \"steelblue\")  # fill color of bars\n\n\n\n\n\n\n\n\n\n\nCaution with prop.table()\n\n\n\n\n\nThe input into prop.table() must be a table rather than a vector or data frame column.\nThe code cell below does return a relative frequency table as we would expect since we did not first create a table of counts from storms$category.\n\n\n\n\ntemp &lt;- prop.table(storms$category)  # do not input a vector\nhead(temp)  #  print first several entries of result\n\n[1] NA NA NA NA NA NA"
  },
  {
    "objectID": "Overview-of-Plots.html#pie-charts-with-pie",
    "href": "Overview-of-Plots.html#pie-charts-with-pie",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Pie Charts with pie()",
    "text": "Pie Charts with pie()\n\nPie charts can also be used to illustrate the distribution of one qualitative variable.\n\nSee https://r-graph-gallery.com/pie-plot.html.\nFor help and a list of options, you can run ?pie.\n\n\n?pie\n\n\n# create pie chart of one qualitative variable\npie(cat.table,  # input table\n    main = \"Distribution of Hurricane Categories\")  # main title"
  },
  {
    "objectID": "Overview-of-Plots.html#converting-to-a-factor-and-then-plot",
    "href": "Overview-of-Plots.html#converting-to-a-factor-and-then-plot",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Converting to a factor() and Then plot()",
    "text": "Converting to a factor() and Then plot()\n\nOne common issue with a qualitative variable is that it is often stored as the wrong datatype.\n\nQualitative data should typically be stored as a factor.\n\nAnother way we can create a bar chart of the counts in each category is to:\n\nFirst convert the qualitative variable to a factor.\nThen use plot() to create an appropriate plot.\n\nRun the code cell below to first see the summary output of the category variable after converting it to a factor.\n\n# creates a copy of storms data set\n# so we don't overwrite original storms\nstorms2 &lt;- storms  \n\nstorms2$category &lt;- factor(storms$category)  # convert category to factor\nsummary(storms2$category)  # get new summary of categories\n\n    1     2     3     4     5  NA's \n 2478   973   579   539   115 14382 \n\n\nNotice the summary is a table of counts in each hurricane category.\n\nOnce the variable status is converted to a factor, the plot() function will know to use a bar chart to give a summary display.\n\n\n# create bar chart from counts of a factor\nplot(storms2$category,  # input a factor\n     main = \"Distribution of Hurricane Category\",  # main title\n     xlab = \"Hurrican Category\",  # horizontal axis label\n     ylab = \"Frequency\",  # vertical axis label\n     col = \"steelblue\")  # color of fill of ba\n\n\n\n\n\nRecall without first changing category to a factor, plot() will create a different graph.\n\n\n# default plot of category when not first converted to factor\nplot(storms$category)"
  },
  {
    "objectID": "Overview-of-Plots.html#side-by-side-boxplots",
    "href": "Overview-of-Plots.html#side-by-side-boxplots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Side by Side Boxplots",
    "text": "Side by Side Boxplots\n\nThere are many classes of storms status in storms.\nIn the storms data:\n\nwind is a quantitative variable.\nstatus is a qualitative variable.\nWe can use the default plot() function to create a side by side boxplots.\n\n\n# create a vector of fill colors\n# one color for each status type.\nmy.colors &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \n               \"blue\", \"gold\", \"cyan\", \"pink\", \"orange\")\n\nplot(wind ~ status,  # quantitative first ~ categorical second\n     data = storms,  # name of data frame\n     col = my.colors,  # fill colors\n     main = \"Wind Speeds of Storms by Status\")  # main title\n\n\n\n\n\nAdding a Legend to Plots\n\n\nThere are a lot of different status of storms.\nIt is not easy (or possible) to tell which boxplot corresponds to which storm status.\nAdding a legend to the plot will help!\n\n\n# create a table of status counts\n# we will pull of the row names of the table\n# as the labels in the legend\nstatus.table &lt;- table(storms$status)\n\nplot(wind ~ status,  # quantitative first ~ categorical second\n     data = storms,  # name of data frame\n     col = my.colors,  # fill colors colors\n     ylab = \"Wind speed in knots\",  # vertical axis label\n     main = \"Wind Speeds of Storms by Status\")  # main title\n\n# we can add a legend to identify which plot is which storm status\nlegend(x = \"topright\",  # place legend in top right corner\n       legend=rownames(status.table),  # each row of table is label in legend\n       fill = my.colors)  # fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#subsetting-data-by-category",
    "href": "Overview-of-Plots.html#subsetting-data-by-category",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Subsetting Data by Category",
    "text": "Subsetting Data by Category\n\nThere are many classes of storms status in storms. Often, we want to only focus on a smaller subset of classes. We can restrict our attention to compare the wind speeds of three of the classes: “tropical storm”, “tropical depression”, and “hurricane”.\n\nWe can subset storms data frame into three separate data frames, one for each status of storm, using the subset() function.\nCurious to learn more about subset? Run ?subset in a code cell to access help documentation.\nThen we can create three separate boxplots of the wind speeds for each status.\n\n\n# split data by storm status\nhur &lt;- subset(storms,  # data frame name\n              status == \"hurricane\",  # logical test to select observations\n              select = wind)  # which quantitative variable(s) to select\n\ntrop.storm &lt;- subset(storms, \n                     status == \"tropical storm\",  # tropical storms\n                     select = wind)\ntrop.dep &lt;- subset(storms, \n                   status == \"tropical depression\",   # tropical depressions\n                   select = wind)\n\n# create side by side boxplot\n# for each of the three subsets\nboxplot(hur$wind, trop.storm$wind, trop.dep$wind, \n        main = \"Windspeed of Storms\", \n        names = c(\"Hurricanes\", \"Tropical Storms\", \"Tropical Depressions\"), \n        col = c(\"red\", \"blue\", \"green\"), \n        xlab = \"Wind speed in knots\", \n        horizontal = TRUE)"
  },
  {
    "objectID": "Overview-of-Plots.html#creating-contingency-or-two-way-table",
    "href": "Overview-of-Plots.html#creating-contingency-or-two-way-table",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creating Contingency or Two-Way Table",
    "text": "Creating Contingency or Two-Way Table\n\nThe command table(x) will count the number of times a value (or string of characters) occurs in a vector x.\n\nOfficial Documentation Page for table()\nNice resource with examples\n\nThe command table(x, y) will similarly create a contingency (or two-way) table to jointly compare counts of x and y.\n\n# create a contingency table for status and month\ncon.table &lt;- table(storms$category, storms$month) \ncon.table  # print output to screen\n\n   \n       1    4    5    6    7    8    9   10   11   12\n  1    5    0    0   18  140  581 1099  462  140   33\n  2    0    0    0    0   25  198  571  150   29    0\n  3    0    0    0    0   18  113  346   86   16    0\n  4    0    0    0    0   18  114  295   88   24    0\n  5    0    0    0    0    1   32   69   13    0    0"
  },
  {
    "objectID": "Overview-of-Plots.html#creating-grouped-frequency-bar-charts",
    "href": "Overview-of-Plots.html#creating-grouped-frequency-bar-charts",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creating Grouped Frequency Bar Charts",
    "text": "Creating Grouped Frequency Bar Charts\n\nAfter creating a two-way table, we can present the results visually in a grouped bar chart.\n\nSee documentation at https://r-graph-gallery.com/211-basic-grouped-or-stacked-barplot.html.\n\n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create side by side bar chart\nbarplot(con.table,  # use counts from contingency table\n        beside = TRUE,  # groups side-by-side\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # fill color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add a legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels from row name in contingency table\n       fill = my.colors2)  # use same fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#grouped-frequency-bar-charts",
    "href": "Overview-of-Plots.html#grouped-frequency-bar-charts",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Grouped Frequency Bar Charts",
    "text": "Grouped Frequency Bar Charts\n\n\nNote beside = FALSE is the default.\nIf we do not specify a beside option, a stacked bar chart is created instead.\nIn the second code cell, we also add a legend to the plot.\n\n\n########################################################\n# Note this has already been run in a previous section\n# You do not need to run again if already created\n#######################################################\n\n# create a contingency table for status and month\ncon.table &lt;- table(storms$category, storms$month) \ncon.table  # print output to screen\n\n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create stacked bar chart\nbarplot(con.table,  # use counts from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # color of bars\n        ylab = \"Frequency\")  # vertical axis label\n\n# add legend to plot\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels\n       fill = my.colors2)  # use same colors"
  },
  {
    "objectID": "Overview-of-Plots.html#stacked-bar-charts-relative-to-grand-total",
    "href": "Overview-of-Plots.html#stacked-bar-charts-relative-to-grand-total",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Stacked Bar Charts Relative to Grand Total",
    "text": "Stacked Bar Charts Relative to Grand Total\n\n\nFirst we create a contingency table using table(x, y).\nThen we use prop.table([table_name]) to convert to frequencies to proportions out of the grand total.\nFinally we can create a group bar chart of relative frequencies.\n\n\n# create two-table of counts\ncon.table &lt;- table(storms$category, storms$month)\n\n# convert counts to proportions\ncon.prop &lt;- prop.table(con.table) \n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create stacked bar chart\nbarplot(con.table,  # use counts from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # color of bars\n        ylab = \"Relative Frequency (of grand total)\")  # vertical axis label\n\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels\n       fill = my.colors)  # use same fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#stacked-bar-chart-relative-to-column-totals",
    "href": "Overview-of-Plots.html#stacked-bar-chart-relative-to-column-totals",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Stacked Bar Chart Relative to Column Totals",
    "text": "Stacked Bar Chart Relative to Column Totals\n\nOften, we would like the proportions in the table to be computed out of the total in each column (instead of the grand total).\n\nWe add the option 2 inside prop.table().\nIn this example, each column is a different month.\n\n\n# create two-table of counts\ncon.table &lt;- table(storms$category, storms$month)\n\n# convert counts to proportions\n# note the option 2 added to command below\ncon.prop.column &lt;- prop.table(con.table, 2)  \n\n# create a vector of colors\nmy.colors2 &lt;- c(\"green\", \"purple\", \"grey\", \"red\", \"blue\") \n\n# create stacked bar chart\nbarplot(con.prop.column,  # use counts from contingency table\n        main = \"Category Hurricanes By Month\",  # main title\n        xlab = \"Month\",  # horizontal axis label\n        col = my.colors2,  # color of bars\n        ylab = \"Relative Frequency (to month totals\")  # vertical axis label\n\nlegend(x=\"topleft\",  # place legend in top left\n       legend=rownames(con.table),  # get labels\n       fill = my.colors)  # use same fill colors"
  },
  {
    "objectID": "Overview-of-Plots.html#loading-ggplot2",
    "href": "Overview-of-Plots.html#loading-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Loading ggplot2",
    "text": "Loading ggplot2\n\n\nThe ggplot2 package is already installed as a default package in Google Colaboratory.\nHowever, recall each time we start or restart a new session and want to access the library of functions and data in the package, we need to load the library of files in the package with the library command.\nRun the first code cell below to load the ggplot2 package.\nIf restarting a new session, you also need to reload the dplyr package to access storms data.\n\n\nlibrary(ggplot2)  # make sure you have installed ggplot2 package\n\n\n# may need to reload\nlibrary(dplyr)"
  },
  {
    "objectID": "Overview-of-Plots.html#plotting-one-numerical-variable-with-ggplot2",
    "href": "Overview-of-Plots.html#plotting-one-numerical-variable-with-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Plotting One Numerical Variable with ggplot2",
    "text": "Plotting One Numerical Variable with ggplot2\n\nTo create various types of plots for one quantitative variable, such as wind:\n\nThe ggplot object is the data frame storms.\nThe aesthetic is the variable wind that we will plot on the x-axis.\nGeometric objects histogram, density, and boxplot are specified in each of the three code cells below.\nThere a numerous options we can include as well.\n\n\n# create a histogram\nggplot(storms, aes(x = wind)) + \n  geom_histogram(fill = \"steelblue\", color=\"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# create a density plot\nggplot(storms, aes(x = wind)) + \n  geom_density(color=\"red\") + \n  theme_bw() # adding theme_bw()  makes white background\n\n\n\n\n\n# create a boxplot\nggplot(storms, aes(x = wind)) + \n  geom_boxplot(color=\"black\", fill=\"blueviolet\")"
  },
  {
    "objectID": "Overview-of-Plots.html#scatter-plots-with-ggplot2",
    "href": "Overview-of-Plots.html#scatter-plots-with-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Scatter Plots with ggplot2",
    "text": "Scatter Plots with ggplot2\n\nTo create a scatter plot to compare two quantitative variables such as wind speed and pressure of storms:\n\nThe ggplot object is the data frame storms.\nThe aesthetic are the variables\n\npressure is the predictor plotted on the x-axis.\nwind is the response plotted on the y-axis.t\n\nGeometric object is scatter.\n\n\n# create a scatter plot\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind))"
  },
  {
    "objectID": "Overview-of-Plots.html#scaling-ggplot2-plots",
    "href": "Overview-of-Plots.html#scaling-ggplot2-plots",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Scaling ggplot2 plots",
    "text": "Scaling ggplot2 plots\n\nIn general, scaling is the process by which ggplot2 maps variables to unique values. When this is done for discrete numeric or qualitative variables, ggplot2 will often scale the variable to distinct colors, symbols, or sizes, depending on the aesthetic mapped.\nIn the example below, we map the status variable to the color aesthetic, which is then scaled to different colors for the different status levels.\n\n# scatter plot with scaling\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind, color = status))\n\n\n\n\n\nScaling by Shape\n\nAlternatively, we can map the status variable to the shape aesthetic, which creates a plot with different shapes for each observation based on the status level.\n\nBy default, 6 shapes can be used.\nThere are 9 different status of storms.\nThe last option manually sets the shapes for each status to avoid an error.\n\n\n# scaling by shape\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind, shape = status)) + \n  scale_shape_manual(values=0:8)  # manually setting shapes\n\n\n\n\n\n\nApplying Multiple Scales\n\nWe can even combine these two aesthetic mappings in a single plot to get different colors and symbols for each level of month and status, respectively.\n\nBy default, 6 shapes can be used.\nThere are 9 different status of storms.\nThe last option manually sets the shapes for each status to avoid an error.\n\n\n# scaling by month and status\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind, color = month, shape = status)) + \n  scale_shape_manual(values=0:8)  # manually setting shapes for status"
  },
  {
    "objectID": "Overview-of-Plots.html#facetting-in-ggplot2",
    "href": "Overview-of-Plots.html#facetting-in-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Facetting in ggplot2",
    "text": "Facetting in ggplot2\n\nFaceting creates separate panels (facets) of a data frame based on one or more faceting variables.\nTo create various scatter plots (one for each category) to compare two quantitative variables such as wind speed and pressure of storms, we can add a facet_grid.\n\nNote the NA plot corresponds to the storms that are not hurricanes.\n\n\n# faceting by category\nggplot(storms) + \n  geom_point(aes(x = pressure, y = wind)) + \n  facet_grid(~ category)"
  },
  {
    "objectID": "Overview-of-Plots.html#bar-charts-with-ggplot2",
    "href": "Overview-of-Plots.html#bar-charts-with-ggplot2",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Bar Charts with ggplot2",
    "text": "Bar Charts with ggplot2\n\nImagine we would like to compare the number of different types of storms (status) that occurred in each month.\n\nStacked Bar Charts of Counts with ggplot2\n\nTo create a stacked bar chart of counts for one or more qualitative variable:\n\nThe ggplot object is the data frame storms.\nGeometric object is geom_bar.\nThe aesthetic is specified as:\n\nFill color, (fill) is status.\nThe height of each bar is summarizing the statistic (stat) is \"count\".\nThe position=\"stack\" creates a stacked bar chart of counts.\n\n\n\n# stacks bars on top of each other \nggplot(storms, aes(x=month)) + \n    geom_bar(aes(fill=status), stat = \"count\", position=\"stack\") + \n    ggtitle(\"Occurrence of Storms by Month\")\n\n\n\n\n\n\nStacked Relative Frequency Bar Charts with ggplot2\n\nTo create a stacked bar chart of relative frequencies for two qualitative variables:\n\nThe ggplot object is the data frame storms.\nGeometric object is geom_bar.\nThe aesthetic is specified as:\n\nFill color, (fill) is status.\nThe height of each bar is summarizing the statistic (stat) is \"count\".\nThe position=\"fill\" creates a stacked bar chart of relative frequencies.\n\n\n\n# stacks bars and standardizing each stack\nggplot(storms, aes(x=month)) + \n    geom_bar(aes(fill=status), stat = \"count\", position=\"fill\") +  \n    ggtitle(\"Occurrence of Storms by Month\")\n\n\n\n\n\n\nGrouped Bar Charts of Counts with ggplot2\n\nTo create various types of bar plots for one or more qualitative variables:\n\nThe ggplot object is the data frame storms.\nGeometric object is geom_bar.\nThe aesthetic is specified as:\n\nFill color, (fill) is status.\nThe height of each bar is summarizing the statistic (stat) is \"count\".\nThe position=\"dodge\" creates a stacked bar chart.\n\n\n\n# creates grouped bar chart\nggplot(storms, aes(x=month)) + \n    geom_bar(aes(fill=status), stat = \"count\", position=\"dodge\") +  \n    ggtitle(\"Occurrence of Storms by Month\")"
  },
  {
    "objectID": "Overview-of-Plots.html#load-library",
    "href": "Overview-of-Plots.html#load-library",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Load Library",
    "text": "Load Library\n\n\nlibrary(mapview)  # load spatial mapping package"
  },
  {
    "objectID": "Overview-of-Plots.html#mapping-all-storms-by-status",
    "href": "Overview-of-Plots.html#mapping-all-storms-by-status",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Mapping All Storms by Status",
    "text": "Mapping All Storms by Status\n\n\nmapview(storms, xcol = \"long\", ycol = \"lat\", \n        zcol = \"status\", \n        crs = 4269, grid = FALSE)"
  },
  {
    "objectID": "Overview-of-Plots.html#mapping-category-5-hurricanes",
    "href": "Overview-of-Plots.html#mapping-category-5-hurricanes",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Mapping Category 5 Hurricanes",
    "text": "Mapping Category 5 Hurricanes\n\nFirst we filter out observations with category equal to 5.\n\ncat5 &lt;- subset(storms , category == \"5\")  # keep only category 5\n\n\nmapview(cat5, xcol = \"long\", ycol = \"lat\", cex = \"wind\", crs = 4269, grid = FALSE)\n\n\nmapview(cat5, xcol = \"long\", ycol = \"lat\", zcol = \"name\", cex = \"wind\", crs = 4269, grid = FALSE)"
  },
  {
    "objectID": "Overview-of-Plots.html#sec-cc",
    "href": "Overview-of-Plots.html#sec-cc",
    "title": "Appendix B — An Overview of Plotting Data in R",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n\n \nStatistical Methods: Exploring the Uncertain by Adam Spiegler is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  }
]